{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Programacion https://learning.edx.org/course/course-v1:HarvardX+TinyML1+3T2020/home Fundamentals of TinyML As mentioned earlier, we\u2019ll be using the Colab environment for much of our programming in this specialization. In order to ensure that everyone has some hands on experience with Colab, please follow the link below to Google\u2019s Introductory Colab. In it they provide a handful of quick tips and hands on exercises to get you started using the Colab environment. https://colab.research.google.com/notebooks/intro.ipynb For machine learning in this course we are going to use Google\u2019s TensorFlow Machine Learning framework. TensorFlow is widely used across industry and academia. To get you started with TensorFlow here is Google\u2019s wonderful intro to TensorFlow video. Sample TensorFlow code While TensorFlow is written with fast custom C++ code under the hood, it has a high level Python interface that we will use throughout this course. As such, you\u2019ll need to be comfortable with basic coding in Python for this course. For example we might create a custom Neural Network model using code that looks something like the below. Don\u2019t worry if you don\u2019t know what all of the words mean, we\u2019ll walk you through all of these topics later in the course. For now just make sure you know enough Python to understand the following high level takeaways: This is a custom class that extends the tf.keras.Model base class It defines four specific kinds of tf.keras.layers It has a call function which will run an input through those layers (functions) If that resonates with you then you probably know enough Python to do well in this course! If you don\u2019t feel like you know enough Python yet, don\u2019t worry. Both Python.org and LearnPython.org have great resources you can use to get you up to speed. We\u2019re sure that with a little work, very soon you\u2019ll be ready to proceed with this course. Load in the TensorFlow library import tensorflow as tf Define my custom Neural Network model class MyModel(tf.keras.Model): def __init__(self): super(MyModel, self).__init__() # define Neural Network layer types self.conv = tf.keras.layers.Conv2D(32, 3, activation='relu') self.flatten = tf.keras.layers.Flatten() self.dense1 = tf.keras.layers.Dense(128, activation='relu') self.dense2 = tf.keras.layers.Dense(10) run my Neural Network model by evaluating # each layer on my input data def call(self, x): x = self.conv(x) x = self.flatten(x) x = self.dense1(x) x = self.dense2(x) return x Create an instance of the model model = MyModel() Coding exercise In this next Colab you will get to explore linear regression and loss functions for yourself. We have provided a cell that will compute the predicted Y values as well as the loss function for your guess of w and b. Simply change their values and explore how the output and loss changes. Note: You may receive a warning that the Colab was not authored by Google, and this may occur on future Colabs as well. That is entirely normal as we authored many of these Colabs ourselves for this course https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-4-ExploringLoss.ipynb Coding exercise: Gradient descent In this next Colab you will get to minimize the loss function for our linear regression model using TensorFlow\u2019s built-in GradientTape class. You will also get to visualize the training process, seeing how both the bias and weight converge on the optimal solution across training epochs. https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-6-MimimizingLoss.ipynb Coding exercise: neural networks In this next Colab you will get to explore training your first neural network\u2014see just how easy that can be with TensorFlow. You\u2019ll also see the result of the question posed at the end of the last video\u2014will the predicted Y be 19 for X = 10? Launch the Colab to find out! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-9-FirstNeuralNetwork.ipynb More neural networks Up to now you\u2019ve been looking at matching X values to Y values when there\u2019s a linear relationship between them. So, for example, you matched the X values in this set [-1, 0 , 1, 2, 3, 4] to the Y values in this set [-3, -1, 1, 3, 5, 7] by figuring out the equation Y=2X-1. You then saw how a very simple neural network with a single neuron within it could be used for this. An input leads into a neuron as an arrow. Out of this an output arrow is generated. This worked very well, because, in reality, what is referred to as a \u2018neuron\u2019 here is simply a function that has two learnable parameters, called a \u2018weight\u2019 and a \u2018bias\u2019, where, the output of the neuron will be: Output = (Weight * Input) + Bias So, for learning the linear relationship between our Xs and Ys, this maps perfectly, where we want the weight to be learned as \u20182\u2019, and the bias as \u2018-1\u2019. In the code you saw this happening. When multiple neurons work together in layers, the learned weights and biases across these layers can then have the effect of letting the neural network learn more complex patterns. You\u2019ll learn more about how this works later in the course. In your first Neural Network you saw neurons that were densely connected to each other, so you saw the Dense layer type. As well as neurons like this, there are also additional layer types in TensorFlow that you\u2019ll encounter. Here\u2019s just a few of them: Convolutional layers contain filters that can be used to transform data. The values of these filters will be learned in the same way as the parameters in the Dense neuron you saw here. Thus, a network containing them can learn how to transform data effectively. This is especially useful in Computer Vision, which you\u2019ll see later in this course. We\u2019ll even use these convolutional layers that are typically used for vision models to do speech detection! Are you wondering how or why? Stay tuned! Recurrent layers learn about the relationships between pieces of data in a sequence. There are many types of recurrent layer, with a popular one called LSTM (Long, Short Term Memory), being particularly effective. Recurrent layers are useful for predicting sequence data (like the weather), or understanding text. You\u2019ll also encounter layer types that don\u2019t learn parameters themselves, but which can affect the other layers. These include layers like dropouts, which are used to reduce the density of connection between dense layers to make them more efficient, pooling which can be used to reduce the amount of data flowing through the network to remove unnecessary information, and lambda lambda layers that allow you to execute arbitrary code. Your journey over the next few videos will primarily deal with the Dense network type, and you\u2019ll start to explore how multiple layers can work together to infer the rules that match your data to your labels. Neural networks in action You\u2019ve now seen a very simple example for how computers can learn. There\u2019s no great mystery to it -- it\u2019s a simple algorithm of making a guess, measuring how good that guess is (aka the loss), and then using this information to optimize the guess, and continually repeating this process to improve the guess. What you\u2019ve seen -- fitting numbers in an equation -- might seem trivial, but the methodology that you used to do this is the same as is used in far more sophisticated scenarios. To understand just how powerful this simple method - Machine Learning - can be, lets now explore a couple of new and exciting case studies: The first is the story of a young woman, Nazrini Siraji who used Machine Learning to detect diseases in crops, helping to stem the destruction of crops in her home country of Uganda: https://www.youtube.com/watch?v=23Q7HciuVyM Next, is air cognizer, built by undergraduate students in India, who realized that pictures of the sky, when matched to labels on air pollution could be used to build a new type of air quality sensor, using just the cameras on their phones: https://www.youtube.com/watch?v=9r2VVM4nfk8 Finally, here\u2019s a talk from a Google engineer about how Google used images of retinas to build a diabetic retinopathy detector using TensorFlow that performs state of the art diagnosis of this disease: https://www.youtube.com/watch?v=oOeZ7IgEN4o","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#fundamentals-of-tinyml","text":"As mentioned earlier, we\u2019ll be using the Colab environment for much of our programming in this specialization. In order to ensure that everyone has some hands on experience with Colab, please follow the link below to Google\u2019s Introductory Colab. In it they provide a handful of quick tips and hands on exercises to get you started using the Colab environment. https://colab.research.google.com/notebooks/intro.ipynb For machine learning in this course we are going to use Google\u2019s TensorFlow Machine Learning framework. TensorFlow is widely used across industry and academia. To get you started with TensorFlow here is Google\u2019s wonderful intro to TensorFlow video.","title":"Fundamentals of TinyML"},{"location":"#sample-tensorflow-code","text":"While TensorFlow is written with fast custom C++ code under the hood, it has a high level Python interface that we will use throughout this course. As such, you\u2019ll need to be comfortable with basic coding in Python for this course. For example we might create a custom Neural Network model using code that looks something like the below. Don\u2019t worry if you don\u2019t know what all of the words mean, we\u2019ll walk you through all of these topics later in the course. For now just make sure you know enough Python to understand the following high level takeaways: This is a custom class that extends the tf.keras.Model base class It defines four specific kinds of tf.keras.layers It has a call function which will run an input through those layers (functions) If that resonates with you then you probably know enough Python to do well in this course! If you don\u2019t feel like you know enough Python yet, don\u2019t worry. Both Python.org and LearnPython.org have great resources you can use to get you up to speed. We\u2019re sure that with a little work, very soon you\u2019ll be ready to proceed with this course.","title":"Sample TensorFlow code"},{"location":"#load-in-the-tensorflow-library","text":"import tensorflow as tf","title":"Load in the TensorFlow library"},{"location":"#define-my-custom-neural-network-model","text":"class MyModel(tf.keras.Model): def __init__(self): super(MyModel, self).__init__() # define Neural Network layer types self.conv = tf.keras.layers.Conv2D(32, 3, activation='relu') self.flatten = tf.keras.layers.Flatten() self.dense1 = tf.keras.layers.Dense(128, activation='relu') self.dense2 = tf.keras.layers.Dense(10) run my Neural Network model by evaluating # each layer on my input data def call(self, x): x = self.conv(x) x = self.flatten(x) x = self.dense1(x) x = self.dense2(x) return x","title":"Define my custom Neural Network model"},{"location":"#create-an-instance-of-the-model","text":"model = MyModel()","title":"Create an instance of the model"},{"location":"#coding-exercise","text":"In this next Colab you will get to explore linear regression and loss functions for yourself. We have provided a cell that will compute the predicted Y values as well as the loss function for your guess of w and b. Simply change their values and explore how the output and loss changes. Note: You may receive a warning that the Colab was not authored by Google, and this may occur on future Colabs as well. That is entirely normal as we authored many of these Colabs ourselves for this course https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-4-ExploringLoss.ipynb","title":"Coding exercise"},{"location":"#coding-exercise-gradient-descent","text":"In this next Colab you will get to minimize the loss function for our linear regression model using TensorFlow\u2019s built-in GradientTape class. You will also get to visualize the training process, seeing how both the bias and weight converge on the optimal solution across training epochs. https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-6-MimimizingLoss.ipynb","title":"Coding exercise: Gradient descent"},{"location":"#coding-exercise-neural-networks","text":"In this next Colab you will get to explore training your first neural network\u2014see just how easy that can be with TensorFlow. You\u2019ll also see the result of the question posed at the end of the last video\u2014will the predicted Y be 19 for X = 10? Launch the Colab to find out! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-9-FirstNeuralNetwork.ipynb","title":"Coding exercise: neural networks"},{"location":"#more-neural-networks","text":"Up to now you\u2019ve been looking at matching X values to Y values when there\u2019s a linear relationship between them. So, for example, you matched the X values in this set [-1, 0 , 1, 2, 3, 4] to the Y values in this set [-3, -1, 1, 3, 5, 7] by figuring out the equation Y=2X-1. You then saw how a very simple neural network with a single neuron within it could be used for this. An input leads into a neuron as an arrow. Out of this an output arrow is generated. This worked very well, because, in reality, what is referred to as a \u2018neuron\u2019 here is simply a function that has two learnable parameters, called a \u2018weight\u2019 and a \u2018bias\u2019, where, the output of the neuron will be: Output = (Weight * Input) + Bias So, for learning the linear relationship between our Xs and Ys, this maps perfectly, where we want the weight to be learned as \u20182\u2019, and the bias as \u2018-1\u2019. In the code you saw this happening. When multiple neurons work together in layers, the learned weights and biases across these layers can then have the effect of letting the neural network learn more complex patterns. You\u2019ll learn more about how this works later in the course. In your first Neural Network you saw neurons that were densely connected to each other, so you saw the Dense layer type. As well as neurons like this, there are also additional layer types in TensorFlow that you\u2019ll encounter. Here\u2019s just a few of them: Convolutional layers contain filters that can be used to transform data. The values of these filters will be learned in the same way as the parameters in the Dense neuron you saw here. Thus, a network containing them can learn how to transform data effectively. This is especially useful in Computer Vision, which you\u2019ll see later in this course. We\u2019ll even use these convolutional layers that are typically used for vision models to do speech detection! Are you wondering how or why? Stay tuned! Recurrent layers learn about the relationships between pieces of data in a sequence. There are many types of recurrent layer, with a popular one called LSTM (Long, Short Term Memory), being particularly effective. Recurrent layers are useful for predicting sequence data (like the weather), or understanding text. You\u2019ll also encounter layer types that don\u2019t learn parameters themselves, but which can affect the other layers. These include layers like dropouts, which are used to reduce the density of connection between dense layers to make them more efficient, pooling which can be used to reduce the amount of data flowing through the network to remove unnecessary information, and lambda lambda layers that allow you to execute arbitrary code. Your journey over the next few videos will primarily deal with the Dense network type, and you\u2019ll start to explore how multiple layers can work together to infer the rules that match your data to your labels.","title":"More neural networks"},{"location":"#neural-networks-in-action","text":"You\u2019ve now seen a very simple example for how computers can learn. There\u2019s no great mystery to it -- it\u2019s a simple algorithm of making a guess, measuring how good that guess is (aka the loss), and then using this information to optimize the guess, and continually repeating this process to improve the guess. What you\u2019ve seen -- fitting numbers in an equation -- might seem trivial, but the methodology that you used to do this is the same as is used in far more sophisticated scenarios. To understand just how powerful this simple method - Machine Learning - can be, lets now explore a couple of new and exciting case studies: The first is the story of a young woman, Nazrini Siraji who used Machine Learning to detect diseases in crops, helping to stem the destruction of crops in her home country of Uganda: https://www.youtube.com/watch?v=23Q7HciuVyM Next, is air cognizer, built by undergraduate students in India, who realized that pictures of the sky, when matched to labels on air pollution could be used to build a new type of air quality sensor, using just the cameras on their phones: https://www.youtube.com/watch?v=9r2VVM4nfk8 Finally, here\u2019s a talk from a Google engineer about how Google used images of retinas to build a diabetic retinopathy detector using TensorFlow that performs state of the art diagnosis of this disease: https://www.youtube.com/watch?v=oOeZ7IgEN4o","title":"Neural networks in action"},{"location":"ExploringLoss/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import math # Edit these parameters to try different loss # measurements. Rerun this cell when done # Your Y will be calculated as Y=wX+b, so # if w=3, and b=-1, then Y=3x-1 w = 3 b = - 1 x = [ - 1 , 0 , 1 , 2 , 3 , 4 ] y = [ - 3 , - 1 , 1 , 3 , 5 , 7 ] myY = [] for thisX in x : thisY = ( w * thisX ) + b myY . append ( thisY ) print ( \"Real Y is \" + str ( y )) print ( \"My Y is \" + str ( myY )) # let's calculate the loss total_square_error = 0 for i in range ( 0 , len ( y )): square_error = ( y [ i ] - myY [ i ]) ** 2 total_square_error += square_error print ( \"My loss is: \" + str ( math . sqrt ( total_square_error ))) Real Y is [-3, -1, 1, 3, 5, 7] My Y is [-4, -1, 2, 5, 8, 11] My loss is: 5.5677643628300215","title":"ExploringLoss"},{"location":"FirstNeuralNetwork/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import tensorflow as tf import numpy as np from tensorflow import keras # define a neural network with one neuron # for more information on TF functions see: https://www.tensorflow.org/api_docs model = tf . keras . Sequential ([ keras . layers . Dense ( units = 1 , input_shape = [ 1 ])]) # use stochastic gradient descent for optimization and # the mean squared error loss function model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) # define some training data (xs as inputs and ys as outputs) xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) ys = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ], dtype = float ) # fit the model to the data (aka train the model) model . fit ( xs , ys , epochs = 500 ) print ( model . predict ([ 10.0 ]))","title":"FirstNeuralNetwork"},{"location":"Linear_Regression_with_Synthetic_Data/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); #@title Copyright 2020 Google LLC. Double-click here for license information. # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Simple Linear Regression with Synthetic Data In this first Colab, you'll explore linear regression with a simple database. Learning objectives: After doing this exercise, you'll know how to do the following: Run Colabs. Tune the following hyperparameters : learning rate number of epochs batch size Interpret different kinds of loss curves . About Colabs Machine Learning Crash Course uses Colaboratories ( Colabs ) for all programming exercises. Colab is Google's implementation of Jupyter Notebook . Like all Jupyter Notebooks, a Colab consists of two kinds of components: Text cells , which contain explanations. You are currently reading a text cell. Code cells , which contain Python code for you to run. Code cells have a light gray background. You read the text cells and run the code cells. Running code cells You must run code cells in order. In other words, you may only run a code cell once all the code cells preceding it have already been run. To run a code cell: Place the cursor anywhere inside the [ ] area at the top left of a code cell. The area inside the [ ] will display an arrow. Click the arrow. Alternatively, you may invoke Runtime->Run all . Note, though, that some of the code cells will fail because not all the coding is complete. (You'll complete the coding as part of the exercise.) Understanding hidden code cells We've hidden the code in code cells that don't advance the learning objectives. For example, we've hidden the code that plots graphs. However, you must still run code cells containing hidden code . You'll know that the code is hidden because you'll see a title (for example, \"Load the functions that build and train a model\") without seeing the code. To view the hidden code, just double click the header. Why did you see an error? If a code cell returns an error when you run it, consider two common problems: You didn't run all of the code cells preceding the current code cell. If the code cell is labeled as a Task , then you haven't written the necessary code. Use the right version of TensorFlow The following hidden code cell ensures that the Colab will run on TensorFlow 2.X, which is the most recent version of TensorFlow: #@title Run this Colab on TensorFlow 2.x %tensorflow_version 2.x Import relevant modules The following cell imports the packages that the program requires: import pandas as pd import tensorflow as tf from matplotlib import pyplot as plt Define functions that build and train a model The following code defines two functions: build_model(my_learning_rate) , which builds an empty model. train_model(model, feature, label, epochs) , which trains the model from the examples (feature and label) you pass. Since you don't need to understand model building code right now, we've hidden this code cell. You may optionally double-click the headline to explore this code. #@title Define the functions that build and train a model def build_model(my_learning_rate): \"\"\"Create and compile a simple linear regression model.\"\"\" # Most simple tf.keras models are sequential. # A sequential model contains one or more layers. model = tf.keras.models.Sequential() # Describe the topography of the model. # The topography of a simple linear regression model # is a single node in a single layer. model.add(tf.keras.layers.Dense(units=1, input_shape=(1,))) # Compile the model topography into code that # TensorFlow can efficiently execute. Configure # training to minimize the model's mean squared error. model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()]) return model def train_model(model, feature, label, epochs, batch_size): \"\"\"Train the model by feeding it data.\"\"\" # Feed the feature values and the label values to the # model. The model will train for the specified number # of epochs, gradually learning how the feature values # relate to the label values. history = model.fit(x=feature, y=label, batch_size=batch_size, epochs=epochs) # Gather the trained model's weight and bias. trained_weight = model.get_weights()[0] trained_bias = model.get_weights()[1] # The list of epochs is stored separately from the # rest of history. epochs = history.epoch # Gather the history (a snapshot) of each epoch. hist = pd.DataFrame(history.history) # Specifically gather the model's root mean #squared error at each epoch. rmse = hist[\"root_mean_squared_error\"] return trained_weight, trained_bias, epochs, rmse print(\"Defined create_model and train_model\") Define plotting functions We're using a popular Python library called Matplotlib to create the following two plots: a plot of the feature values vs. the label values, and a line showing the output of the trained model. a loss curve . We hid the following code cell because learning Matplotlib is not relevant to the learning objectives. Regardless, you must still run all hidden code cells. #@title Define the plotting functions def plot_the_model(trained_weight, trained_bias, feature, label): \"\"\"Plot the trained model against the training feature and label.\"\"\" # Label the axes. plt.xlabel(\"feature\") plt.ylabel(\"label\") # Plot the feature values vs. label values. plt.scatter(feature, label) # Create a red line representing the model. The red line starts # at coordinates (x0, y0) and ends at coordinates (x1, y1). x0 = 0 y0 = trained_bias x1 = feature[-1] y1 = trained_bias + (trained_weight * x1) plt.plot([x0, x1], [y0, y1], c='r') # Render the scatter plot and the red line. plt.show() def plot_the_loss_curve(epochs, rmse): \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\" plt.figure() plt.xlabel(\"Epoch\") plt.ylabel(\"Root Mean Squared Error\") plt.plot(epochs, rmse, label=\"Loss\") plt.legend() plt.ylim([rmse.min()*0.97, rmse.max()]) plt.show() print(\"Defined the plot_the_model and plot_the_loss_curve functions.\") Define the dataset The dataset consists of 12 examples . Each example consists of one feature and one label . my_feature = ([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]) my_label = ([5.0, 8.8, 9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2]) Specify the hyperparameters The hyperparameters in this Colab are as follows: learning rate epochs batch_size The following code cell initializes these hyperparameters and then invokes the functions that build and train the model. learning_rate=0.01 epochs=10 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Task 1: Examine the graphs Examine the top graph. The blue dots identify the actual data; the red line identifies the output of the trained model. Ideally, the red line should align nicely with the blue dots. Does it? Probably not. A certain amount of randomness plays into training a model, so you'll get somewhat different results every time you train. That said, unless you are an extremely lucky person, the red line probably doesn't align nicely with the blue dots. Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently. Task 2: Increase the number of epochs Training loss should steadily decrease, steeply at first, and then more slowly. Eventually, training loss should eventually stay steady (zero slope or nearly zero slope), which indicates that training has converged . In Task 1, the training loss did not converge. One possible solution is to train for more epochs. Your task is to increase the number of epochs sufficiently to get the model to converge. However, it is inefficient to train past convergence, so don't just set the number of epochs to an arbitrarily high value. Examine the loss curve. Does the model converge? learning_rate=0.01 epochs= ? # Replace ? with an integer. my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.01 epochs=450 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) # The loss curve suggests that the model does converge. Task 3: Increase the learning rate In Task 2, you increased the number of epochs to get the model to converge. Sometimes, you can get the model to converge more quickly by increasing the learning rate. However, setting the learning rate too high often makes it impossible for a model to converge. In Task 3, we've intentionally set the learning rate too high. Run the following code cell and see what happens. # Increase the learning rate and decrease the number of epochs. learning_rate=100 epochs=500 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) The resulting model is terrible; the red line doesn't align with the blue dots. Furthermore, the loss curve oscillates like a roller coaster . An oscillating loss curve strongly suggests that the learning rate is too high. Task 4: Find the ideal combination of epochs and learning rate Assign values to the following two hyperparameters to make training converge as efficiently as possible: learning_rate epochs # Set the learning rate and number of epochs learning_rate= ? # Replace ? with a floating-point number epochs= ? # Replace ? with an integer my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.14 epochs=70 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Task 5: Adjust the batch size The system recalculates the model's loss value and adjusts the model's weights and bias after each iteration . Each iteration is the span in which the system processes one batch. For example, if the batch size is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples. One epoch spans sufficient iterations to process every example in the dataset. For example, if the batch size is 12, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations. It is tempting to simply set the batch size to the number of examples in the dataset (12, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. Experiment with batch_size in the following code cell. What's the smallest integer you can set for batch_size and still have the model converge in a hundred epochs? learning_rate=0.05 epochs=100 my_batch_size= ? # Replace ? with an integer. my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.05 epochs=125 my_batch_size=1 # Wow, a batch size of 1 works! my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Summary of hyperparameter tuning Most machine learning problems require a lot of hyperparameter tuning. Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb: Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. If the training loss does not converge, train for more epochs. If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging. If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate. Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination. Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation. For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.","title":"Linear Regression with Synthetic Data"},{"location":"Linear_Regression_with_Synthetic_Data/#simple-linear-regression-with-synthetic-data","text":"In this first Colab, you'll explore linear regression with a simple database.","title":"Simple Linear Regression with Synthetic Data"},{"location":"Linear_Regression_with_Synthetic_Data/#learning-objectives","text":"After doing this exercise, you'll know how to do the following: Run Colabs. Tune the following hyperparameters : learning rate number of epochs batch size Interpret different kinds of loss curves .","title":"Learning objectives:"},{"location":"Linear_Regression_with_Synthetic_Data/#about-colabs","text":"Machine Learning Crash Course uses Colaboratories ( Colabs ) for all programming exercises. Colab is Google's implementation of Jupyter Notebook . Like all Jupyter Notebooks, a Colab consists of two kinds of components: Text cells , which contain explanations. You are currently reading a text cell. Code cells , which contain Python code for you to run. Code cells have a light gray background. You read the text cells and run the code cells.","title":"About Colabs"},{"location":"Linear_Regression_with_Synthetic_Data/#running-code-cells","text":"You must run code cells in order. In other words, you may only run a code cell once all the code cells preceding it have already been run. To run a code cell: Place the cursor anywhere inside the [ ] area at the top left of a code cell. The area inside the [ ] will display an arrow. Click the arrow. Alternatively, you may invoke Runtime->Run all . Note, though, that some of the code cells will fail because not all the coding is complete. (You'll complete the coding as part of the exercise.)","title":"Running code cells"},{"location":"Linear_Regression_with_Synthetic_Data/#understanding-hidden-code-cells","text":"We've hidden the code in code cells that don't advance the learning objectives. For example, we've hidden the code that plots graphs. However, you must still run code cells containing hidden code . You'll know that the code is hidden because you'll see a title (for example, \"Load the functions that build and train a model\") without seeing the code. To view the hidden code, just double click the header.","title":"Understanding hidden code cells"},{"location":"Linear_Regression_with_Synthetic_Data/#why-did-you-see-an-error","text":"If a code cell returns an error when you run it, consider two common problems: You didn't run all of the code cells preceding the current code cell. If the code cell is labeled as a Task , then you haven't written the necessary code.","title":"Why did you see an error?"},{"location":"Linear_Regression_with_Synthetic_Data/#use-the-right-version-of-tensorflow","text":"The following hidden code cell ensures that the Colab will run on TensorFlow 2.X, which is the most recent version of TensorFlow: #@title Run this Colab on TensorFlow 2.x %tensorflow_version 2.x","title":"Use the right version of TensorFlow"},{"location":"Linear_Regression_with_Synthetic_Data/#import-relevant-modules","text":"The following cell imports the packages that the program requires: import pandas as pd import tensorflow as tf from matplotlib import pyplot as plt","title":"Import relevant modules"},{"location":"Linear_Regression_with_Synthetic_Data/#define-functions-that-build-and-train-a-model","text":"The following code defines two functions: build_model(my_learning_rate) , which builds an empty model. train_model(model, feature, label, epochs) , which trains the model from the examples (feature and label) you pass. Since you don't need to understand model building code right now, we've hidden this code cell. You may optionally double-click the headline to explore this code. #@title Define the functions that build and train a model def build_model(my_learning_rate): \"\"\"Create and compile a simple linear regression model.\"\"\" # Most simple tf.keras models are sequential. # A sequential model contains one or more layers. model = tf.keras.models.Sequential() # Describe the topography of the model. # The topography of a simple linear regression model # is a single node in a single layer. model.add(tf.keras.layers.Dense(units=1, input_shape=(1,))) # Compile the model topography into code that # TensorFlow can efficiently execute. Configure # training to minimize the model's mean squared error. model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()]) return model def train_model(model, feature, label, epochs, batch_size): \"\"\"Train the model by feeding it data.\"\"\" # Feed the feature values and the label values to the # model. The model will train for the specified number # of epochs, gradually learning how the feature values # relate to the label values. history = model.fit(x=feature, y=label, batch_size=batch_size, epochs=epochs) # Gather the trained model's weight and bias. trained_weight = model.get_weights()[0] trained_bias = model.get_weights()[1] # The list of epochs is stored separately from the # rest of history. epochs = history.epoch # Gather the history (a snapshot) of each epoch. hist = pd.DataFrame(history.history) # Specifically gather the model's root mean #squared error at each epoch. rmse = hist[\"root_mean_squared_error\"] return trained_weight, trained_bias, epochs, rmse print(\"Defined create_model and train_model\")","title":"Define functions that build and train a model"},{"location":"Linear_Regression_with_Synthetic_Data/#define-plotting-functions","text":"We're using a popular Python library called Matplotlib to create the following two plots: a plot of the feature values vs. the label values, and a line showing the output of the trained model. a loss curve . We hid the following code cell because learning Matplotlib is not relevant to the learning objectives. Regardless, you must still run all hidden code cells. #@title Define the plotting functions def plot_the_model(trained_weight, trained_bias, feature, label): \"\"\"Plot the trained model against the training feature and label.\"\"\" # Label the axes. plt.xlabel(\"feature\") plt.ylabel(\"label\") # Plot the feature values vs. label values. plt.scatter(feature, label) # Create a red line representing the model. The red line starts # at coordinates (x0, y0) and ends at coordinates (x1, y1). x0 = 0 y0 = trained_bias x1 = feature[-1] y1 = trained_bias + (trained_weight * x1) plt.plot([x0, x1], [y0, y1], c='r') # Render the scatter plot and the red line. plt.show() def plot_the_loss_curve(epochs, rmse): \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\" plt.figure() plt.xlabel(\"Epoch\") plt.ylabel(\"Root Mean Squared Error\") plt.plot(epochs, rmse, label=\"Loss\") plt.legend() plt.ylim([rmse.min()*0.97, rmse.max()]) plt.show() print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")","title":"Define plotting functions"},{"location":"Linear_Regression_with_Synthetic_Data/#define-the-dataset","text":"The dataset consists of 12 examples . Each example consists of one feature and one label . my_feature = ([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]) my_label = ([5.0, 8.8, 9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])","title":"Define the dataset"},{"location":"Linear_Regression_with_Synthetic_Data/#specify-the-hyperparameters","text":"The hyperparameters in this Colab are as follows: learning rate epochs batch_size The following code cell initializes these hyperparameters and then invokes the functions that build and train the model. learning_rate=0.01 epochs=10 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Specify the hyperparameters"},{"location":"Linear_Regression_with_Synthetic_Data/#task-1-examine-the-graphs","text":"Examine the top graph. The blue dots identify the actual data; the red line identifies the output of the trained model. Ideally, the red line should align nicely with the blue dots. Does it? Probably not. A certain amount of randomness plays into training a model, so you'll get somewhat different results every time you train. That said, unless you are an extremely lucky person, the red line probably doesn't align nicely with the blue dots. Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently.","title":"Task 1: Examine the graphs"},{"location":"Linear_Regression_with_Synthetic_Data/#task-2-increase-the-number-of-epochs","text":"Training loss should steadily decrease, steeply at first, and then more slowly. Eventually, training loss should eventually stay steady (zero slope or nearly zero slope), which indicates that training has converged . In Task 1, the training loss did not converge. One possible solution is to train for more epochs. Your task is to increase the number of epochs sufficiently to get the model to converge. However, it is inefficient to train past convergence, so don't just set the number of epochs to an arbitrarily high value. Examine the loss curve. Does the model converge? learning_rate=0.01 epochs= ? # Replace ? with an integer. my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.01 epochs=450 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) # The loss curve suggests that the model does converge.","title":"Task 2: Increase the number of epochs"},{"location":"Linear_Regression_with_Synthetic_Data/#task-3-increase-the-learning-rate","text":"In Task 2, you increased the number of epochs to get the model to converge. Sometimes, you can get the model to converge more quickly by increasing the learning rate. However, setting the learning rate too high often makes it impossible for a model to converge. In Task 3, we've intentionally set the learning rate too high. Run the following code cell and see what happens. # Increase the learning rate and decrease the number of epochs. learning_rate=100 epochs=500 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) The resulting model is terrible; the red line doesn't align with the blue dots. Furthermore, the loss curve oscillates like a roller coaster . An oscillating loss curve strongly suggests that the learning rate is too high.","title":"Task 3: Increase the learning rate"},{"location":"Linear_Regression_with_Synthetic_Data/#task-4-find-the-ideal-combination-of-epochs-and-learning-rate","text":"Assign values to the following two hyperparameters to make training converge as efficiently as possible: learning_rate epochs # Set the learning rate and number of epochs learning_rate= ? # Replace ? with a floating-point number epochs= ? # Replace ? with an integer my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.14 epochs=70 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Task 4: Find the ideal combination of epochs and learning rate"},{"location":"Linear_Regression_with_Synthetic_Data/#task-5-adjust-the-batch-size","text":"The system recalculates the model's loss value and adjusts the model's weights and bias after each iteration . Each iteration is the span in which the system processes one batch. For example, if the batch size is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples. One epoch spans sufficient iterations to process every example in the dataset. For example, if the batch size is 12, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations. It is tempting to simply set the batch size to the number of examples in the dataset (12, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. Experiment with batch_size in the following code cell. What's the smallest integer you can set for batch_size and still have the model converge in a hundred epochs? learning_rate=0.05 epochs=100 my_batch_size= ? # Replace ? with an integer. my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.05 epochs=125 my_batch_size=1 # Wow, a batch size of 1 works! my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Task 5: Adjust the batch size"},{"location":"Linear_Regression_with_Synthetic_Data/#summary-of-hyperparameter-tuning","text":"Most machine learning problems require a lot of hyperparameter tuning. Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb: Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. If the training loss does not converge, train for more epochs. If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging. If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate. Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination. Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation. For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.","title":"Summary of hyperparameter tuning"},{"location":"Mimimizing_Loss/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # First import the functions we will need from __future__ import absolute_import , division , print_function , unicode_literals try : # %tensorflow_version only exists in Colab. % tensorflow_version 2. x except Exception : pass import tensorflow as tf import numpy as np import matplotlib.pyplot as plt GradientTape The Calculus is managed by a TensorFlow Gradient Tape. You can learn more about the gradient tape at https://www.tensorflow.org/api_docs/python/tf/GradientTape, and we will discuss it later in the course. # Define our initial guess INITIAL_W = 10.0 INITIAL_B = 10.0 # Define our loss function def loss ( predicted_y , target_y ): return tf . reduce_mean ( tf . square ( predicted_y - target_y )) # Define our training procedure def train ( model , inputs , outputs , learning_rate ): with tf . GradientTape () as t : current_loss = loss ( model ( inputs ), outputs ) # Here is where you differentiate the model values with respect to the loss function dw , db = t . gradient ( current_loss , [ model . w , model . b ]) # And here is where you update the model values based on the learning rate chosen model . w . assign_sub ( learning_rate * dw ) model . b . assign_sub ( learning_rate * db ) return current_loss # Define our simple linear regression model class Model ( object ): def __init__ ( self ): # Initialize the weights self . w = tf . Variable ( INITIAL_W ) self . b = tf . Variable ( INITIAL_B ) def __call__ ( self , x ): return self . w * x + self . b Train our model # Define our input data and learning rate xs = [ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ] ys = [ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ] LEARNING_RATE = 0.09 # Instantiate our model model = Model () # Collect the history of w-values and b-values to plot later list_w , list_b = [], [] epochs = range ( 50 ) losses = [] for epoch in epochs : list_w . append ( model . w . numpy ()) list_b . append ( model . b . numpy ()) current_loss = train ( model , xs , ys , learning_rate = LEARNING_RATE ) losses . append ( current_loss ) print ( 'Epoch %2d : w= %1.2f b= %1.2f , loss= %2.5f ' % ( epoch , list_w [ - 1 ], list_b [ - 1 ], current_loss )) Plot our trained values over time # Plot the w-values and b-values for each training Epoch against the true values TRUE_w = 2.0 TRUE_b = - 1.0 plt . plot ( epochs , list_w , 'r' , epochs , list_b , 'b' ) plt . plot ([ TRUE_w ] * len ( epochs ), 'r--' , [ TRUE_b ] * len ( epochs ), 'b--' ) plt . legend ([ 'w' , 'b' , 'True w' , 'True b' ]) plt . show ()","title":"Mimimizing Loss"},{"location":"Mimimizing_Loss/#gradienttape","text":"The Calculus is managed by a TensorFlow Gradient Tape. You can learn more about the gradient tape at https://www.tensorflow.org/api_docs/python/tf/GradientTape, and we will discuss it later in the course. # Define our initial guess INITIAL_W = 10.0 INITIAL_B = 10.0 # Define our loss function def loss ( predicted_y , target_y ): return tf . reduce_mean ( tf . square ( predicted_y - target_y )) # Define our training procedure def train ( model , inputs , outputs , learning_rate ): with tf . GradientTape () as t : current_loss = loss ( model ( inputs ), outputs ) # Here is where you differentiate the model values with respect to the loss function dw , db = t . gradient ( current_loss , [ model . w , model . b ]) # And here is where you update the model values based on the learning rate chosen model . w . assign_sub ( learning_rate * dw ) model . b . assign_sub ( learning_rate * db ) return current_loss # Define our simple linear regression model class Model ( object ): def __init__ ( self ): # Initialize the weights self . w = tf . Variable ( INITIAL_W ) self . b = tf . Variable ( INITIAL_B ) def __call__ ( self , x ): return self . w * x + self . b","title":"GradientTape"},{"location":"Mimimizing_Loss/#train-our-model","text":"# Define our input data and learning rate xs = [ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ] ys = [ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ] LEARNING_RATE = 0.09 # Instantiate our model model = Model () # Collect the history of w-values and b-values to plot later list_w , list_b = [], [] epochs = range ( 50 ) losses = [] for epoch in epochs : list_w . append ( model . w . numpy ()) list_b . append ( model . b . numpy ()) current_loss = train ( model , xs , ys , learning_rate = LEARNING_RATE ) losses . append ( current_loss ) print ( 'Epoch %2d : w= %1.2f b= %1.2f , loss= %2.5f ' % ( epoch , list_w [ - 1 ], list_b [ - 1 ], current_loss ))","title":"Train our model"},{"location":"Mimimizing_Loss/#plot-our-trained-values-over-time","text":"# Plot the w-values and b-values for each training Epoch against the true values TRUE_w = 2.0 TRUE_b = - 1.0 plt . plot ( epochs , list_w , 'r' , epochs , list_b , 'b' ) plt . plot ([ TRUE_w ] * len ( epochs ), 'r--' , [ TRUE_b ] * len ( epochs ), 'b--' ) plt . legend ([ 'w' , 'b' , 'True w' , 'True b' ]) plt . show ()","title":"Plot our trained values over time"},{"location":"Welcome_To_Colaboratory/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Welcome to Colab! If you're already familiar with Colab, check out this video to learn about interactive tables, the executed code history view, and the command palette. What is Colab? Colab, or \"Colaboratory\", allows you to write and execute Python in your browser, with - Zero configuration required - Access to GPUs free of charge - Easy sharing Whether you're a student , a data scientist or an AI researcher , Colab can make your work easier. Watch Introduction to Colab to learn more, or just get started below! Getting started The document you are reading is not a static web page, but an interactive environment called a Colab notebook that lets you write and execute code. For example, here is a code cell with a short Python script that computes a value, stores it in a variable, and prints the result: seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day 86400 To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing. Variables that you define in one cell can later be used in other cells: seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week 604800 Colab notebooks allow you to combine executable code and rich text in a single document, along with images , HTML , LaTeX and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see Overview of Colab . To create a new Colab notebook you can use the File menu above, or use the following link: create a new Colab notebook . Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see jupyter.org . Data science With Colab you can harness the full power of popular Python libraries to analyze and visualize data. The code cell below uses numpy to generate some random data, and uses matplotlib to visualize it. To edit the code, just click the cell and start editing. import numpy as np from matplotlib import pyplot as plt ys = 200 + np.random.randn(100) x = [x for x in range(len(ys))] plt.plot(x, ys, '-') plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6) plt.title(\"Sample Visualization\") plt.show() You can import your own data into Colab notebooks from your Google Drive account, including from spreadsheets, as well as from Github and many other sources. To learn more about importing data, and how Colab can be used for data science, see the links below under Working with Data . Machine learning With Colab you can import an image dataset, train an image classifier on it, and evaluate the model, all in just a few lines of code . Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including GPUs and TPUs , regardless of the power of your machine. All you need is a browser. Colab is used extensively in the machine learning community with applications including: - Getting started with TensorFlow - Developing and training neural networks - Experimenting with TPUs - Disseminating AI research - Creating tutorials To see sample Colab notebooks that demonstrate machine learning applications, see the machine learning examples below. More Resources Working with Notebooks in Colab Overview of Colaboratory Guide to Markdown Importing libraries and installing dependencies Saving and loading notebooks in GitHub Interactive forms Interactive widgets TensorFlow 2 in Colab Working with Data Loading data: Drive, Sheets, and Google Cloud Storage Charts: visualizing data Getting started with BigQuery Machine Learning Crash Course These are a few of the notebooks from Google's online Machine Learning course. See the full course website for more. - Intro to Pandas DataFrame - Linear regression with tf.keras using synthetic data Using Accelerated Hardware TensorFlow with GPUs TensorFlow with TPUs Featured examples NeMo Voice Swap : Use Nvidia's NeMo conversational AI Toolkit to swap a voice in an audio fragment with a computer generated one. Retraining an Image Classifier : Build a Keras model on top of a pre-trained image classifier to distinguish flowers. Text Classification : Classify IMDB movie reviews as either positive or negative . Style Transfer : Use deep learning to transfer style between images. Multilingual Universal Sentence Encoder Q&A : Use a machine learning model to answer questions from the SQuAD dataset. Video Interpolation : Predict what happened in a video between the first and the last frame.","title":"Welcome To Colaboratory"},{"location":"Welcome_To_Colaboratory/#welcome-to-colab","text":"If you're already familiar with Colab, check out this video to learn about interactive tables, the executed code history view, and the command palette.","title":"Welcome to Colab!"},{"location":"Welcome_To_Colaboratory/#what-is-colab","text":"Colab, or \"Colaboratory\", allows you to write and execute Python in your browser, with - Zero configuration required - Access to GPUs free of charge - Easy sharing Whether you're a student , a data scientist or an AI researcher , Colab can make your work easier. Watch Introduction to Colab to learn more, or just get started below!","title":"What is Colab?"},{"location":"Welcome_To_Colaboratory/#getting-started","text":"The document you are reading is not a static web page, but an interactive environment called a Colab notebook that lets you write and execute code. For example, here is a code cell with a short Python script that computes a value, stores it in a variable, and prints the result: seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day 86400 To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing. Variables that you define in one cell can later be used in other cells: seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week 604800 Colab notebooks allow you to combine executable code and rich text in a single document, along with images , HTML , LaTeX and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see Overview of Colab . To create a new Colab notebook you can use the File menu above, or use the following link: create a new Colab notebook . Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see jupyter.org .","title":"Getting started"},{"location":"Welcome_To_Colaboratory/#data-science","text":"With Colab you can harness the full power of popular Python libraries to analyze and visualize data. The code cell below uses numpy to generate some random data, and uses matplotlib to visualize it. To edit the code, just click the cell and start editing. import numpy as np from matplotlib import pyplot as plt ys = 200 + np.random.randn(100) x = [x for x in range(len(ys))] plt.plot(x, ys, '-') plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6) plt.title(\"Sample Visualization\") plt.show() You can import your own data into Colab notebooks from your Google Drive account, including from spreadsheets, as well as from Github and many other sources. To learn more about importing data, and how Colab can be used for data science, see the links below under Working with Data .","title":"Data science"},{"location":"Welcome_To_Colaboratory/#machine-learning","text":"With Colab you can import an image dataset, train an image classifier on it, and evaluate the model, all in just a few lines of code . Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including GPUs and TPUs , regardless of the power of your machine. All you need is a browser. Colab is used extensively in the machine learning community with applications including: - Getting started with TensorFlow - Developing and training neural networks - Experimenting with TPUs - Disseminating AI research - Creating tutorials To see sample Colab notebooks that demonstrate machine learning applications, see the machine learning examples below.","title":"Machine learning"},{"location":"Welcome_To_Colaboratory/#more-resources","text":"","title":"More Resources"},{"location":"Welcome_To_Colaboratory/#working-with-notebooks-in-colab","text":"Overview of Colaboratory Guide to Markdown Importing libraries and installing dependencies Saving and loading notebooks in GitHub Interactive forms Interactive widgets TensorFlow 2 in Colab","title":"Working with Notebooks in Colab"},{"location":"Welcome_To_Colaboratory/#working-with-data","text":"Loading data: Drive, Sheets, and Google Cloud Storage Charts: visualizing data Getting started with BigQuery","title":"Working with Data"},{"location":"Welcome_To_Colaboratory/#machine-learning-crash-course","text":"These are a few of the notebooks from Google's online Machine Learning course. See the full course website for more. - Intro to Pandas DataFrame - Linear regression with tf.keras using synthetic data","title":"Machine Learning Crash Course"},{"location":"Welcome_To_Colaboratory/#using-accelerated-hardware","text":"TensorFlow with GPUs TensorFlow with TPUs","title":"Using Accelerated Hardware"},{"location":"Welcome_To_Colaboratory/#featured-examples","text":"NeMo Voice Swap : Use Nvidia's NeMo conversational AI Toolkit to swap a voice in an audio fragment with a computer generated one. Retraining an Image Classifier : Build a Keras model on top of a pre-trained image classifier to distinguish flowers. Text Classification : Classify IMDB movie reviews as either positive or negative . Style Transfer : Use deep learning to transfer style between images. Multilingual Universal Sentence Encoder Q&A : Use a machine learning model to answer questions from the SQuAD dataset. Video Interpolation : Predict what happened in a video between the first and the last frame.","title":"Featured examples"}]}