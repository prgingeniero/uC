{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"4_4_8_Flatbuffers/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); TensorFlow Lite Flatbuffer Manipulation Example It's possible to read, modify, and write TensorFlow Lite model files from Python. This notebook shows you how. Run in Google Colab View source on GitHub Software installation The goal is to build a set of Python classes that represent the types stored inside the TFL Flatbuffer files. To do this we need several dependencies: - The 'flatc' compiler, that converts the model format stored in a text schema to Python accessor classes. - The text schema itself, describing the model format. - The Flatbuffer Python library that the accessor classes rely on. The 'flatc' compiler isn't available as a binary download, so we need to build it from source. The version has to match the Flatbuffer Python library on the system, or the resulting generated code won't work, since it will be trying to use a different API. The FB Python library is at version 1.12.0, so we make sure we download the tagged snapshot of the source at that same version from GitHub. Install Flatbuffer Python Library We should already have version 1.12.0 by default on this Colab, but use pip to ensure it's installed, and then import it so it's available from Python. !pip install flatbuffers==1.12.0 import flatbuffers Build the 'flatc' Compiler We need 'flatc' to generate the Python accessor classes we'll use to read and write the serialized files, and since it's not easily available as a binary, we grab the source code from the right version and build it directly. This will take a few minutes. Once the 'flatc' binary has been built, copy it to the /usr/local/bin folder so that it's easily accessible as a command. # Build and install the Flatbuffer compiler. %cd /content/ !rm -rf flatbuffers* !curl -L \"https://github.com/google/flatbuffers/archive/v1.12.0.zip\" -o flatbuffers.zip !unzip -q flatbuffers.zip !mv flatbuffers-1.12.0 flatbuffers %cd flatbuffers !cmake -G \"Unix Makefiles\" -DCMAKE_BUILD_TYPE=Release !make -j 8 !cp flatc /usr/local/bin/ Fetch the Schema The schema is a text file that describes the layout of the model file format , and it's part of the TensorFlow source code, so we need to fetch the latest version from GitHub. %cd /content/ !rm -rf tensorflow %tensorflow_version 1.x !wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip !unzip v2.4.1.zip &> 0 !mv tensorflow-2.4.1/ tensorflow/ Generate the Python Accessor Classes The 'flatc' compiler takes the information from the text schema, and creates Python code to read and write the data held inside the serialized Flatbuffer file. The Python classes are written into the tflite folder, with names like Model.py , and define classes like ModelT that contain members that you can use to read or write the contents of the data structures defined by the schema. !flatc --python --gen-object-api tensorflow/tensorflow/lite/schema/schema_v3.fbs Model Loading and Saving These are some simple wrapper functions that demonstrate how to load data from a file, turn it into a ModelT Python object that can be modified, and then save it out to a new file. We have to do a little bit of hackery with the Python search paths so that the class files we generated from the schema can be imported. import sys # This hackery allows us to import the Python files we've just generated. sys.path.append(\"/content/tflite/\") import Model def load_model_from_file(model_filename): with open(model_filename, \"rb\") as file: buffer_data = file.read() model_obj = Model.Model.GetRootAsModel(buffer_data, 0) model = Model.ModelT.InitFromObj(model_obj) return model def save_model_to_file(model, model_filename): builder = flatbuffers.Builder(1024) model_offset = model.Pack(builder) builder.Finish(model_offset, file_identifier=b'TFL3') model_data = builder.Output() with open(model_filename, 'wb') as out_file: out_file.write(model_data) Download an Example Model This pulls down a trained model from the speech commands tutorial that we can use to test the Flatbuffer loading code. !curl -O 'https://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/speech_commands_model_2020_04_27.zip' !unzip speech_commands_model_2020_04_27.zip Load, Modify, and Save a Model The code below loads the float model, applies a small change to the float weights, and saves it out again. In this case we're just changing the contents of the weights, but any of the other properties of the model can be added, removed, or modified, including tensors, ops, and metadata. I've found the easiest way to understand the syntax used and data available is to look at the generated classes in tflite/ and the text schema that describes the format . # Use numpy to make the manipulation of the weight arrays easier. import numpy as np # Load the float model we downloaded as a ModelT object. model = load_model_from_file('/content/speech_commands_model/speech_commands_model_float.tflite') # Loop through all the weights held in the model. for buffer in model.buffers: # Skip missing or small weight arrays. if buffer.data is not None and len(buffer.data) > 1024: # Pull the weight array from the model, and cast it to 32-bit floats since # we know that's the type for all the weights in this model. In a real # application we'd need to check the data type from the tensor information # stored in the model.subgraphs original_weights = np.frombuffer(buffer.data, dtype=np.float32) # This is the line where the weights are altered. # Try replacing it with your own version, for example: # munged_weights = np.add(original_weights, 0.002) munged_weights = np.round(original_weights * (1/0.02)) * 0.02 # Write the altered data back into the model. buffer.data = munged_weights.tobytes() # Save the ModelT object as a TFL Flatbuffer file. save_model_to_file(model, '/content/speech_commands_model/speech_commands_model_modified.tflite') Evaluating the impact of your changes Setup KWS infrastructure To evaluate the impact of your changes on the accuracy of the speech model we need to load a test data set and some utility classes to read the files and convert them into the right input form for the network. The full dataset is several gigabytes in size, so it may take a few minutes to download. sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\") import input_data import models # A comma-delimited list of the words you want to train for. # The options are: yes,no,up,down,left,right,on,off,stop,go # All the other words will be used to train an \"unknown\" label and silent # audio data with no spoken words will be used to train a \"silence\" label. WANTED_WORDS = \"yes,no\" # The number of steps and learning rates can be specified as comma-separated # lists to define the rate at each stage. For example, # TRAINING_STEPS=12000,3000 and LEARNING_RATE=0.001,0.0001 # will run 12,000 training loops in total, with a rate of 0.001 for the first # 8,000, and 0.0001 for the final 3,000. TRAINING_STEPS = \"12000,3000\" LEARNING_RATE = \"0.001,0.0001\" # Calculate the total number of steps, which is used to identify the checkpoint # file name. TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\")))) # Calculate the percentage of 'silence' and 'unknown' training samples required # to ensure that we have equal number of samples for each label. number_of_labels = WANTED_WORDS.count(',') + 1 number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label equal_percentage_of_training_samples = int(100.0/(number_of_total_labels)) SILENT_PERCENTAGE = equal_percentage_of_training_samples UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples # Constants which are shared during training and inference PREPROCESS = 'micro' WINDOW_STRIDE =20 MODEL_ARCHITECTURE = 'tiny_conv' # Other options include: single_fc, conv, # low_latency_conv, low_latency_svdf, tiny_embedding_conv # Constants used during training only VERBOSITY = 'WARN' EVAL_STEP_INTERVAL = '1000' SAVE_STEP_INTERVAL = '1000' # Constants for training directories and filepaths DATASET_DIR = 'dataset/' LOGS_DIR = 'logs/' TRAIN_DIR = 'train/' # for training checkpoints and other files. SAMPLE_RATE = 16000 CLIP_DURATION_MS = 1000 WINDOW_SIZE_MS = 30.0 FEATURE_BIN_COUNT = 40 BACKGROUND_FREQUENCY = 0.8 BACKGROUND_VOLUME_RANGE = 0.1 TIME_SHIFT_MS = 100.0 DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz' VALIDATION_PERCENTAGE = 10 TESTING_PERCENTAGE = 10 model_settings = models.prepare_model_settings( len(input_data.prepare_words_list(WANTED_WORDS.split(','))), SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS, WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS) audio_processor = input_data.AudioProcessor( DATA_URL, DATASET_DIR, SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE, WANTED_WORDS.split(','), VALIDATION_PERCENTAGE, TESTING_PERCENTAGE, model_settings, LOGS_DIR) >> Downloading speech_commands_v0.02.tar.gz 100.0% INFO:tensorflow:Successfully downloaded speech_commands_v0.02.tar.gz (2428923189 bytes) import tensorflow as tf # define our helper function to test the model accuracy def test_model_accuracy(model_filename): with tf.compat.v1.Session() as sess: test_data, test_labels = audio_processor.get_data( -1, 0, model_settings, 0, 0, 0, 'testing', sess) interpreter = tf.lite.Interpreter(model_filename) interpreter.allocate_tensors() input_index = interpreter.get_input_details()[0][\"index\"] output_index = interpreter.get_output_details()[0][\"index\"] model_output = interpreter.tensor(output_index) correct_predictions = 0 for i in range(len(test_data)): current_input = test_data[i] current_label = test_labels[i] flattened_input = np.array(current_input.flatten(), dtype=np.float32).reshape(1, 1960) interpreter.set_tensor(input_index, flattened_input) interpreter.invoke() top_prediction = model_output()[0].argmax() if top_prediction == current_label: correct_predictions += 1 print('Accuracy is %f%% (N=%d)' % ((correct_predictions * 100) / len(test_data), len(test_data))) Test your Models Finally we can test both the standard and your modified models! Float model evaluation You should see ~91% accuracy with a 67KB model. Note: the exact accuracy may vary by a percent or two as it tests the model on a random sampling of the dataset and therefore sometimes gets particularly lucky or unlucky! !ls -lah /content/speech_commands_model/speech_commands_model_float.tflite test_model_accuracy('/content/speech_commands_model/speech_commands_model_float.tflite') Modified model evaluation Test the impact of your changes! !ls -lah /content/speech_commands_model/speech_commands_model_modified.tflite test_model_accuracy('/content/speech_commands_model/speech_commands_model_modified.tflite') If you'd like to try more modified models just scroll back up to the Load, Modify, and Save a Model section and re-run the modified model generation step and then skip back down to the above to re-test!","title":"4 4 8 Flatbuffers"},{"location":"4_4_8_Flatbuffers/#tensorflow-lite-flatbuffer-manipulation-example","text":"It's possible to read, modify, and write TensorFlow Lite model files from Python. This notebook shows you how. Run in Google Colab View source on GitHub","title":"TensorFlow Lite Flatbuffer Manipulation Example"},{"location":"4_4_8_Flatbuffers/#software-installation","text":"The goal is to build a set of Python classes that represent the types stored inside the TFL Flatbuffer files. To do this we need several dependencies: - The 'flatc' compiler, that converts the model format stored in a text schema to Python accessor classes. - The text schema itself, describing the model format. - The Flatbuffer Python library that the accessor classes rely on. The 'flatc' compiler isn't available as a binary download, so we need to build it from source. The version has to match the Flatbuffer Python library on the system, or the resulting generated code won't work, since it will be trying to use a different API. The FB Python library is at version 1.12.0, so we make sure we download the tagged snapshot of the source at that same version from GitHub.","title":"Software installation"},{"location":"4_4_8_Flatbuffers/#install-flatbuffer-python-library","text":"We should already have version 1.12.0 by default on this Colab, but use pip to ensure it's installed, and then import it so it's available from Python. !pip install flatbuffers==1.12.0 import flatbuffers","title":"Install Flatbuffer Python Library"},{"location":"4_4_8_Flatbuffers/#build-the-flatc-compiler","text":"We need 'flatc' to generate the Python accessor classes we'll use to read and write the serialized files, and since it's not easily available as a binary, we grab the source code from the right version and build it directly. This will take a few minutes. Once the 'flatc' binary has been built, copy it to the /usr/local/bin folder so that it's easily accessible as a command. # Build and install the Flatbuffer compiler. %cd /content/ !rm -rf flatbuffers* !curl -L \"https://github.com/google/flatbuffers/archive/v1.12.0.zip\" -o flatbuffers.zip !unzip -q flatbuffers.zip !mv flatbuffers-1.12.0 flatbuffers %cd flatbuffers !cmake -G \"Unix Makefiles\" -DCMAKE_BUILD_TYPE=Release !make -j 8 !cp flatc /usr/local/bin/","title":"Build the 'flatc' Compiler"},{"location":"4_4_8_Flatbuffers/#fetch-the-schema","text":"The schema is a text file that describes the layout of the model file format , and it's part of the TensorFlow source code, so we need to fetch the latest version from GitHub. %cd /content/ !rm -rf tensorflow %tensorflow_version 1.x !wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip !unzip v2.4.1.zip &> 0 !mv tensorflow-2.4.1/ tensorflow/","title":"Fetch the Schema"},{"location":"4_4_8_Flatbuffers/#generate-the-python-accessor-classes","text":"The 'flatc' compiler takes the information from the text schema, and creates Python code to read and write the data held inside the serialized Flatbuffer file. The Python classes are written into the tflite folder, with names like Model.py , and define classes like ModelT that contain members that you can use to read or write the contents of the data structures defined by the schema. !flatc --python --gen-object-api tensorflow/tensorflow/lite/schema/schema_v3.fbs","title":"Generate the Python Accessor Classes"},{"location":"4_4_8_Flatbuffers/#model-loading-and-saving","text":"These are some simple wrapper functions that demonstrate how to load data from a file, turn it into a ModelT Python object that can be modified, and then save it out to a new file. We have to do a little bit of hackery with the Python search paths so that the class files we generated from the schema can be imported. import sys # This hackery allows us to import the Python files we've just generated. sys.path.append(\"/content/tflite/\") import Model def load_model_from_file(model_filename): with open(model_filename, \"rb\") as file: buffer_data = file.read() model_obj = Model.Model.GetRootAsModel(buffer_data, 0) model = Model.ModelT.InitFromObj(model_obj) return model def save_model_to_file(model, model_filename): builder = flatbuffers.Builder(1024) model_offset = model.Pack(builder) builder.Finish(model_offset, file_identifier=b'TFL3') model_data = builder.Output() with open(model_filename, 'wb') as out_file: out_file.write(model_data)","title":"Model Loading and Saving"},{"location":"4_4_8_Flatbuffers/#download-an-example-model","text":"This pulls down a trained model from the speech commands tutorial that we can use to test the Flatbuffer loading code. !curl -O 'https://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/speech_commands_model_2020_04_27.zip' !unzip speech_commands_model_2020_04_27.zip","title":"Download an Example Model"},{"location":"4_4_8_Flatbuffers/#load-modify-and-save-a-model","text":"The code below loads the float model, applies a small change to the float weights, and saves it out again. In this case we're just changing the contents of the weights, but any of the other properties of the model can be added, removed, or modified, including tensors, ops, and metadata. I've found the easiest way to understand the syntax used and data available is to look at the generated classes in tflite/ and the text schema that describes the format . # Use numpy to make the manipulation of the weight arrays easier. import numpy as np # Load the float model we downloaded as a ModelT object. model = load_model_from_file('/content/speech_commands_model/speech_commands_model_float.tflite') # Loop through all the weights held in the model. for buffer in model.buffers: # Skip missing or small weight arrays. if buffer.data is not None and len(buffer.data) > 1024: # Pull the weight array from the model, and cast it to 32-bit floats since # we know that's the type for all the weights in this model. In a real # application we'd need to check the data type from the tensor information # stored in the model.subgraphs original_weights = np.frombuffer(buffer.data, dtype=np.float32) # This is the line where the weights are altered. # Try replacing it with your own version, for example: # munged_weights = np.add(original_weights, 0.002) munged_weights = np.round(original_weights * (1/0.02)) * 0.02 # Write the altered data back into the model. buffer.data = munged_weights.tobytes() # Save the ModelT object as a TFL Flatbuffer file. save_model_to_file(model, '/content/speech_commands_model/speech_commands_model_modified.tflite')","title":"Load, Modify, and Save a Model"},{"location":"4_4_8_Flatbuffers/#evaluating-the-impact-of-your-changes","text":"","title":"Evaluating the impact of your changes"},{"location":"4_4_8_Flatbuffers/#setup-kws-infrastructure","text":"To evaluate the impact of your changes on the accuracy of the speech model we need to load a test data set and some utility classes to read the files and convert them into the right input form for the network. The full dataset is several gigabytes in size, so it may take a few minutes to download. sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\") import input_data import models # A comma-delimited list of the words you want to train for. # The options are: yes,no,up,down,left,right,on,off,stop,go # All the other words will be used to train an \"unknown\" label and silent # audio data with no spoken words will be used to train a \"silence\" label. WANTED_WORDS = \"yes,no\" # The number of steps and learning rates can be specified as comma-separated # lists to define the rate at each stage. For example, # TRAINING_STEPS=12000,3000 and LEARNING_RATE=0.001,0.0001 # will run 12,000 training loops in total, with a rate of 0.001 for the first # 8,000, and 0.0001 for the final 3,000. TRAINING_STEPS = \"12000,3000\" LEARNING_RATE = \"0.001,0.0001\" # Calculate the total number of steps, which is used to identify the checkpoint # file name. TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\")))) # Calculate the percentage of 'silence' and 'unknown' training samples required # to ensure that we have equal number of samples for each label. number_of_labels = WANTED_WORDS.count(',') + 1 number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label equal_percentage_of_training_samples = int(100.0/(number_of_total_labels)) SILENT_PERCENTAGE = equal_percentage_of_training_samples UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples # Constants which are shared during training and inference PREPROCESS = 'micro' WINDOW_STRIDE =20 MODEL_ARCHITECTURE = 'tiny_conv' # Other options include: single_fc, conv, # low_latency_conv, low_latency_svdf, tiny_embedding_conv # Constants used during training only VERBOSITY = 'WARN' EVAL_STEP_INTERVAL = '1000' SAVE_STEP_INTERVAL = '1000' # Constants for training directories and filepaths DATASET_DIR = 'dataset/' LOGS_DIR = 'logs/' TRAIN_DIR = 'train/' # for training checkpoints and other files. SAMPLE_RATE = 16000 CLIP_DURATION_MS = 1000 WINDOW_SIZE_MS = 30.0 FEATURE_BIN_COUNT = 40 BACKGROUND_FREQUENCY = 0.8 BACKGROUND_VOLUME_RANGE = 0.1 TIME_SHIFT_MS = 100.0 DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz' VALIDATION_PERCENTAGE = 10 TESTING_PERCENTAGE = 10 model_settings = models.prepare_model_settings( len(input_data.prepare_words_list(WANTED_WORDS.split(','))), SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS, WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS) audio_processor = input_data.AudioProcessor( DATA_URL, DATASET_DIR, SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE, WANTED_WORDS.split(','), VALIDATION_PERCENTAGE, TESTING_PERCENTAGE, model_settings, LOGS_DIR) >> Downloading speech_commands_v0.02.tar.gz 100.0% INFO:tensorflow:Successfully downloaded speech_commands_v0.02.tar.gz (2428923189 bytes) import tensorflow as tf # define our helper function to test the model accuracy def test_model_accuracy(model_filename): with tf.compat.v1.Session() as sess: test_data, test_labels = audio_processor.get_data( -1, 0, model_settings, 0, 0, 0, 'testing', sess) interpreter = tf.lite.Interpreter(model_filename) interpreter.allocate_tensors() input_index = interpreter.get_input_details()[0][\"index\"] output_index = interpreter.get_output_details()[0][\"index\"] model_output = interpreter.tensor(output_index) correct_predictions = 0 for i in range(len(test_data)): current_input = test_data[i] current_label = test_labels[i] flattened_input = np.array(current_input.flatten(), dtype=np.float32).reshape(1, 1960) interpreter.set_tensor(input_index, flattened_input) interpreter.invoke() top_prediction = model_output()[0].argmax() if top_prediction == current_label: correct_predictions += 1 print('Accuracy is %f%% (N=%d)' % ((correct_predictions * 100) / len(test_data), len(test_data)))","title":"Setup KWS infrastructure"},{"location":"4_4_8_Flatbuffers/#test-your-models","text":"Finally we can test both the standard and your modified models! Float model evaluation You should see ~91% accuracy with a 67KB model. Note: the exact accuracy may vary by a percent or two as it tests the model on a random sampling of the dataset and therefore sometimes gets particularly lucky or unlucky! !ls -lah /content/speech_commands_model/speech_commands_model_float.tflite test_model_accuracy('/content/speech_commands_model/speech_commands_model_float.tflite') Modified model evaluation Test the impact of your changes! !ls -lah /content/speech_commands_model/speech_commands_model_modified.tflite test_model_accuracy('/content/speech_commands_model/speech_commands_model_modified.tflite') If you'd like to try more modified models just scroll back up to the Load, Modify, and Save a Model section and re-run the modified model generation step and then skip back down to the above to re-test!","title":"Test your Models"},{"location":"home/","text":"Deploying TinyML https://learning.edx.org/course/course-v1:HarvardX+TinyML3+1T2021/home C++ for Python Users C++ for Python Users If you are comfortable with C/C++, please feel free to move on to the next reading. If you are new to C++, we hope this introductory material will be helpful for you. If you are comfortable with C/C++, please feel free to move on to the next reading. If you are new to C++, we hope this introductory material will be helpful for you. Python, the language you have been using in all of your Colabs, is a dynamically-typed, \u201chigh-level\u201d language that is interpreted at runtime. C++ (also written as Cpp), on the other hand, is a statically-typed, \u201clow-level\u201d language that is pre-compiled before running, allowing for very compact code. The good news is that since we are using the Arduino platform, we won\u2019t have to deal with much of the complications of C++ as that is taken care of for us by the many libraries and board files we will be able to leverage (more on that soon). We\u2019ll then just have to pay attention to the changes in syntax which are more cosmetic than functional. That is, all of the main loops and conditional statements (e.g., for, if-else) remain the same functionally! In order to make sure you feel comfortable we\u2019ve put together a set of short appendix items and have collected some other good resources to walk you through the main changes between Python and C++ for Arudino which you can find below. As you go through the course, if you have good ideas for other introductory material, changes to the current material, or additional resources, please let us know and we\u2019d be very excited to add them to the list for future learners! For more information, take a look at our short appendix document that covers: Data Types Scope, Parentheses, and Semicolons Functions Libraries, Header Files, #include Other General Syntax Points Additional resources: The C++ Language Tutor The Google C++ Style Guide The Arduino Standard Library Language Reference Diversity of Embedded Systems Embedded Systems In this reading, we will explore the diversity of embedded systems. If you take a few minutes to look around the room you\u2019re in, you\u2019ll no doubt recognize the proliferation of embedded hardware within our world. Embedded systems are increasingly ubiquitous because they serve to complete specific computational tasks within their environment, allowing us to not only collect all sorts of data from distributed sensors but also to process the resulting information locally and act accordingly. That we can accomplish this in situ, or within the environment that this hardware is embedded within, at a fraction of the cost and physical scale of most general-purpose computing hardware, opens the door to many advancements in automation and generally to the creation of smart, connected things. While the specifications and capabilities of a given embedded system ought to be tailored for its application, in general, we can look to the common construct for an embedded system that links sensing to processing and later actuation through the process of transduction, the conversion of one form of energy into another, for throughlines. In sense, we convert energy from a physical phenomenon into an electrical signal we can go on to digitize and compute with. In actuation, the converse. In this course, we\u2019ll focus predominantly on the central element of said construct: the computing hardware, and review specifications and technical considerations that vary between implementations of embedded systems. An image of the Arduino Nano 33 BLE Sense noting the location of the IMU, USB port, Microphone, temperature + humidity sensors, and processor + bluetooth module. Development Boards In the context of this course, we have talked about and deployed a microcontroller unit (or MCU) development board from Arduino, the Nano 33 BLE Sense. Here, we distinguish the nRF52840 MCU from the development board it is soldered to. What\u2019s interesting about this board is that despite its relatively small size, it is jam-packed with many of the representative elements of an embedded system, all on one printed circuit board (PCB): a collection of surface-mount sensors, a form of an actuator in the on-board programmable RGB LED, and an integrated MCU-BLE module, where BLE is an acronym for a branch of the Bluetooth standard called Bluetooth Low Energy. Further still, the PCB that the MCU is soldered to serves to \u2018break out\u2019 additional IO from the controller to provide interfaces for external, off-board modules and connections to daughter boards. So, while in many ways, the Nano 33 BLE Sense is representative of an embedded system, it is separated from commercial implementations in that it serves no particular purpose, other than the open-ended development of an application that is. Put another way, a development board has the advantage of enabling many potential use cases. Further, you don\u2019t have to go through the process of designing circuitry, capturing schematics, or physical layout for such a board to get started prototyping a concept that could go on to be manufactured with specific intent later. The tradeoffs you make in employing a development board often involve board size and cost. Board Size Board size, and shape, have fairly meaningful implications for use in the field, given these simple parameters constrain where and how a system can be deployed. Some environments or contexts within which we\u2019d like to embed a system are forgiving (like the Google Home, say), but other manifestations (smart glasses, for example) depend wholly on the requisite hardware being quite tiny and perhaps of unique form. Somewhat obviously, board size is determined by the number and physical size, or package, of the components that live upon it. At a minimum, an embedded system must include a MCU chip alongside a power source, often a lithium polymer battery, and power circuitry. The nRF52840 MCU on the Nano 33 BLE Sense lives within a MCU-BLE module, the U-Blox NINA-B306, which spans 10 by 15 mm, dimensions that include a trace antenna. The MPM3610 step-down converter used to down-regulate the 5V delivered to the board over USB occupies 3 by 5 mm, alongside small SMD passives. The entire board, meanwhile, spans 18 by 45 mm. While impressive, clearly a purpose-designed PCB could remove some of the sensors and IO breakout unnecessary for a specific application to reduce the overall scale even further, perhaps by about 60 to 70%. Cost Unsurprisingly, the cost of a development board scales with its MCU\u2019s compute capability and general feature set. In selecting the appropriate MCU for an application, it is important to remember that MCUs are often deployed to complete specific tasks, where the complexity they will face is predetermined or constrained. For very simple tasks, we highlight the ATTINY85 (an 8-bit processor with 8 kB of flash) that can, depending on the package selected, cost well less than $1 EA and even less at scale, perfect for simple computing requirements. In the context of TinyML, the AI-capable NINA-B306 module featuring the Nordic Semiconductor nRF52840 chip (an ARM Cortex M4) and all-in-one BLE hardware (including antenna) costs about $10 EA and is more generally representative. In view of the on-board sensors and design costs, the Arduino Nano 33 BLE Sense costs just over $30, not even an hour's time of an electrical engineer who might design such a board \u2014 a tremendous value. In general, the boards we originally considered for this class span from $2 (BluePill) to $54 (Disco-F746NG). You can find a list of the boards we considered in the table below. Ultimately, our staff selected the Arduino Nano 33 BLE Sense for its versatility, in providing a large selection of on-board sensors, accessible IO via breakout pins, and a BLE module for projects that involve wireless communication, with reasonably representative compute specifications for resource-constrained hardware. We\u2019ll explore the compute specifications in a bit more detail in the next video and reading. TinyML Development Board Comparison \u25fdofficially TFLM supported \u25a2 unofficially compatible boards Board MCU CPU Clock Memory IO Sensor(s) Radio Arduino Nano 33 BLE Sense Nordic nRF52840 32-bit ARM Cortex-M4F 64 MHz 1 MB flash 256 kB RAM x8 12-bit ADCs x14 DIO UART, I2C, SPI Mic, IMU, temp, humidity, gesture, pressure, proximity, brightness, color BLE Espressif ESP32-DevKitC ESP32 D0WDQ6 32-bit, 2-core Xtensa LX6 240 MHz 4 MB flash 520 kB RAM x18 12-bit ADCs x34 DIO UART, I2C, SPI Hall effect, capacitive touch WiFi, BLE Espressif EYE ESP32 D0WD 32-bit, 2-core Xtensa LX6 240 MHz 4 MB flash 520 kB RAM SPI via surface pads Mic, camera WiFi, BLE Teensy 4.0 NXP iMXRT1062 32-bit ARM Cortex-M7 600 MHz 2 MB flash 1 MB RAM x14 10-bit ADCs x40 DIO** UART, I2C, SPI Internal temperature, capacitive touch None MAX32630FTHR Maxim MAX32620 32-bit ARM Cortex-M4F 96 MHz 2 MB flash 512 kB RAM x4 10-bit ADCs x16 DIO UART, I2C, SPI Accelerometer, gyroscope BLE This board also features 4 MB flash and 8 MB of PSRAM external to the MCU, shared programmable functions, **with external touchpads Board ASIC DSP Clock Memory IO Sensor(s) Radio Himax WiseEye WE-I Plus EVB HX6537-A 32-bit ARC EM9D DSP 400 MHz 2 MB flash 2 MB RAM x3 DIO I2C Mic, accelerometer, camera None We include the Himax WiseEye as an officially supported example of hardware optimization for TensorFlow Lite that calls on an application-specific integrated circuit (ASIC). Arduino Cores, Frameworks, mbedOS, and \u2018Bare Metal\u2019 Arduino Cores, Frameworks, mbedOS, and \u2018Bare Metal\u2019 In working with the Nano 33 BLE Sense, you have interacted with some of the underlying layers between your application and the board\u2019s physical hardware. Here, we quickly define some key terms (Arduino core, embedded frameworks, mbedOS, RTOS, and bare-metal) before discussing the implications of these concepts to your application. What is an Arduino Core? An Arduino \u201ccore\u201d is central to a particular microcontroller\u2019s compatibility with the Arduino framework. You can think of it as a low-level software API for a specific chip or family, like AVR or SAMD. Since each microcontroller has its own architecture, the concomitant core acts as an abstraction layer between your application and the physical hardware. In this context, libraries can be thought of as extensions to the core that are portable between chips or board variants for a particular chip. More granularly, each Arduino core contains a pair of generic files: arduino.h that handles declarations for what are often called \u2018built-in\u2019 functions like pinMode(), digitalWrite(), analogRead(), delay(), et cetera as well as macros for constants, like HIGH or INPUT_PULLUP, and operations like bitToggle() or abs(); main.cpp declares the basic program structure for a sketch, as it relates to setup() and loop(). We should note that arduino.h also sets the stage for board-specific pin mappings defined in pins_arduino.h at /variants/ . The remainder of a core is largely architecture-specific. There is a collection of C files with names wiring.c and wiring_ .c that define hardware initialization, timing, and functions related to IO of various types, like analogRead(). This nomenclature is a nod to the Wiring API that Arduino utilizes. There are also the header and C++ files for serial classes Stream, Print, and HardwareSerial, alongside a light USB stack. If you\u2019re interested in digging deeper into a specific core, check out this appendix document. What is an Embedded Framework? Loosely, to account for variation between manifestations, an embedded framework is a collection of development tools for microcontrollers that may or may not be portable and often serve as some combination of low-level API, software development kit (SDK), and hardware abstraction layer (HAL). For instance, the Arduino framework comprises its integrated development environment (IDE), built-in functions (defined by cores), as well as library extensions. As an open-source project, the Arduino framework is perhaps a bit more difficult to put bounds around, given its reliance on related projects, namely Wiring and its predecessor Processing, born out of the MIT Media Lab. Here are some other examples: CMSIS - Arm Cortex Microcontroller Software Interface Standard FreeRTOS - a real-time operating system (RTOS) kernel for embedded devices Mbed - an embedded RTOS for Internet-of-Things, drivers for I/O devices STM32Cube - HAL between STM32 devices and other libraries If you work with more than one framework, we recommend PlatformIO, an extension of Visual Studio Code, that enables you to call on many frameworks, with a unified workflow. What is an RTOS? Okay, so two of the examples we provide for embedded frameworks make mention of a real-time operating system or RTOS. In a circular definition, an RTOS is an operating system that serves real-time applications. More helpfully, an embedded RTOS is extremely light-weight system software that organizes the attention of microcontroller processing cores, often singular, to minimize unnecessary task switching and to prioritize time-sensitive computation so that the processing time required for a given task is ideally less than the interval to the next input. We won\u2019t dwell on the details here, but you can learn more about task scheduling, apparent multitasking (threads), and other RTOS fundamentals, here. What are Mbed and mbedOS? Mbed is a competing embedded framework to Arduino, with a fair bit of overlap. A glance at the \u2018What is Mbed?\u2019 section of their landing page illustrates this. Both are centered around a C++ API and feature IDEs, example code, libraries, and drivers for common components. Perhaps most striking is the difference in accessibility and other professional-grade considerations Mbed makes, like security. Notably, while mbedOS can be and usually is an RTOS, for applications that require thread management, there is also a limited \u2018bare metal\u2019 profile that you can utilize to cut down on the memory utilization tied to RTOS overhead. Why are we talking about mbedOS? Well, the Nano 33 BLE Sense runs on it. Here is an Arduino blog post from Martino Facchin, the head of Arduino\u2019s firmware development team, that explains the Nano 33 BLE Sense unique core, and its reliance on mbedOS. What is Bare Metal? You may happen upon variations of this definition, but at a high, inclusive level, \u2018bare metal\u2019 programming in an embedded systems context means utilizing microcontroller hardware independent of an operating system or scheduler, at the least. Typically, there is a base super loop, not unlike the Arduino sketch structure, and all processing is defined by your application. In many cases, this also means your code is written independently of pre-existing frameworks. This is where you\u2019ll find some variation in definitions. The colloquial spirit of bare-metal programming is counter to most developer mindsets, in that because microcontroller tasks are often simple but mission-critical, their developers want strict control over processing that precludes reliance on abstraction. A \u2018true\u2019 bare-metal project, then, will have a developer that either writes or, if supplemented by a light-weight framework, fully understands each layer, from the physical hardware on up, using an MCU/SoC datasheet as their guide to registering definitions and hardware architecture. Implications for your Application As you can imagine, your choice of the framework (or lack thereof) should stem from the needs of your application. If you have no hard time-constraints, an RTOS could be eating away at precious resources that may be constrained. However, there are obvious challenges to developing a bare-metal solution, which can largely be summarized by \u2018recreating known solutions.\u2019 If your task is simple, time-insensitive, mission-critical, or needs to be deployed on very constrained hardware, a bare-metal approach may be appropriate, assuming you have the knowledge and time required for the development. If you have a complex web of tasks in front of you that could be organized by a scheduler, an RTOS is really a no-brainer. In most cases, there is room for innovation in the middle, where you place trust in proven frameworks and libraries, but manage processing attention in a simple base loop, perhaps organized as a finite state machine or FSM. What is TensorFlow Lite for Microcontrollers? What is TF Lite Micro? As a future TinyML engineer, it is essential to understand the inner workings of the software you use to know its capabilities and limitations. So, in the upcoming series of videos and readings, you will learn more about the challenges that led to the development of TF Lite Micro, straight from the source. Pete Warden from Google, who leads the team that works on TF Lite Micro, will introduce TF Lite Micro and give us a sneak peek into its internal workings. Here is a preview of what is to come next. TensorFlow has become the most popular deep learning framework, superseding other popular frameworks such as PyTorch and Keras. TensorFlow, developed by Google, contains a Python frontend with highly optimized C++ code at its core, making it simple to program, fast, and efficient. The library has a large developer community and is now seen as the de facto standard for most machine learning applications. Despite this, TensorFlow is not suitable for every scenario. The standard TensorFlow library is ~400 MB in size, and even running a relatively small model (e.g., 200 MB) can take up a considerable amount of random access memory (> 1 GB). Such large storage and memory requirements make running simple models on lightweight systems largely intractable. Recognizing this issue, Google developed a more lightweight framework, TensorFlow Lite, also sometimes referred to as TensorFlow Mobile. The TFLite binary is approximately 1 MB in size, considerably more compact than the original library, making it possible to run deep learning models on mobile devices such as smartphones. This compression was achieved by removing superfluous functionality that is largely unnecessary for mobile deployment. While this is an improvement, our problem still remains: even TFLite is not suitable for every scenario. Many important deep learning applications exist at the microcontroller-level, which are significantly more resource-constrained than mobile devices, often equipped with less than 1 MB of storage and 256 KB RAM. Clearly, deploying TFLite models is not feasible for microcontrollers, so an alternative solution was needed. Enter TF Lite Micro. TF Lite Micro takes the compression of the TensorFlow library to the extreme, removing all but essential functionality. In fact, the core runtime of the library takes up only 16 KB, several orders of magnitude smaller than TFLite. With such a small memory footprint, this lightweight framework makes it possible to deploy deep learning models on the smallest of microcontrollers, such as an Arduino Nano. However, this is not without its complications. Deploying models with TF Lite Micro is fraught with new and unique challenges when building models. For example, since all functionality for plotting and debugging is removed, troubleshooting model issues is difficult. Additionally, since many microcontrollers do not have floating-point units or use 8-bit arithmetic, the model weights and activations must be suitably quantized on the microcontroller system. Since model training requires near-machine precision to perform gradient descent, this largely precludes on-device training. Thus, TF Lite Micro models must first be trained on a device with greater computational resources before being ported to the microcontroller, adding an additional stage to the machine learning workflow. Despite this, the benefits provided by TF Lite Micro - the ability to perform machine learning inference on microcontroller devices - far exceed the challenges, heralding a new era of machine learning that is often referred to as tiny machine learning. TensorFlow Lite Flatbuffer Manipulation Example In this Colab, we\u2019ll dig a little deeper into the TensorFlow Lite Flatbuffer file format. We\u2019ll load in the Flatbuffer library and the TensorFlow Lite Schema, which will allow us to access and directly modify the weights in the pretrained KWS model manually. You\u2019ll then get to see how your direct modifications impact the final model accuracy using the Speech Commands dataset! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/4-4-8-Flatbuffers.ipynb Summary of TFLM TFLite Micro Developer Design Principles There are four overarching design principles that TFMicro was built upon in order to address some of the challenges faced by developers when working with tinyML for embedded systems. This reading provides a synopsis of these core principles, as outlined in further detail in the TensorFlow Lite Micro paper. Principle 1: Minimize Feature Scope for Portability This principle proposes that an embedded machine learning (ML) framework should assume, by default, that the model, input data, and output arrays are in memory, and do not need to be loaded into memory. In addition, accessing peripherals, such as an on-device camera, should not be the job of the ML framework. These functions still need to be fulfilled, but principally should not be fulfilled by the ML framework. While this may seem unimportant, some microcontrollers do not have memory management (e.g. malloc) and other capabilities. Thus, trying to accommodate all varieties of platforms would bloat the library in an attempt to provide sufficient portability. Fortunately, due to the self-contained nature of machine learning models, the model can be run on-device without the need to access peripherals and system functions. Principle 2: Enable Vendor Contributions to Span Ecosystem Embedded devices come in all shapes and sizes, and require kernels to perform tinyML functions. The more optimized these kernels are for a particular device, the better performance will be achieved. However, because of the many differences between device platforms, there is no one-size-fits-all optimization solution. Consequently, the TFMicro team by itself is unable to support the wide variety of platforms that may want to run tinyML, and thus, vendors with strong motivation (i.e., those involved in microcontroller development) are encouraged to contribute to help bridge the gap. These vendors often have little experience with deep learning, and thus, TFMicro must provide sufficient resources to allow these teams to easily contribute. One way this is accomplished is by encouraging vendors to submit to a library repository and to provide tests and benchmarks for vendors to assess their hardware performance. Principle 3: Reuse TensorFlow Tools for Scalability The third principle focuses on scalability. More than 1,400 operations (e.g. CONV2D) are supported by TensorFlow and other machine learning training frameworks. However, inference frameworks (i.e., those actually deploying the model) typically only support a fraction of these operations. For most use-cases, this will likely not cause issues since the most commonly used operations will likely be supported, but this inherent difference leads to a mismatch between the set of potential models produced by the training framework and the set of potential models that can be deployed by the inference framework. An exporter is used to convert a model from a training framework, such as TensorFlow, to a model for an inference framework, such as TFLite or TFLite for Microcontrollers. This model can then be deployed directly to a device and run using the library interpreter. Often, the training and inference frameworks are developed by different entities, which can present difficulties for developers when there are compatibility issues between the various stages of the developmental pipeline. This may render otherwise functional models unusable when trying to be deployed to a client device, especially when the incompatibilities are abstracted in high-level libraries such as Keras. Due to these concerns, the TFMicro developers decided to reuse as many TensorFlow tools available as possible to help minimize such complications and compatibility issues. Principle 4: Build System for Heterogeneous Support The last principle focuses on promoting a flexible build environment. There are a large number of different types of embedded devices that may wish to use tinyML, and thus TFMicro should be designed without preference to any particular platform. This prevents vendor lock-in and also attracts a larger developer ecosystem due to improved portability. To combat this, TFMicro prioritizes code that can be built across a wide variety of integrated development environments (IDEs) and toolchains. These four principles help to facilitate a developer ecosystem that is oriented towards maximizing portability between various hardware platforms, architectures, frameworks, and toolchains.","title":"Deploying TinyML"},{"location":"home/#deploying-tinyml","text":"https://learning.edx.org/course/course-v1:HarvardX+TinyML3+1T2021/home","title":"Deploying TinyML"},{"location":"home/#c-for-python-users","text":"C++ for Python Users If you are comfortable with C/C++, please feel free to move on to the next reading. If you are new to C++, we hope this introductory material will be helpful for you. If you are comfortable with C/C++, please feel free to move on to the next reading. If you are new to C++, we hope this introductory material will be helpful for you. Python, the language you have been using in all of your Colabs, is a dynamically-typed, \u201chigh-level\u201d language that is interpreted at runtime. C++ (also written as Cpp), on the other hand, is a statically-typed, \u201clow-level\u201d language that is pre-compiled before running, allowing for very compact code. The good news is that since we are using the Arduino platform, we won\u2019t have to deal with much of the complications of C++ as that is taken care of for us by the many libraries and board files we will be able to leverage (more on that soon). We\u2019ll then just have to pay attention to the changes in syntax which are more cosmetic than functional. That is, all of the main loops and conditional statements (e.g., for, if-else) remain the same functionally! In order to make sure you feel comfortable we\u2019ve put together a set of short appendix items and have collected some other good resources to walk you through the main changes between Python and C++ for Arudino which you can find below. As you go through the course, if you have good ideas for other introductory material, changes to the current material, or additional resources, please let us know and we\u2019d be very excited to add them to the list for future learners! For more information, take a look at our short appendix document that covers: Data Types Scope, Parentheses, and Semicolons Functions Libraries, Header Files, #include Other General Syntax Points Additional resources: The C++ Language Tutor The Google C++ Style Guide The Arduino Standard Library Language Reference","title":"C++ for Python Users"},{"location":"home/#diversity-of-embedded-systems","text":"Embedded Systems In this reading, we will explore the diversity of embedded systems. If you take a few minutes to look around the room you\u2019re in, you\u2019ll no doubt recognize the proliferation of embedded hardware within our world. Embedded systems are increasingly ubiquitous because they serve to complete specific computational tasks within their environment, allowing us to not only collect all sorts of data from distributed sensors but also to process the resulting information locally and act accordingly. That we can accomplish this in situ, or within the environment that this hardware is embedded within, at a fraction of the cost and physical scale of most general-purpose computing hardware, opens the door to many advancements in automation and generally to the creation of smart, connected things. While the specifications and capabilities of a given embedded system ought to be tailored for its application, in general, we can look to the common construct for an embedded system that links sensing to processing and later actuation through the process of transduction, the conversion of one form of energy into another, for throughlines. In sense, we convert energy from a physical phenomenon into an electrical signal we can go on to digitize and compute with. In actuation, the converse. In this course, we\u2019ll focus predominantly on the central element of said construct: the computing hardware, and review specifications and technical considerations that vary between implementations of embedded systems. An image of the Arduino Nano 33 BLE Sense noting the location of the IMU, USB port, Microphone, temperature + humidity sensors, and processor + bluetooth module. Development Boards In the context of this course, we have talked about and deployed a microcontroller unit (or MCU) development board from Arduino, the Nano 33 BLE Sense. Here, we distinguish the nRF52840 MCU from the development board it is soldered to. What\u2019s interesting about this board is that despite its relatively small size, it is jam-packed with many of the representative elements of an embedded system, all on one printed circuit board (PCB): a collection of surface-mount sensors, a form of an actuator in the on-board programmable RGB LED, and an integrated MCU-BLE module, where BLE is an acronym for a branch of the Bluetooth standard called Bluetooth Low Energy. Further still, the PCB that the MCU is soldered to serves to \u2018break out\u2019 additional IO from the controller to provide interfaces for external, off-board modules and connections to daughter boards. So, while in many ways, the Nano 33 BLE Sense is representative of an embedded system, it is separated from commercial implementations in that it serves no particular purpose, other than the open-ended development of an application that is. Put another way, a development board has the advantage of enabling many potential use cases. Further, you don\u2019t have to go through the process of designing circuitry, capturing schematics, or physical layout for such a board to get started prototyping a concept that could go on to be manufactured with specific intent later. The tradeoffs you make in employing a development board often involve board size and cost. Board Size Board size, and shape, have fairly meaningful implications for use in the field, given these simple parameters constrain where and how a system can be deployed. Some environments or contexts within which we\u2019d like to embed a system are forgiving (like the Google Home, say), but other manifestations (smart glasses, for example) depend wholly on the requisite hardware being quite tiny and perhaps of unique form. Somewhat obviously, board size is determined by the number and physical size, or package, of the components that live upon it. At a minimum, an embedded system must include a MCU chip alongside a power source, often a lithium polymer battery, and power circuitry. The nRF52840 MCU on the Nano 33 BLE Sense lives within a MCU-BLE module, the U-Blox NINA-B306, which spans 10 by 15 mm, dimensions that include a trace antenna. The MPM3610 step-down converter used to down-regulate the 5V delivered to the board over USB occupies 3 by 5 mm, alongside small SMD passives. The entire board, meanwhile, spans 18 by 45 mm. While impressive, clearly a purpose-designed PCB could remove some of the sensors and IO breakout unnecessary for a specific application to reduce the overall scale even further, perhaps by about 60 to 70%. Cost Unsurprisingly, the cost of a development board scales with its MCU\u2019s compute capability and general feature set. In selecting the appropriate MCU for an application, it is important to remember that MCUs are often deployed to complete specific tasks, where the complexity they will face is predetermined or constrained. For very simple tasks, we highlight the ATTINY85 (an 8-bit processor with 8 kB of flash) that can, depending on the package selected, cost well less than $1 EA and even less at scale, perfect for simple computing requirements. In the context of TinyML, the AI-capable NINA-B306 module featuring the Nordic Semiconductor nRF52840 chip (an ARM Cortex M4) and all-in-one BLE hardware (including antenna) costs about $10 EA and is more generally representative. In view of the on-board sensors and design costs, the Arduino Nano 33 BLE Sense costs just over $30, not even an hour's time of an electrical engineer who might design such a board \u2014 a tremendous value. In general, the boards we originally considered for this class span from $2 (BluePill) to $54 (Disco-F746NG). You can find a list of the boards we considered in the table below. Ultimately, our staff selected the Arduino Nano 33 BLE Sense for its versatility, in providing a large selection of on-board sensors, accessible IO via breakout pins, and a BLE module for projects that involve wireless communication, with reasonably representative compute specifications for resource-constrained hardware. We\u2019ll explore the compute specifications in a bit more detail in the next video and reading. TinyML Development Board Comparison \u25fdofficially TFLM supported \u25a2 unofficially compatible boards Board MCU CPU Clock Memory IO Sensor(s) Radio Arduino Nano 33 BLE Sense Nordic nRF52840 32-bit ARM Cortex-M4F 64 MHz 1 MB flash 256 kB RAM x8 12-bit ADCs x14 DIO UART, I2C, SPI Mic, IMU, temp, humidity, gesture, pressure, proximity, brightness, color BLE Espressif ESP32-DevKitC ESP32 D0WDQ6 32-bit, 2-core Xtensa LX6 240 MHz 4 MB flash 520 kB RAM x18 12-bit ADCs x34 DIO UART, I2C, SPI Hall effect, capacitive touch WiFi, BLE Espressif EYE ESP32 D0WD 32-bit, 2-core Xtensa LX6 240 MHz 4 MB flash 520 kB RAM SPI via surface pads Mic, camera WiFi, BLE Teensy 4.0 NXP iMXRT1062 32-bit ARM Cortex-M7 600 MHz 2 MB flash 1 MB RAM x14 10-bit ADCs x40 DIO** UART, I2C, SPI Internal temperature, capacitive touch None MAX32630FTHR Maxim MAX32620 32-bit ARM Cortex-M4F 96 MHz 2 MB flash 512 kB RAM x4 10-bit ADCs x16 DIO UART, I2C, SPI Accelerometer, gyroscope BLE This board also features 4 MB flash and 8 MB of PSRAM external to the MCU, shared programmable functions, **with external touchpads Board ASIC DSP Clock Memory IO Sensor(s) Radio Himax WiseEye WE-I Plus EVB HX6537-A 32-bit ARC EM9D DSP 400 MHz 2 MB flash 2 MB RAM x3 DIO I2C Mic, accelerometer, camera None We include the Himax WiseEye as an officially supported example of hardware optimization for TensorFlow Lite that calls on an application-specific integrated circuit (ASIC).","title":"Diversity of Embedded Systems"},{"location":"home/#arduino-cores-frameworks-mbedos-and-bare-metal","text":"Arduino Cores, Frameworks, mbedOS, and \u2018Bare Metal\u2019 In working with the Nano 33 BLE Sense, you have interacted with some of the underlying layers between your application and the board\u2019s physical hardware. Here, we quickly define some key terms (Arduino core, embedded frameworks, mbedOS, RTOS, and bare-metal) before discussing the implications of these concepts to your application. What is an Arduino Core? An Arduino \u201ccore\u201d is central to a particular microcontroller\u2019s compatibility with the Arduino framework. You can think of it as a low-level software API for a specific chip or family, like AVR or SAMD. Since each microcontroller has its own architecture, the concomitant core acts as an abstraction layer between your application and the physical hardware. In this context, libraries can be thought of as extensions to the core that are portable between chips or board variants for a particular chip. More granularly, each Arduino core contains a pair of generic files: arduino.h that handles declarations for what are often called \u2018built-in\u2019 functions like pinMode(), digitalWrite(), analogRead(), delay(), et cetera as well as macros for constants, like HIGH or INPUT_PULLUP, and operations like bitToggle() or abs(); main.cpp declares the basic program structure for a sketch, as it relates to setup() and loop(). We should note that arduino.h also sets the stage for board-specific pin mappings defined in pins_arduino.h at /variants/ . The remainder of a core is largely architecture-specific. There is a collection of C files with names wiring.c and wiring_ .c that define hardware initialization, timing, and functions related to IO of various types, like analogRead(). This nomenclature is a nod to the Wiring API that Arduino utilizes. There are also the header and C++ files for serial classes Stream, Print, and HardwareSerial, alongside a light USB stack. If you\u2019re interested in digging deeper into a specific core, check out this appendix document. What is an Embedded Framework? Loosely, to account for variation between manifestations, an embedded framework is a collection of development tools for microcontrollers that may or may not be portable and often serve as some combination of low-level API, software development kit (SDK), and hardware abstraction layer (HAL). For instance, the Arduino framework comprises its integrated development environment (IDE), built-in functions (defined by cores), as well as library extensions. As an open-source project, the Arduino framework is perhaps a bit more difficult to put bounds around, given its reliance on related projects, namely Wiring and its predecessor Processing, born out of the MIT Media Lab. Here are some other examples: CMSIS - Arm Cortex Microcontroller Software Interface Standard FreeRTOS - a real-time operating system (RTOS) kernel for embedded devices Mbed - an embedded RTOS for Internet-of-Things, drivers for I/O devices STM32Cube - HAL between STM32 devices and other libraries If you work with more than one framework, we recommend PlatformIO, an extension of Visual Studio Code, that enables you to call on many frameworks, with a unified workflow. What is an RTOS? Okay, so two of the examples we provide for embedded frameworks make mention of a real-time operating system or RTOS. In a circular definition, an RTOS is an operating system that serves real-time applications. More helpfully, an embedded RTOS is extremely light-weight system software that organizes the attention of microcontroller processing cores, often singular, to minimize unnecessary task switching and to prioritize time-sensitive computation so that the processing time required for a given task is ideally less than the interval to the next input. We won\u2019t dwell on the details here, but you can learn more about task scheduling, apparent multitasking (threads), and other RTOS fundamentals, here. What are Mbed and mbedOS? Mbed is a competing embedded framework to Arduino, with a fair bit of overlap. A glance at the \u2018What is Mbed?\u2019 section of their landing page illustrates this. Both are centered around a C++ API and feature IDEs, example code, libraries, and drivers for common components. Perhaps most striking is the difference in accessibility and other professional-grade considerations Mbed makes, like security. Notably, while mbedOS can be and usually is an RTOS, for applications that require thread management, there is also a limited \u2018bare metal\u2019 profile that you can utilize to cut down on the memory utilization tied to RTOS overhead. Why are we talking about mbedOS? Well, the Nano 33 BLE Sense runs on it. Here is an Arduino blog post from Martino Facchin, the head of Arduino\u2019s firmware development team, that explains the Nano 33 BLE Sense unique core, and its reliance on mbedOS. What is Bare Metal? You may happen upon variations of this definition, but at a high, inclusive level, \u2018bare metal\u2019 programming in an embedded systems context means utilizing microcontroller hardware independent of an operating system or scheduler, at the least. Typically, there is a base super loop, not unlike the Arduino sketch structure, and all processing is defined by your application. In many cases, this also means your code is written independently of pre-existing frameworks. This is where you\u2019ll find some variation in definitions. The colloquial spirit of bare-metal programming is counter to most developer mindsets, in that because microcontroller tasks are often simple but mission-critical, their developers want strict control over processing that precludes reliance on abstraction. A \u2018true\u2019 bare-metal project, then, will have a developer that either writes or, if supplemented by a light-weight framework, fully understands each layer, from the physical hardware on up, using an MCU/SoC datasheet as their guide to registering definitions and hardware architecture. Implications for your Application As you can imagine, your choice of the framework (or lack thereof) should stem from the needs of your application. If you have no hard time-constraints, an RTOS could be eating away at precious resources that may be constrained. However, there are obvious challenges to developing a bare-metal solution, which can largely be summarized by \u2018recreating known solutions.\u2019 If your task is simple, time-insensitive, mission-critical, or needs to be deployed on very constrained hardware, a bare-metal approach may be appropriate, assuming you have the knowledge and time required for the development. If you have a complex web of tasks in front of you that could be organized by a scheduler, an RTOS is really a no-brainer. In most cases, there is room for innovation in the middle, where you place trust in proven frameworks and libraries, but manage processing attention in a simple base loop, perhaps organized as a finite state machine or FSM.","title":"Arduino Cores, Frameworks, mbedOS, and \u2018Bare Metal\u2019"},{"location":"home/#what-is-tensorflow-lite-for-microcontrollers","text":"What is TF Lite Micro? As a future TinyML engineer, it is essential to understand the inner workings of the software you use to know its capabilities and limitations. So, in the upcoming series of videos and readings, you will learn more about the challenges that led to the development of TF Lite Micro, straight from the source. Pete Warden from Google, who leads the team that works on TF Lite Micro, will introduce TF Lite Micro and give us a sneak peek into its internal workings. Here is a preview of what is to come next. TensorFlow has become the most popular deep learning framework, superseding other popular frameworks such as PyTorch and Keras. TensorFlow, developed by Google, contains a Python frontend with highly optimized C++ code at its core, making it simple to program, fast, and efficient. The library has a large developer community and is now seen as the de facto standard for most machine learning applications. Despite this, TensorFlow is not suitable for every scenario. The standard TensorFlow library is ~400 MB in size, and even running a relatively small model (e.g., 200 MB) can take up a considerable amount of random access memory (> 1 GB). Such large storage and memory requirements make running simple models on lightweight systems largely intractable. Recognizing this issue, Google developed a more lightweight framework, TensorFlow Lite, also sometimes referred to as TensorFlow Mobile. The TFLite binary is approximately 1 MB in size, considerably more compact than the original library, making it possible to run deep learning models on mobile devices such as smartphones. This compression was achieved by removing superfluous functionality that is largely unnecessary for mobile deployment. While this is an improvement, our problem still remains: even TFLite is not suitable for every scenario. Many important deep learning applications exist at the microcontroller-level, which are significantly more resource-constrained than mobile devices, often equipped with less than 1 MB of storage and 256 KB RAM. Clearly, deploying TFLite models is not feasible for microcontrollers, so an alternative solution was needed. Enter TF Lite Micro. TF Lite Micro takes the compression of the TensorFlow library to the extreme, removing all but essential functionality. In fact, the core runtime of the library takes up only 16 KB, several orders of magnitude smaller than TFLite. With such a small memory footprint, this lightweight framework makes it possible to deploy deep learning models on the smallest of microcontrollers, such as an Arduino Nano. However, this is not without its complications. Deploying models with TF Lite Micro is fraught with new and unique challenges when building models. For example, since all functionality for plotting and debugging is removed, troubleshooting model issues is difficult. Additionally, since many microcontrollers do not have floating-point units or use 8-bit arithmetic, the model weights and activations must be suitably quantized on the microcontroller system. Since model training requires near-machine precision to perform gradient descent, this largely precludes on-device training. Thus, TF Lite Micro models must first be trained on a device with greater computational resources before being ported to the microcontroller, adding an additional stage to the machine learning workflow. Despite this, the benefits provided by TF Lite Micro - the ability to perform machine learning inference on microcontroller devices - far exceed the challenges, heralding a new era of machine learning that is often referred to as tiny machine learning.","title":"What is TensorFlow Lite for Microcontrollers?"},{"location":"home/#tensorflow-lite-flatbuffer-manipulation-example","text":"In this Colab, we\u2019ll dig a little deeper into the TensorFlow Lite Flatbuffer file format. We\u2019ll load in the Flatbuffer library and the TensorFlow Lite Schema, which will allow us to access and directly modify the weights in the pretrained KWS model manually. You\u2019ll then get to see how your direct modifications impact the final model accuracy using the Speech Commands dataset! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/4-4-8-Flatbuffers.ipynb","title":"TensorFlow Lite Flatbuffer Manipulation Example"},{"location":"home/#summary-of-tflm","text":"TFLite Micro Developer Design Principles There are four overarching design principles that TFMicro was built upon in order to address some of the challenges faced by developers when working with tinyML for embedded systems. This reading provides a synopsis of these core principles, as outlined in further detail in the TensorFlow Lite Micro paper. Principle 1: Minimize Feature Scope for Portability This principle proposes that an embedded machine learning (ML) framework should assume, by default, that the model, input data, and output arrays are in memory, and do not need to be loaded into memory. In addition, accessing peripherals, such as an on-device camera, should not be the job of the ML framework. These functions still need to be fulfilled, but principally should not be fulfilled by the ML framework. While this may seem unimportant, some microcontrollers do not have memory management (e.g. malloc) and other capabilities. Thus, trying to accommodate all varieties of platforms would bloat the library in an attempt to provide sufficient portability. Fortunately, due to the self-contained nature of machine learning models, the model can be run on-device without the need to access peripherals and system functions. Principle 2: Enable Vendor Contributions to Span Ecosystem Embedded devices come in all shapes and sizes, and require kernels to perform tinyML functions. The more optimized these kernels are for a particular device, the better performance will be achieved. However, because of the many differences between device platforms, there is no one-size-fits-all optimization solution. Consequently, the TFMicro team by itself is unable to support the wide variety of platforms that may want to run tinyML, and thus, vendors with strong motivation (i.e., those involved in microcontroller development) are encouraged to contribute to help bridge the gap. These vendors often have little experience with deep learning, and thus, TFMicro must provide sufficient resources to allow these teams to easily contribute. One way this is accomplished is by encouraging vendors to submit to a library repository and to provide tests and benchmarks for vendors to assess their hardware performance. Principle 3: Reuse TensorFlow Tools for Scalability The third principle focuses on scalability. More than 1,400 operations (e.g. CONV2D) are supported by TensorFlow and other machine learning training frameworks. However, inference frameworks (i.e., those actually deploying the model) typically only support a fraction of these operations. For most use-cases, this will likely not cause issues since the most commonly used operations will likely be supported, but this inherent difference leads to a mismatch between the set of potential models produced by the training framework and the set of potential models that can be deployed by the inference framework. An exporter is used to convert a model from a training framework, such as TensorFlow, to a model for an inference framework, such as TFLite or TFLite for Microcontrollers. This model can then be deployed directly to a device and run using the library interpreter. Often, the training and inference frameworks are developed by different entities, which can present difficulties for developers when there are compatibility issues between the various stages of the developmental pipeline. This may render otherwise functional models unusable when trying to be deployed to a client device, especially when the incompatibilities are abstracted in high-level libraries such as Keras. Due to these concerns, the TFMicro developers decided to reuse as many TensorFlow tools available as possible to help minimize such complications and compatibility issues. Principle 4: Build System for Heterogeneous Support The last principle focuses on promoting a flexible build environment. There are a large number of different types of embedded devices that may wish to use tinyML, and thus TFMicro should be designed without preference to any particular platform. This prevents vendor lock-in and also attracts a larger developer ecosystem due to improved portability. To combat this, TFMicro prioritizes code that can be built across a wide variety of integrated development environments (IDEs) and toolchains. These four principles help to facilitate a developer ecosystem that is oriented towards maximizing portability between various hardware platforms, architectures, frameworks, and toolchains.","title":"Summary of TFLM"}]}