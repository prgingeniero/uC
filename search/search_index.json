{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Programacion https://learning.edx.org/course/course-v1:HarvardX+TinyML1+3T2020/home Fundamentals of TinyML As mentioned earlier, we\u2019ll be using the Colab environment for much of our programming in this specialization. In order to ensure that everyone has some hands on experience with Colab, please follow the link below to Google\u2019s Introductory Colab. In it they provide a handful of quick tips and hands on exercises to get you started using the Colab environment. https://colab.research.google.com/notebooks/intro.ipynb","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#fundamentals-of-tinyml","text":"As mentioned earlier, we\u2019ll be using the Colab environment for much of our programming in this specialization. In order to ensure that everyone has some hands on experience with Colab, please follow the link below to Google\u2019s Introductory Colab. In it they provide a handful of quick tips and hands on exercises to get you started using the Colab environment. https://colab.research.google.com/notebooks/intro.ipynb","title":"Fundamentals of TinyML"},{"location":"Linear_Regression_with_Synthetic_Data/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); #@title Copyright 2020 Google LLC. Double-click here for license information. # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Simple Linear Regression with Synthetic Data In this first Colab, you'll explore linear regression with a simple database. Learning objectives: After doing this exercise, you'll know how to do the following: Run Colabs. Tune the following hyperparameters : learning rate number of epochs batch size Interpret different kinds of loss curves . About Colabs Machine Learning Crash Course uses Colaboratories ( Colabs ) for all programming exercises. Colab is Google's implementation of Jupyter Notebook . Like all Jupyter Notebooks, a Colab consists of two kinds of components: Text cells , which contain explanations. You are currently reading a text cell. Code cells , which contain Python code for you to run. Code cells have a light gray background. You read the text cells and run the code cells. Running code cells You must run code cells in order. In other words, you may only run a code cell once all the code cells preceding it have already been run. To run a code cell: Place the cursor anywhere inside the [ ] area at the top left of a code cell. The area inside the [ ] will display an arrow. Click the arrow. Alternatively, you may invoke Runtime->Run all . Note, though, that some of the code cells will fail because not all the coding is complete. (You'll complete the coding as part of the exercise.) Understanding hidden code cells We've hidden the code in code cells that don't advance the learning objectives. For example, we've hidden the code that plots graphs. However, you must still run code cells containing hidden code . You'll know that the code is hidden because you'll see a title (for example, \"Load the functions that build and train a model\") without seeing the code. To view the hidden code, just double click the header. Why did you see an error? If a code cell returns an error when you run it, consider two common problems: You didn't run all of the code cells preceding the current code cell. If the code cell is labeled as a Task , then you haven't written the necessary code. Use the right version of TensorFlow The following hidden code cell ensures that the Colab will run on TensorFlow 2.X, which is the most recent version of TensorFlow: #@title Run this Colab on TensorFlow 2.x %tensorflow_version 2.x Import relevant modules The following cell imports the packages that the program requires: import pandas as pd import tensorflow as tf from matplotlib import pyplot as plt Define functions that build and train a model The following code defines two functions: build_model(my_learning_rate) , which builds an empty model. train_model(model, feature, label, epochs) , which trains the model from the examples (feature and label) you pass. Since you don't need to understand model building code right now, we've hidden this code cell. You may optionally double-click the headline to explore this code. #@title Define the functions that build and train a model def build_model(my_learning_rate): \"\"\"Create and compile a simple linear regression model.\"\"\" # Most simple tf.keras models are sequential. # A sequential model contains one or more layers. model = tf.keras.models.Sequential() # Describe the topography of the model. # The topography of a simple linear regression model # is a single node in a single layer. model.add(tf.keras.layers.Dense(units=1, input_shape=(1,))) # Compile the model topography into code that # TensorFlow can efficiently execute. Configure # training to minimize the model's mean squared error. model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()]) return model def train_model(model, feature, label, epochs, batch_size): \"\"\"Train the model by feeding it data.\"\"\" # Feed the feature values and the label values to the # model. The model will train for the specified number # of epochs, gradually learning how the feature values # relate to the label values. history = model.fit(x=feature, y=label, batch_size=batch_size, epochs=epochs) # Gather the trained model's weight and bias. trained_weight = model.get_weights()[0] trained_bias = model.get_weights()[1] # The list of epochs is stored separately from the # rest of history. epochs = history.epoch # Gather the history (a snapshot) of each epoch. hist = pd.DataFrame(history.history) # Specifically gather the model's root mean #squared error at each epoch. rmse = hist[\"root_mean_squared_error\"] return trained_weight, trained_bias, epochs, rmse print(\"Defined create_model and train_model\") Define plotting functions We're using a popular Python library called Matplotlib to create the following two plots: a plot of the feature values vs. the label values, and a line showing the output of the trained model. a loss curve . We hid the following code cell because learning Matplotlib is not relevant to the learning objectives. Regardless, you must still run all hidden code cells. #@title Define the plotting functions def plot_the_model(trained_weight, trained_bias, feature, label): \"\"\"Plot the trained model against the training feature and label.\"\"\" # Label the axes. plt.xlabel(\"feature\") plt.ylabel(\"label\") # Plot the feature values vs. label values. plt.scatter(feature, label) # Create a red line representing the model. The red line starts # at coordinates (x0, y0) and ends at coordinates (x1, y1). x0 = 0 y0 = trained_bias x1 = feature[-1] y1 = trained_bias + (trained_weight * x1) plt.plot([x0, x1], [y0, y1], c='r') # Render the scatter plot and the red line. plt.show() def plot_the_loss_curve(epochs, rmse): \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\" plt.figure() plt.xlabel(\"Epoch\") plt.ylabel(\"Root Mean Squared Error\") plt.plot(epochs, rmse, label=\"Loss\") plt.legend() plt.ylim([rmse.min()*0.97, rmse.max()]) plt.show() print(\"Defined the plot_the_model and plot_the_loss_curve functions.\") Define the dataset The dataset consists of 12 examples . Each example consists of one feature and one label . my_feature = ([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]) my_label = ([5.0, 8.8, 9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2]) Specify the hyperparameters The hyperparameters in this Colab are as follows: learning rate epochs batch_size The following code cell initializes these hyperparameters and then invokes the functions that build and train the model. learning_rate=0.01 epochs=10 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Task 1: Examine the graphs Examine the top graph. The blue dots identify the actual data; the red line identifies the output of the trained model. Ideally, the red line should align nicely with the blue dots. Does it? Probably not. A certain amount of randomness plays into training a model, so you'll get somewhat different results every time you train. That said, unless you are an extremely lucky person, the red line probably doesn't align nicely with the blue dots. Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently. Task 2: Increase the number of epochs Training loss should steadily decrease, steeply at first, and then more slowly. Eventually, training loss should eventually stay steady (zero slope or nearly zero slope), which indicates that training has converged . In Task 1, the training loss did not converge. One possible solution is to train for more epochs. Your task is to increase the number of epochs sufficiently to get the model to converge. However, it is inefficient to train past convergence, so don't just set the number of epochs to an arbitrarily high value. Examine the loss curve. Does the model converge? learning_rate=0.01 epochs= ? # Replace ? with an integer. my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.01 epochs=450 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) # The loss curve suggests that the model does converge. Task 3: Increase the learning rate In Task 2, you increased the number of epochs to get the model to converge. Sometimes, you can get the model to converge more quickly by increasing the learning rate. However, setting the learning rate too high often makes it impossible for a model to converge. In Task 3, we've intentionally set the learning rate too high. Run the following code cell and see what happens. # Increase the learning rate and decrease the number of epochs. learning_rate=100 epochs=500 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) The resulting model is terrible; the red line doesn't align with the blue dots. Furthermore, the loss curve oscillates like a roller coaster . An oscillating loss curve strongly suggests that the learning rate is too high. Task 4: Find the ideal combination of epochs and learning rate Assign values to the following two hyperparameters to make training converge as efficiently as possible: learning_rate epochs # Set the learning rate and number of epochs learning_rate= ? # Replace ? with a floating-point number epochs= ? # Replace ? with an integer my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.14 epochs=70 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Task 5: Adjust the batch size The system recalculates the model's loss value and adjusts the model's weights and bias after each iteration . Each iteration is the span in which the system processes one batch. For example, if the batch size is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples. One epoch spans sufficient iterations to process every example in the dataset. For example, if the batch size is 12, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations. It is tempting to simply set the batch size to the number of examples in the dataset (12, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. Experiment with batch_size in the following code cell. What's the smallest integer you can set for batch_size and still have the model converge in a hundred epochs? learning_rate=0.05 epochs=100 my_batch_size= ? # Replace ? with an integer. my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.05 epochs=125 my_batch_size=1 # Wow, a batch size of 1 works! my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Summary of hyperparameter tuning Most machine learning problems require a lot of hyperparameter tuning. Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb: Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. If the training loss does not converge, train for more epochs. If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging. If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate. Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination. Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation. For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.","title":"Linear Regression with Synthetic Data"},{"location":"Linear_Regression_with_Synthetic_Data/#simple-linear-regression-with-synthetic-data","text":"In this first Colab, you'll explore linear regression with a simple database.","title":"Simple Linear Regression with Synthetic Data"},{"location":"Linear_Regression_with_Synthetic_Data/#learning-objectives","text":"After doing this exercise, you'll know how to do the following: Run Colabs. Tune the following hyperparameters : learning rate number of epochs batch size Interpret different kinds of loss curves .","title":"Learning objectives:"},{"location":"Linear_Regression_with_Synthetic_Data/#about-colabs","text":"Machine Learning Crash Course uses Colaboratories ( Colabs ) for all programming exercises. Colab is Google's implementation of Jupyter Notebook . Like all Jupyter Notebooks, a Colab consists of two kinds of components: Text cells , which contain explanations. You are currently reading a text cell. Code cells , which contain Python code for you to run. Code cells have a light gray background. You read the text cells and run the code cells.","title":"About Colabs"},{"location":"Linear_Regression_with_Synthetic_Data/#running-code-cells","text":"You must run code cells in order. In other words, you may only run a code cell once all the code cells preceding it have already been run. To run a code cell: Place the cursor anywhere inside the [ ] area at the top left of a code cell. The area inside the [ ] will display an arrow. Click the arrow. Alternatively, you may invoke Runtime->Run all . Note, though, that some of the code cells will fail because not all the coding is complete. (You'll complete the coding as part of the exercise.)","title":"Running code cells"},{"location":"Linear_Regression_with_Synthetic_Data/#understanding-hidden-code-cells","text":"We've hidden the code in code cells that don't advance the learning objectives. For example, we've hidden the code that plots graphs. However, you must still run code cells containing hidden code . You'll know that the code is hidden because you'll see a title (for example, \"Load the functions that build and train a model\") without seeing the code. To view the hidden code, just double click the header.","title":"Understanding hidden code cells"},{"location":"Linear_Regression_with_Synthetic_Data/#why-did-you-see-an-error","text":"If a code cell returns an error when you run it, consider two common problems: You didn't run all of the code cells preceding the current code cell. If the code cell is labeled as a Task , then you haven't written the necessary code.","title":"Why did you see an error?"},{"location":"Linear_Regression_with_Synthetic_Data/#use-the-right-version-of-tensorflow","text":"The following hidden code cell ensures that the Colab will run on TensorFlow 2.X, which is the most recent version of TensorFlow: #@title Run this Colab on TensorFlow 2.x %tensorflow_version 2.x","title":"Use the right version of TensorFlow"},{"location":"Linear_Regression_with_Synthetic_Data/#import-relevant-modules","text":"The following cell imports the packages that the program requires: import pandas as pd import tensorflow as tf from matplotlib import pyplot as plt","title":"Import relevant modules"},{"location":"Linear_Regression_with_Synthetic_Data/#define-functions-that-build-and-train-a-model","text":"The following code defines two functions: build_model(my_learning_rate) , which builds an empty model. train_model(model, feature, label, epochs) , which trains the model from the examples (feature and label) you pass. Since you don't need to understand model building code right now, we've hidden this code cell. You may optionally double-click the headline to explore this code. #@title Define the functions that build and train a model def build_model(my_learning_rate): \"\"\"Create and compile a simple linear regression model.\"\"\" # Most simple tf.keras models are sequential. # A sequential model contains one or more layers. model = tf.keras.models.Sequential() # Describe the topography of the model. # The topography of a simple linear regression model # is a single node in a single layer. model.add(tf.keras.layers.Dense(units=1, input_shape=(1,))) # Compile the model topography into code that # TensorFlow can efficiently execute. Configure # training to minimize the model's mean squared error. model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()]) return model def train_model(model, feature, label, epochs, batch_size): \"\"\"Train the model by feeding it data.\"\"\" # Feed the feature values and the label values to the # model. The model will train for the specified number # of epochs, gradually learning how the feature values # relate to the label values. history = model.fit(x=feature, y=label, batch_size=batch_size, epochs=epochs) # Gather the trained model's weight and bias. trained_weight = model.get_weights()[0] trained_bias = model.get_weights()[1] # The list of epochs is stored separately from the # rest of history. epochs = history.epoch # Gather the history (a snapshot) of each epoch. hist = pd.DataFrame(history.history) # Specifically gather the model's root mean #squared error at each epoch. rmse = hist[\"root_mean_squared_error\"] return trained_weight, trained_bias, epochs, rmse print(\"Defined create_model and train_model\")","title":"Define functions that build and train a model"},{"location":"Linear_Regression_with_Synthetic_Data/#define-plotting-functions","text":"We're using a popular Python library called Matplotlib to create the following two plots: a plot of the feature values vs. the label values, and a line showing the output of the trained model. a loss curve . We hid the following code cell because learning Matplotlib is not relevant to the learning objectives. Regardless, you must still run all hidden code cells. #@title Define the plotting functions def plot_the_model(trained_weight, trained_bias, feature, label): \"\"\"Plot the trained model against the training feature and label.\"\"\" # Label the axes. plt.xlabel(\"feature\") plt.ylabel(\"label\") # Plot the feature values vs. label values. plt.scatter(feature, label) # Create a red line representing the model. The red line starts # at coordinates (x0, y0) and ends at coordinates (x1, y1). x0 = 0 y0 = trained_bias x1 = feature[-1] y1 = trained_bias + (trained_weight * x1) plt.plot([x0, x1], [y0, y1], c='r') # Render the scatter plot and the red line. plt.show() def plot_the_loss_curve(epochs, rmse): \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\" plt.figure() plt.xlabel(\"Epoch\") plt.ylabel(\"Root Mean Squared Error\") plt.plot(epochs, rmse, label=\"Loss\") plt.legend() plt.ylim([rmse.min()*0.97, rmse.max()]) plt.show() print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")","title":"Define plotting functions"},{"location":"Linear_Regression_with_Synthetic_Data/#define-the-dataset","text":"The dataset consists of 12 examples . Each example consists of one feature and one label . my_feature = ([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]) my_label = ([5.0, 8.8, 9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])","title":"Define the dataset"},{"location":"Linear_Regression_with_Synthetic_Data/#specify-the-hyperparameters","text":"The hyperparameters in this Colab are as follows: learning rate epochs batch_size The following code cell initializes these hyperparameters and then invokes the functions that build and train the model. learning_rate=0.01 epochs=10 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Specify the hyperparameters"},{"location":"Linear_Regression_with_Synthetic_Data/#task-1-examine-the-graphs","text":"Examine the top graph. The blue dots identify the actual data; the red line identifies the output of the trained model. Ideally, the red line should align nicely with the blue dots. Does it? Probably not. A certain amount of randomness plays into training a model, so you'll get somewhat different results every time you train. That said, unless you are an extremely lucky person, the red line probably doesn't align nicely with the blue dots. Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently.","title":"Task 1: Examine the graphs"},{"location":"Linear_Regression_with_Synthetic_Data/#task-2-increase-the-number-of-epochs","text":"Training loss should steadily decrease, steeply at first, and then more slowly. Eventually, training loss should eventually stay steady (zero slope or nearly zero slope), which indicates that training has converged . In Task 1, the training loss did not converge. One possible solution is to train for more epochs. Your task is to increase the number of epochs sufficiently to get the model to converge. However, it is inefficient to train past convergence, so don't just set the number of epochs to an arbitrarily high value. Examine the loss curve. Does the model converge? learning_rate=0.01 epochs= ? # Replace ? with an integer. my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.01 epochs=450 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) # The loss curve suggests that the model does converge.","title":"Task 2: Increase the number of epochs"},{"location":"Linear_Regression_with_Synthetic_Data/#task-3-increase-the-learning-rate","text":"In Task 2, you increased the number of epochs to get the model to converge. Sometimes, you can get the model to converge more quickly by increasing the learning rate. However, setting the learning rate too high often makes it impossible for a model to converge. In Task 3, we've intentionally set the learning rate too high. Run the following code cell and see what happens. # Increase the learning rate and decrease the number of epochs. learning_rate=100 epochs=500 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) The resulting model is terrible; the red line doesn't align with the blue dots. Furthermore, the loss curve oscillates like a roller coaster . An oscillating loss curve strongly suggests that the learning rate is too high.","title":"Task 3: Increase the learning rate"},{"location":"Linear_Regression_with_Synthetic_Data/#task-4-find-the-ideal-combination-of-epochs-and-learning-rate","text":"Assign values to the following two hyperparameters to make training converge as efficiently as possible: learning_rate epochs # Set the learning rate and number of epochs learning_rate= ? # Replace ? with a floating-point number epochs= ? # Replace ? with an integer my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.14 epochs=70 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Task 4: Find the ideal combination of epochs and learning rate"},{"location":"Linear_Regression_with_Synthetic_Data/#task-5-adjust-the-batch-size","text":"The system recalculates the model's loss value and adjusts the model's weights and bias after each iteration . Each iteration is the span in which the system processes one batch. For example, if the batch size is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples. One epoch spans sufficient iterations to process every example in the dataset. For example, if the batch size is 12, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations. It is tempting to simply set the batch size to the number of examples in the dataset (12, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. Experiment with batch_size in the following code cell. What's the smallest integer you can set for batch_size and still have the model converge in a hundred epochs? learning_rate=0.05 epochs=100 my_batch_size= ? # Replace ? with an integer. my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.05 epochs=125 my_batch_size=1 # Wow, a batch size of 1 works! my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Task 5: Adjust the batch size"},{"location":"Linear_Regression_with_Synthetic_Data/#summary-of-hyperparameter-tuning","text":"Most machine learning problems require a lot of hyperparameter tuning. Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb: Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. If the training loss does not converge, train for more epochs. If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging. If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate. Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination. Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation. For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.","title":"Summary of hyperparameter tuning"},{"location":"Welcome_To_Colaboratory/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Welcome to Colab! If you're already familiar with Colab, check out this video to learn about interactive tables, the executed code history view, and the command palette. What is Colab? Colab, or \"Colaboratory\", allows you to write and execute Python in your browser, with - Zero configuration required - Access to GPUs free of charge - Easy sharing Whether you're a student , a data scientist or an AI researcher , Colab can make your work easier. Watch Introduction to Colab to learn more, or just get started below! Getting started The document you are reading is not a static web page, but an interactive environment called a Colab notebook that lets you write and execute code. For example, here is a code cell with a short Python script that computes a value, stores it in a variable, and prints the result: seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day 86400 To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing. Variables that you define in one cell can later be used in other cells: seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week 604800 Colab notebooks allow you to combine executable code and rich text in a single document, along with images , HTML , LaTeX and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see Overview of Colab . To create a new Colab notebook you can use the File menu above, or use the following link: create a new Colab notebook . Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see jupyter.org . Data science With Colab you can harness the full power of popular Python libraries to analyze and visualize data. The code cell below uses numpy to generate some random data, and uses matplotlib to visualize it. To edit the code, just click the cell and start editing. import numpy as np from matplotlib import pyplot as plt ys = 200 + np.random.randn(100) x = [x for x in range(len(ys))] plt.plot(x, ys, '-') plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6) plt.title(\"Sample Visualization\") plt.show() You can import your own data into Colab notebooks from your Google Drive account, including from spreadsheets, as well as from Github and many other sources. To learn more about importing data, and how Colab can be used for data science, see the links below under Working with Data . Machine learning With Colab you can import an image dataset, train an image classifier on it, and evaluate the model, all in just a few lines of code . Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including GPUs and TPUs , regardless of the power of your machine. All you need is a browser. Colab is used extensively in the machine learning community with applications including: - Getting started with TensorFlow - Developing and training neural networks - Experimenting with TPUs - Disseminating AI research - Creating tutorials To see sample Colab notebooks that demonstrate machine learning applications, see the machine learning examples below. More Resources Working with Notebooks in Colab Overview of Colaboratory Guide to Markdown Importing libraries and installing dependencies Saving and loading notebooks in GitHub Interactive forms Interactive widgets TensorFlow 2 in Colab Working with Data Loading data: Drive, Sheets, and Google Cloud Storage Charts: visualizing data Getting started with BigQuery Machine Learning Crash Course These are a few of the notebooks from Google's online Machine Learning course. See the full course website for more. - Intro to Pandas DataFrame - Linear regression with tf.keras using synthetic data Using Accelerated Hardware TensorFlow with GPUs TensorFlow with TPUs Featured examples NeMo Voice Swap : Use Nvidia's NeMo conversational AI Toolkit to swap a voice in an audio fragment with a computer generated one. Retraining an Image Classifier : Build a Keras model on top of a pre-trained image classifier to distinguish flowers. Text Classification : Classify IMDB movie reviews as either positive or negative . Style Transfer : Use deep learning to transfer style between images. Multilingual Universal Sentence Encoder Q&A : Use a machine learning model to answer questions from the SQuAD dataset. Video Interpolation : Predict what happened in a video between the first and the last frame.","title":"Welcome To Colaboratory"},{"location":"Welcome_To_Colaboratory/#welcome-to-colab","text":"If you're already familiar with Colab, check out this video to learn about interactive tables, the executed code history view, and the command palette.","title":"Welcome to Colab!"},{"location":"Welcome_To_Colaboratory/#what-is-colab","text":"Colab, or \"Colaboratory\", allows you to write and execute Python in your browser, with - Zero configuration required - Access to GPUs free of charge - Easy sharing Whether you're a student , a data scientist or an AI researcher , Colab can make your work easier. Watch Introduction to Colab to learn more, or just get started below!","title":"What is Colab?"},{"location":"Welcome_To_Colaboratory/#getting-started","text":"The document you are reading is not a static web page, but an interactive environment called a Colab notebook that lets you write and execute code. For example, here is a code cell with a short Python script that computes a value, stores it in a variable, and prints the result: seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day 86400 To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing. Variables that you define in one cell can later be used in other cells: seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week 604800 Colab notebooks allow you to combine executable code and rich text in a single document, along with images , HTML , LaTeX and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see Overview of Colab . To create a new Colab notebook you can use the File menu above, or use the following link: create a new Colab notebook . Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see jupyter.org .","title":"Getting started"},{"location":"Welcome_To_Colaboratory/#data-science","text":"With Colab you can harness the full power of popular Python libraries to analyze and visualize data. The code cell below uses numpy to generate some random data, and uses matplotlib to visualize it. To edit the code, just click the cell and start editing. import numpy as np from matplotlib import pyplot as plt ys = 200 + np.random.randn(100) x = [x for x in range(len(ys))] plt.plot(x, ys, '-') plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6) plt.title(\"Sample Visualization\") plt.show() You can import your own data into Colab notebooks from your Google Drive account, including from spreadsheets, as well as from Github and many other sources. To learn more about importing data, and how Colab can be used for data science, see the links below under Working with Data .","title":"Data science"},{"location":"Welcome_To_Colaboratory/#machine-learning","text":"With Colab you can import an image dataset, train an image classifier on it, and evaluate the model, all in just a few lines of code . Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including GPUs and TPUs , regardless of the power of your machine. All you need is a browser. Colab is used extensively in the machine learning community with applications including: - Getting started with TensorFlow - Developing and training neural networks - Experimenting with TPUs - Disseminating AI research - Creating tutorials To see sample Colab notebooks that demonstrate machine learning applications, see the machine learning examples below.","title":"Machine learning"},{"location":"Welcome_To_Colaboratory/#more-resources","text":"","title":"More Resources"},{"location":"Welcome_To_Colaboratory/#working-with-notebooks-in-colab","text":"Overview of Colaboratory Guide to Markdown Importing libraries and installing dependencies Saving and loading notebooks in GitHub Interactive forms Interactive widgets TensorFlow 2 in Colab","title":"Working with Notebooks in Colab"},{"location":"Welcome_To_Colaboratory/#working-with-data","text":"Loading data: Drive, Sheets, and Google Cloud Storage Charts: visualizing data Getting started with BigQuery","title":"Working with Data"},{"location":"Welcome_To_Colaboratory/#machine-learning-crash-course","text":"These are a few of the notebooks from Google's online Machine Learning course. See the full course website for more. - Intro to Pandas DataFrame - Linear regression with tf.keras using synthetic data","title":"Machine Learning Crash Course"},{"location":"Welcome_To_Colaboratory/#using-accelerated-hardware","text":"TensorFlow with GPUs TensorFlow with TPUs","title":"Using Accelerated Hardware"},{"location":"Welcome_To_Colaboratory/#featured-examples","text":"NeMo Voice Swap : Use Nvidia's NeMo conversational AI Toolkit to swap a voice in an audio fragment with a computer generated one. Retraining an Image Classifier : Build a Keras model on top of a pre-trained image classifier to distinguish flowers. Text Classification : Classify IMDB movie reviews as either positive or negative . Style Transfer : Use deep learning to transfer style between images. Multilingual Universal Sentence Encoder Q&A : Use a machine learning model to answer questions from the SQuAD dataset. Video Interpolation : Predict what happened in a video between the first and the last frame.","title":"Featured examples"},{"location":"contacto/","text":"","title":"Contacto"}]}