{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Programacion https://learning.edx.org/course/course-v1:HarvardX+TinyML1+3T2020/home Fundamentals of TinyML As mentioned earlier, we\u2019ll be using the Colab environment for much of our programming in this specialization. In order to ensure that everyone has some hands on experience with Colab, please follow the link below to Google\u2019s Introductory Colab. In it they provide a handful of quick tips and hands on exercises to get you started using the Colab environment. https://colab.research.google.com/notebooks/intro.ipynb For machine learning in this course we are going to use Google\u2019s TensorFlow Machine Learning framework. TensorFlow is widely used across industry and academia. To get you started with TensorFlow here is Google\u2019s wonderful intro to TensorFlow video. Sample TensorFlow code While TensorFlow is written with fast custom C++ code under the hood, it has a high level Python interface that we will use throughout this course. As such, you\u2019ll need to be comfortable with basic coding in Python for this course. For example we might create a custom Neural Network model using code that looks something like the below. Don\u2019t worry if you don\u2019t know what all of the words mean, we\u2019ll walk you through all of these topics later in the course. For now just make sure you know enough Python to understand the following high level takeaways: This is a custom class that extends the tf.keras.Model base class It defines four specific kinds of tf.keras.layers It has a call function which will run an input through those layers (functions) If that resonates with you then you probably know enough Python to do well in this course! If you don\u2019t feel like you know enough Python yet, don\u2019t worry. Both Python.org and LearnPython.org have great resources you can use to get you up to speed. We\u2019re sure that with a little work, very soon you\u2019ll be ready to proceed with this course. Load in the TensorFlow library import tensorflow as tf Define my custom Neural Network model class MyModel(tf.keras.Model): def __init__(self): super(MyModel, self).__init__() # define Neural Network layer types self.conv = tf.keras.layers.Conv2D(32, 3, activation='relu') self.flatten = tf.keras.layers.Flatten() self.dense1 = tf.keras.layers.Dense(128, activation='relu') self.dense2 = tf.keras.layers.Dense(10) run my Neural Network model by evaluating # each layer on my input data def call(self, x): x = self.conv(x) x = self.flatten(x) x = self.dense1(x) x = self.dense2(x) return x Create an instance of the model model = MyModel() Coding exercise In this next Colab you will get to explore linear regression and loss functions for yourself. We have provided a cell that will compute the predicted Y values as well as the loss function for your guess of w and b. Simply change their values and explore how the output and loss changes. Note: You may receive a warning that the Colab was not authored by Google, and this may occur on future Colabs as well. That is entirely normal as we authored many of these Colabs ourselves for this course https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-4-ExploringLoss.ipynb Coding exercise: Gradient descent In this next Colab you will get to minimize the loss function for our linear regression model using TensorFlow\u2019s built-in GradientTape class. You will also get to visualize the training process, seeing how both the bias and weight converge on the optimal solution across training epochs. https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-6-MimimizingLoss.ipynb Coding exercise: neural networks In this next Colab you will get to explore training your first neural network\u2014see just how easy that can be with TensorFlow. You\u2019ll also see the result of the question posed at the end of the last video\u2014will the predicted Y be 19 for X = 10? Launch the Colab to find out! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-9-FirstNeuralNetwork.ipynb More neural networks Up to now you\u2019ve been looking at matching X values to Y values when there\u2019s a linear relationship between them. So, for example, you matched the X values in this set [-1, 0 , 1, 2, 3, 4] to the Y values in this set [-3, -1, 1, 3, 5, 7] by figuring out the equation Y=2X-1. You then saw how a very simple neural network with a single neuron within it could be used for this. An input leads into a neuron as an arrow. Out of this an output arrow is generated. This worked very well, because, in reality, what is referred to as a \u2018neuron\u2019 here is simply a function that has two learnable parameters, called a \u2018weight\u2019 and a \u2018bias\u2019, where, the output of the neuron will be: Output = (Weight * Input) + Bias So, for learning the linear relationship between our Xs and Ys, this maps perfectly, where we want the weight to be learned as \u20182\u2019, and the bias as \u2018-1\u2019. In the code you saw this happening. When multiple neurons work together in layers, the learned weights and biases across these layers can then have the effect of letting the neural network learn more complex patterns. You\u2019ll learn more about how this works later in the course. In your first Neural Network you saw neurons that were densely connected to each other, so you saw the Dense layer type. As well as neurons like this, there are also additional layer types in TensorFlow that you\u2019ll encounter. Here\u2019s just a few of them: Convolutional layers contain filters that can be used to transform data. The values of these filters will be learned in the same way as the parameters in the Dense neuron you saw here. Thus, a network containing them can learn how to transform data effectively. This is especially useful in Computer Vision, which you\u2019ll see later in this course. We\u2019ll even use these convolutional layers that are typically used for vision models to do speech detection! Are you wondering how or why? Stay tuned! Recurrent layers learn about the relationships between pieces of data in a sequence. There are many types of recurrent layer, with a popular one called LSTM (Long, Short Term Memory), being particularly effective. Recurrent layers are useful for predicting sequence data (like the weather), or understanding text. You\u2019ll also encounter layer types that don\u2019t learn parameters themselves, but which can affect the other layers. These include layers like dropouts, which are used to reduce the density of connection between dense layers to make them more efficient, pooling which can be used to reduce the amount of data flowing through the network to remove unnecessary information, and lambda lambda layers that allow you to execute arbitrary code. Your journey over the next few videos will primarily deal with the Dense network type, and you\u2019ll start to explore how multiple layers can work together to infer the rules that match your data to your labels. Neural networks in action You\u2019ve now seen a very simple example for how computers can learn. There\u2019s no great mystery to it -- it\u2019s a simple algorithm of making a guess, measuring how good that guess is (aka the loss), and then using this information to optimize the guess, and continually repeating this process to improve the guess. What you\u2019ve seen -- fitting numbers in an equation -- might seem trivial, but the methodology that you used to do this is the same as is used in far more sophisticated scenarios. To understand just how powerful this simple method - Machine Learning - can be, lets now explore a couple of new and exciting case studies: The first is the story of a young woman, Nazrini Siraji who used Machine Learning to detect diseases in crops, helping to stem the destruction of crops in her home country of Uganda: https://www.youtube.com/watch?v=23Q7HciuVyM Next, is air cognizer, built by undergraduate students in India, who realized that pictures of the sky, when matched to labels on air pollution could be used to build a new type of air quality sensor, using just the cameras on their phones: https://www.youtube.com/watch?v=9r2VVM4nfk8 Finally, here\u2019s a talk from a Google engineer about how Google used images of retinas to build a diabetic retinopathy detector using TensorFlow that performs state of the art diagnosis of this disease: https://www.youtube.com/watch?v=oOeZ7IgEN4o Coding exercise: neurons in action Now that we\u2019ve seen that we can use a single neuron to solve the linear regression problem, go back to the minimizing loss Colab and see if you can spot how we were using neurons all along. Do you find this surprising? Did you figure it out the first time you saw the Colab? If so, what gave it away? https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-2-3-MimimizingLoss.ipynb Coding exercise: multi-layer neural network Now let\u2019s explore the code from the previous reading through a Colab. You\u2019ll be able to train both the single layer and multi-layer model described in the previous reading and explore their predictions and trained weights and biases. Which model learns faster? Which ends up making better predictions? https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-2-5-FirstNeuralNetworkRevisited.ipynb Coding exercise: DNN Now let\u2019s explore the MNIST classification example through a Colab. You\u2019ll get to train the model just described in the previous reading, learn how to inspect and analyze the various trained layers in the model, and test the final model on additional data sets! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-2-7-ExploringCategorical.ipynb Coding assignment In this assignment we'll dive a little deeper with a series of hands on exercises to better understand DNN learning with Tensorflow. Remember that if you are taking the class for a certificate we will be asking you questions about the assignment in the test! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-2-12-AssignmentQuestion.ipynb Up to this point you have learned the Machine Learning paradigm, and how it works to allow a computer to figure out the parameters that fit a function. When that function gives you the rules to map data to answers, you\u2019ve changed from a programming paradigm where you have to figure out the rules that act on the data to give you answers: Rules and data feed into traditional programming which then outputs answers. To one that looks like this, where you provide the answers with the data, and the computer figures out the rules that maps them to each other: Answers and data feed into machine learning and from this rules emerge. To achieve this, of course, you still need to write code, but instead of writing the code for the rules, you write the code that figures out what the rules are. The algorithm to do this looks like the following: Making a guess leads to measuring your accuracy which leads to optimizing your guess. This then gets repeated where you make a guess again and repeat the process. Arrows connect the steps. As an example, you used two sets of numbers, for X and Y and wrote code that used the above algorithm to figure out the relationship between them where Y = wX+b, and the above algorithm could figure out the values of w and b that would fit. This was the basis of a mathematical neuron, where the neuron could store the values of w and b, and again, using the above algorithm, those values could be figured out to give the desired Y for an input X. An input of X is fed into a neuron and an out Y is created. When these neurons are used together, in layers, more complex relationships could be figured out, and you saw how having multiple output neurons could take you into the territory of classification, where, each neuron could have a role in the final output, which of course is perfect for computer vision. Below is a simplified neural network diagram illustrating this point where input data can be classified -- i.e. does the network \u2018see\u2019 a dog or a cat. An image of a cat\u2019s face is fed into multiple neurons. Each of these neurons outputs a decision of whether the cat\u2019s face represents a cat or a dog. Basic neurons, stacked together like this form what is called a Dense layer, based on the fact that they\u2019re densely connected to each other. When there are multiple layers between the input and the output, you have a Deep neural network, which gives us the term Deep Learning. Additionally, the layers between the input and the output are often called >hidden layers. As you continue your Machine Learning journey, you\u2019ll discover that there are many other types of layer that learn different parameters to just the w and b that a neuron learns. For example, there are Recurrent layers, that are beyond the scope of this course, but they are able to learn values in a sequence where each neuron learns values and passes those values to another neuron in the same layer. The basic idea can be represented like this: Recurrent layers are represented by having each layer indexed by a subscript. Here there is x zero x one and x two. On the outputs there is y zero y one and y two. Where, if we have a sequence of values, X0, X1, X2 that we want to learn a corresponding sequence Y0, Y1, Y2 for, then the idea is the the neurons (F) can not only try to match X0->Y0, but also pass values along the sequence, indicated by the right pointing arrows, so the input to the second neuron is the output from the first and X1, from which it can learn the parameters for Y1 and so on. There are many variations on this idea, but a very powerful one is called Long Short Term Memory where not only are values passed from neuron to neuron, so that X0 can impact Y1, X1 can impact Y2, but values from further back can also have an impact -- so that X0 could impact Y99 for example. This is achieved using a data structure called a Cell State where context can be preserved across multiple neurons. Recurrent layers are represented by having each layer indexed by a subscript. Here there is x zero x one and x two. On the outputs there is y zero y one and y two. Another type of neural network layer is called a Convolution, where a filter that can transform data, particularly image data, can be learned. You\u2019ll explore those next! I hope this helps you understand that Machine Learning goes beyond simple neurons that learn a w and b in the Y=wX+b scenario. Coding exercise: filters In this Colab you\u2019ll explore how filters and convolutions can be used to extract features from images. You\u2019ll see firsthand how different filters can extract different features. You\u2019ll also learn about pooling and explore how it can help reduce the amount of information while retaining important features. Click the link to get started! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-3-3-ExploringConvolutions.ipynb Coding exercise: CNN In this Colab you\u2019ll explore the power of Convolutional Neural Networks (CNNs). You\u2019ll train both a traditional DNN and a CNN and see how CNNs can far outperform standard networks on computer vision tasks. You\u2019ll then dive into both the convolutional and max pooling layers that power the CNN. https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-3-5-FashionMNISTConvolutions.ipynb Coding exercise: computer vision In this Colab you\u2019ll build off of the prior Colab exercise using CNNs to learn the Fashion MNIST dataset, and you\u2019ll start to visualize the convolution and pooling layers to better understand how the model \u201csees\u201d the world. Hopefully this will give you more insight into how neural networks see the world as compared to how you see the world! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-3-7-FashionMNISTConvolutionsVisualizations.ipynb Coding exercise: complex images In this Colab you\u2019ll get to explore hands on the concepts discussed in the previous video. You\u2019ll start by downloading and organizing the horses v. humans image database using Generators. You\u2019ll then train a model using your organized data and then get to upload your own horse or human image and see if the model can detect it correctly. Finally you\u2019ll get to visualize the model layers. Note: as many learners have pointed out in the discussion below, the final model will only work well on images that are similar to those in our limited example dataset. As such, do not be surprised if you can trick the model! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-4-3-HorsesOrHumans.ipynb Coding exercise: Image Augmentation In this Colab, we\u2019ll again explore the horses v. humans Colab. This time, however, we will train with all forms of data augmentation turned on. You\u2019ll see how this generates quite a bit more data (which takes longer to train) but produces a much more generalizable model since we are avoiding overfitting. You\u2019ll get to test this by again having the chance to upload your own horse or human image and see if the model can detect it correctly. Finally you\u2019ll again get to visualize the model layers. As many students have noted in the discussion forum, since we are training on a very limited dataset, with a very small model, the final model does make a decent amount of mistakes. You'll find that if you test with images that look closer to the training dataset it will work much better. This is because while we are avoiding some overfitting with data augmentation, all machine learning models will struggle to generalize far beyond the training dataset! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-4-6-HorseOrHumanWithAugmentation.ipynb","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#fundamentals-of-tinyml","text":"As mentioned earlier, we\u2019ll be using the Colab environment for much of our programming in this specialization. In order to ensure that everyone has some hands on experience with Colab, please follow the link below to Google\u2019s Introductory Colab. In it they provide a handful of quick tips and hands on exercises to get you started using the Colab environment. https://colab.research.google.com/notebooks/intro.ipynb For machine learning in this course we are going to use Google\u2019s TensorFlow Machine Learning framework. TensorFlow is widely used across industry and academia. To get you started with TensorFlow here is Google\u2019s wonderful intro to TensorFlow video.","title":"Fundamentals of TinyML"},{"location":"#sample-tensorflow-code","text":"While TensorFlow is written with fast custom C++ code under the hood, it has a high level Python interface that we will use throughout this course. As such, you\u2019ll need to be comfortable with basic coding in Python for this course. For example we might create a custom Neural Network model using code that looks something like the below. Don\u2019t worry if you don\u2019t know what all of the words mean, we\u2019ll walk you through all of these topics later in the course. For now just make sure you know enough Python to understand the following high level takeaways: This is a custom class that extends the tf.keras.Model base class It defines four specific kinds of tf.keras.layers It has a call function which will run an input through those layers (functions) If that resonates with you then you probably know enough Python to do well in this course! If you don\u2019t feel like you know enough Python yet, don\u2019t worry. Both Python.org and LearnPython.org have great resources you can use to get you up to speed. We\u2019re sure that with a little work, very soon you\u2019ll be ready to proceed with this course.","title":"Sample TensorFlow code"},{"location":"#load-in-the-tensorflow-library","text":"import tensorflow as tf","title":"Load in the TensorFlow library"},{"location":"#define-my-custom-neural-network-model","text":"class MyModel(tf.keras.Model): def __init__(self): super(MyModel, self).__init__() # define Neural Network layer types self.conv = tf.keras.layers.Conv2D(32, 3, activation='relu') self.flatten = tf.keras.layers.Flatten() self.dense1 = tf.keras.layers.Dense(128, activation='relu') self.dense2 = tf.keras.layers.Dense(10) run my Neural Network model by evaluating # each layer on my input data def call(self, x): x = self.conv(x) x = self.flatten(x) x = self.dense1(x) x = self.dense2(x) return x","title":"Define my custom Neural Network model"},{"location":"#create-an-instance-of-the-model","text":"model = MyModel()","title":"Create an instance of the model"},{"location":"#coding-exercise","text":"In this next Colab you will get to explore linear regression and loss functions for yourself. We have provided a cell that will compute the predicted Y values as well as the loss function for your guess of w and b. Simply change their values and explore how the output and loss changes. Note: You may receive a warning that the Colab was not authored by Google, and this may occur on future Colabs as well. That is entirely normal as we authored many of these Colabs ourselves for this course https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-4-ExploringLoss.ipynb","title":"Coding exercise"},{"location":"#coding-exercise-gradient-descent","text":"In this next Colab you will get to minimize the loss function for our linear regression model using TensorFlow\u2019s built-in GradientTape class. You will also get to visualize the training process, seeing how both the bias and weight converge on the optimal solution across training epochs. https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-6-MimimizingLoss.ipynb","title":"Coding exercise: Gradient descent"},{"location":"#coding-exercise-neural-networks","text":"In this next Colab you will get to explore training your first neural network\u2014see just how easy that can be with TensorFlow. You\u2019ll also see the result of the question posed at the end of the last video\u2014will the predicted Y be 19 for X = 10? Launch the Colab to find out! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-1-9-FirstNeuralNetwork.ipynb","title":"Coding exercise: neural networks"},{"location":"#more-neural-networks","text":"Up to now you\u2019ve been looking at matching X values to Y values when there\u2019s a linear relationship between them. So, for example, you matched the X values in this set [-1, 0 , 1, 2, 3, 4] to the Y values in this set [-3, -1, 1, 3, 5, 7] by figuring out the equation Y=2X-1. You then saw how a very simple neural network with a single neuron within it could be used for this. An input leads into a neuron as an arrow. Out of this an output arrow is generated. This worked very well, because, in reality, what is referred to as a \u2018neuron\u2019 here is simply a function that has two learnable parameters, called a \u2018weight\u2019 and a \u2018bias\u2019, where, the output of the neuron will be: Output = (Weight * Input) + Bias So, for learning the linear relationship between our Xs and Ys, this maps perfectly, where we want the weight to be learned as \u20182\u2019, and the bias as \u2018-1\u2019. In the code you saw this happening. When multiple neurons work together in layers, the learned weights and biases across these layers can then have the effect of letting the neural network learn more complex patterns. You\u2019ll learn more about how this works later in the course. In your first Neural Network you saw neurons that were densely connected to each other, so you saw the Dense layer type. As well as neurons like this, there are also additional layer types in TensorFlow that you\u2019ll encounter. Here\u2019s just a few of them: Convolutional layers contain filters that can be used to transform data. The values of these filters will be learned in the same way as the parameters in the Dense neuron you saw here. Thus, a network containing them can learn how to transform data effectively. This is especially useful in Computer Vision, which you\u2019ll see later in this course. We\u2019ll even use these convolutional layers that are typically used for vision models to do speech detection! Are you wondering how or why? Stay tuned! Recurrent layers learn about the relationships between pieces of data in a sequence. There are many types of recurrent layer, with a popular one called LSTM (Long, Short Term Memory), being particularly effective. Recurrent layers are useful for predicting sequence data (like the weather), or understanding text. You\u2019ll also encounter layer types that don\u2019t learn parameters themselves, but which can affect the other layers. These include layers like dropouts, which are used to reduce the density of connection between dense layers to make them more efficient, pooling which can be used to reduce the amount of data flowing through the network to remove unnecessary information, and lambda lambda layers that allow you to execute arbitrary code. Your journey over the next few videos will primarily deal with the Dense network type, and you\u2019ll start to explore how multiple layers can work together to infer the rules that match your data to your labels.","title":"More neural networks"},{"location":"#neural-networks-in-action","text":"You\u2019ve now seen a very simple example for how computers can learn. There\u2019s no great mystery to it -- it\u2019s a simple algorithm of making a guess, measuring how good that guess is (aka the loss), and then using this information to optimize the guess, and continually repeating this process to improve the guess. What you\u2019ve seen -- fitting numbers in an equation -- might seem trivial, but the methodology that you used to do this is the same as is used in far more sophisticated scenarios. To understand just how powerful this simple method - Machine Learning - can be, lets now explore a couple of new and exciting case studies: The first is the story of a young woman, Nazrini Siraji who used Machine Learning to detect diseases in crops, helping to stem the destruction of crops in her home country of Uganda: https://www.youtube.com/watch?v=23Q7HciuVyM Next, is air cognizer, built by undergraduate students in India, who realized that pictures of the sky, when matched to labels on air pollution could be used to build a new type of air quality sensor, using just the cameras on their phones: https://www.youtube.com/watch?v=9r2VVM4nfk8 Finally, here\u2019s a talk from a Google engineer about how Google used images of retinas to build a diabetic retinopathy detector using TensorFlow that performs state of the art diagnosis of this disease: https://www.youtube.com/watch?v=oOeZ7IgEN4o","title":"Neural networks in action"},{"location":"#coding-exercise-neurons-in-action","text":"Now that we\u2019ve seen that we can use a single neuron to solve the linear regression problem, go back to the minimizing loss Colab and see if you can spot how we were using neurons all along. Do you find this surprising? Did you figure it out the first time you saw the Colab? If so, what gave it away? https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-2-3-MimimizingLoss.ipynb","title":"Coding exercise: neurons in action"},{"location":"#coding-exercise-multi-layer-neural-network","text":"Now let\u2019s explore the code from the previous reading through a Colab. You\u2019ll be able to train both the single layer and multi-layer model described in the previous reading and explore their predictions and trained weights and biases. Which model learns faster? Which ends up making better predictions? https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-2-5-FirstNeuralNetworkRevisited.ipynb","title":"Coding exercise: multi-layer neural network"},{"location":"#coding-exercise-dnn","text":"Now let\u2019s explore the MNIST classification example through a Colab. You\u2019ll get to train the model just described in the previous reading, learn how to inspect and analyze the various trained layers in the model, and test the final model on additional data sets! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-2-7-ExploringCategorical.ipynb","title":"Coding exercise: DNN"},{"location":"#coding-assignment","text":"In this assignment we'll dive a little deeper with a series of hands on exercises to better understand DNN learning with Tensorflow. Remember that if you are taking the class for a certificate we will be asking you questions about the assignment in the test! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-2-12-AssignmentQuestion.ipynb Up to this point you have learned the Machine Learning paradigm, and how it works to allow a computer to figure out the parameters that fit a function. When that function gives you the rules to map data to answers, you\u2019ve changed from a programming paradigm where you have to figure out the rules that act on the data to give you answers: Rules and data feed into traditional programming which then outputs answers. To one that looks like this, where you provide the answers with the data, and the computer figures out the rules that maps them to each other: Answers and data feed into machine learning and from this rules emerge. To achieve this, of course, you still need to write code, but instead of writing the code for the rules, you write the code that figures out what the rules are. The algorithm to do this looks like the following: Making a guess leads to measuring your accuracy which leads to optimizing your guess. This then gets repeated where you make a guess again and repeat the process. Arrows connect the steps. As an example, you used two sets of numbers, for X and Y and wrote code that used the above algorithm to figure out the relationship between them where Y = wX+b, and the above algorithm could figure out the values of w and b that would fit. This was the basis of a mathematical neuron, where the neuron could store the values of w and b, and again, using the above algorithm, those values could be figured out to give the desired Y for an input X. An input of X is fed into a neuron and an out Y is created. When these neurons are used together, in layers, more complex relationships could be figured out, and you saw how having multiple output neurons could take you into the territory of classification, where, each neuron could have a role in the final output, which of course is perfect for computer vision. Below is a simplified neural network diagram illustrating this point where input data can be classified -- i.e. does the network \u2018see\u2019 a dog or a cat. An image of a cat\u2019s face is fed into multiple neurons. Each of these neurons outputs a decision of whether the cat\u2019s face represents a cat or a dog. Basic neurons, stacked together like this form what is called a Dense layer, based on the fact that they\u2019re densely connected to each other. When there are multiple layers between the input and the output, you have a Deep neural network, which gives us the term Deep Learning. Additionally, the layers between the input and the output are often called >hidden layers. As you continue your Machine Learning journey, you\u2019ll discover that there are many other types of layer that learn different parameters to just the w and b that a neuron learns. For example, there are Recurrent layers, that are beyond the scope of this course, but they are able to learn values in a sequence where each neuron learns values and passes those values to another neuron in the same layer. The basic idea can be represented like this: Recurrent layers are represented by having each layer indexed by a subscript. Here there is x zero x one and x two. On the outputs there is y zero y one and y two. Where, if we have a sequence of values, X0, X1, X2 that we want to learn a corresponding sequence Y0, Y1, Y2 for, then the idea is the the neurons (F) can not only try to match X0->Y0, but also pass values along the sequence, indicated by the right pointing arrows, so the input to the second neuron is the output from the first and X1, from which it can learn the parameters for Y1 and so on. There are many variations on this idea, but a very powerful one is called Long Short Term Memory where not only are values passed from neuron to neuron, so that X0 can impact Y1, X1 can impact Y2, but values from further back can also have an impact -- so that X0 could impact Y99 for example. This is achieved using a data structure called a Cell State where context can be preserved across multiple neurons. Recurrent layers are represented by having each layer indexed by a subscript. Here there is x zero x one and x two. On the outputs there is y zero y one and y two. Another type of neural network layer is called a Convolution, where a filter that can transform data, particularly image data, can be learned. You\u2019ll explore those next! I hope this helps you understand that Machine Learning goes beyond simple neurons that learn a w and b in the Y=wX+b scenario.","title":"Coding assignment"},{"location":"#coding-exercise-filters","text":"In this Colab you\u2019ll explore how filters and convolutions can be used to extract features from images. You\u2019ll see firsthand how different filters can extract different features. You\u2019ll also learn about pooling and explore how it can help reduce the amount of information while retaining important features. Click the link to get started! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-3-3-ExploringConvolutions.ipynb","title":"Coding exercise: filters"},{"location":"#coding-exercise-cnn","text":"In this Colab you\u2019ll explore the power of Convolutional Neural Networks (CNNs). You\u2019ll train both a traditional DNN and a CNN and see how CNNs can far outperform standard networks on computer vision tasks. You\u2019ll then dive into both the convolutional and max pooling layers that power the CNN. https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-3-5-FashionMNISTConvolutions.ipynb","title":"Coding exercise: CNN"},{"location":"#coding-exercise-computer-vision","text":"In this Colab you\u2019ll build off of the prior Colab exercise using CNNs to learn the Fashion MNIST dataset, and you\u2019ll start to visualize the convolution and pooling layers to better understand how the model \u201csees\u201d the world. Hopefully this will give you more insight into how neural networks see the world as compared to how you see the world! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-3-7-FashionMNISTConvolutionsVisualizations.ipynb","title":"Coding exercise: computer vision"},{"location":"#coding-exercise-complex-images","text":"In this Colab you\u2019ll get to explore hands on the concepts discussed in the previous video. You\u2019ll start by downloading and organizing the horses v. humans image database using Generators. You\u2019ll then train a model using your organized data and then get to upload your own horse or human image and see if the model can detect it correctly. Finally you\u2019ll get to visualize the model layers. Note: as many learners have pointed out in the discussion below, the final model will only work well on images that are similar to those in our limited example dataset. As such, do not be surprised if you can trick the model! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-4-3-HorsesOrHumans.ipynb","title":"Coding exercise: complex images"},{"location":"#coding-exercise-image-augmentation","text":"In this Colab, we\u2019ll again explore the horses v. humans Colab. This time, however, we will train with all forms of data augmentation turned on. You\u2019ll see how this generates quite a bit more data (which takes longer to train) but produces a much more generalizable model since we are avoiding overfitting. You\u2019ll get to test this by again having the chance to upload your own horse or human image and see if the model can detect it correctly. Finally you\u2019ll again get to visualize the model layers. As many students have noted in the discussion forum, since we are training on a very limited dataset, with a very small model, the final model does make a decent amount of mistakes. You'll find that if you test with images that look closer to the training dataset it will work much better. This is because while we are avoiding some overfitting with data augmentation, all machine learning models will struggle to generalize far beyond the training dataset! https://colab.research.google.com/github/tinyMLx/colabs/blob/master/2-4-6-HorseOrHumanWithAugmentation.ipynb","title":"Coding exercise: Image Augmentation"},{"location":"2_2_12_Exercise/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Exploring DNN learning with TensorFlow In this assignment we'll dive a little deeper with a series of hands on exercises to better understand DNN learning with Tensorflow. Remember that if you are taking the class for a certificate we will be asking you questions about the assignment in the test! We start by setting up the problem for you. import tensorflow as tf # Load in fashion MNIST mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( test_images , test_labels ) = mnist . load_data () # Define the base model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) Neural Networks learn the best when the data is scaled / normalized to fall in a constant range. One practitioners often use is the range [0,1]. How might you do this to the training and test images used here? A hint: these images are saved in the standard RGB format training_images = #YOUR CODE HERE# test_images = #YOUR CODE HERE# Using these improved images lets compile our model using an adaptive optimizer to learn faster and a categorical loss function to differentiate between the the various classes we are trying to classify. Since this is a very simple dataset we will only train for 5 epochs. # compile the model model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # fit the model to the training data model . fit ( training_images , training_labels , epochs = 5 ) # test the model on the test data model . evaluate ( test_images , test_labels ) Once it's done training -- you should see an accuracy value at the end of the final epoch. It might look something like 0.8648. This tells you that your neural network is about 86% accurate in classifying the training data. I.E., it figured out a pattern match between the image and the labels that worked 86% of the time. But how would it work with unseen data? That's why we have the test images. We can call model.evaluate , and pass in the two sets, and it will report back the loss for each. This should reach about .8747 or thereabouts, showing about 87% accuracy. Not Bad! But what did it actually learn? If we inference on the model using model.predict we get out the following list of values. What does it represent? A hint: trying running print(test_labels[0]) classifications = model . predict ( test_images ) print ( classifications [ 0 ]) Let's now look at the layers in your model. What happens if you double the number of neurons in the dense layer. What different results do you get for loss, training time etc? Why do you think that's the case? NUMBER_OF_NEURONS = #YOUR_CODE_HERE# # define the new model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( NUMBER_OF_NEURONS , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) # compile fit and evaluate the model again model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 ) model . evaluate ( test_images , test_labels ) Consider the effects of additional layers in the network instead of simply more neurons to the same layer. First update the model to add an additional dense layer into the model between the two existing Dense layers. YOUR_NEW_LAYER = #YOUR_CODE_HERE# model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), YOUR_NEW_LAYER , tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) Lets then compile, fit, and evaluate our model. What happens to the error? How does this compare to the original model and the model with double the number of neurons? # compile fit and evaluate the model again model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 ) model . evaluate ( test_images , test_labels ) Before you trained, you normalized the data. What would be the impact of removing that? To see it for yourself fill in the following lines of code to get a non-normalized set of data and then re-fit and evaluate the model using this data. # get new non-normalized mnist data training_images_non = #YOUR_CODE_HERE# test_images_non = #YOUR_CODE_HERE# # re-compile, re-fit and re-evaluate model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), YOUR_NEW_LAYER , tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images_non , training_labels , epochs = 5 ) model . evaluate ( test_images_non , test_labels ) classifications = model . predict ( test_images_non ) Sometimes if you set the training for too many epochs you may find that training stops improving and you wish you could quit early. Good news, you can! TensorFlow has a function called Callbacks which can check the results from each epoch. Modify this callback function to make sure it exits training early but not before reaching at least the second epoch! A hint: logs.get(METRIC_NAME) will return the value of METRIC_NAME at the current step # define and instantiate your custom Callback class myCallback ( tf . keras . callbacks . Callback ): def on_epoch_end ( self , epoch , logs = {}): if ( #YOUR_CODE_HERE# ): self . model . stop_training = True callbacks = myCallback () # re-compile, re-fit and re-evaluate model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), YOUR_NEW_LAYER , tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 , callbacks = [ callbacks ])","title":"2 2 12 Exercise"},{"location":"2_2_12_Exercise/#exploring-dnn-learning-with-tensorflow","text":"In this assignment we'll dive a little deeper with a series of hands on exercises to better understand DNN learning with Tensorflow. Remember that if you are taking the class for a certificate we will be asking you questions about the assignment in the test! We start by setting up the problem for you. import tensorflow as tf # Load in fashion MNIST mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( test_images , test_labels ) = mnist . load_data () # Define the base model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) Neural Networks learn the best when the data is scaled / normalized to fall in a constant range. One practitioners often use is the range [0,1]. How might you do this to the training and test images used here? A hint: these images are saved in the standard RGB format training_images = #YOUR CODE HERE# test_images = #YOUR CODE HERE# Using these improved images lets compile our model using an adaptive optimizer to learn faster and a categorical loss function to differentiate between the the various classes we are trying to classify. Since this is a very simple dataset we will only train for 5 epochs. # compile the model model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # fit the model to the training data model . fit ( training_images , training_labels , epochs = 5 ) # test the model on the test data model . evaluate ( test_images , test_labels ) Once it's done training -- you should see an accuracy value at the end of the final epoch. It might look something like 0.8648. This tells you that your neural network is about 86% accurate in classifying the training data. I.E., it figured out a pattern match between the image and the labels that worked 86% of the time. But how would it work with unseen data? That's why we have the test images. We can call model.evaluate , and pass in the two sets, and it will report back the loss for each. This should reach about .8747 or thereabouts, showing about 87% accuracy. Not Bad! But what did it actually learn? If we inference on the model using model.predict we get out the following list of values. What does it represent? A hint: trying running print(test_labels[0]) classifications = model . predict ( test_images ) print ( classifications [ 0 ]) Let's now look at the layers in your model. What happens if you double the number of neurons in the dense layer. What different results do you get for loss, training time etc? Why do you think that's the case? NUMBER_OF_NEURONS = #YOUR_CODE_HERE# # define the new model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( NUMBER_OF_NEURONS , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) # compile fit and evaluate the model again model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 ) model . evaluate ( test_images , test_labels ) Consider the effects of additional layers in the network instead of simply more neurons to the same layer. First update the model to add an additional dense layer into the model between the two existing Dense layers. YOUR_NEW_LAYER = #YOUR_CODE_HERE# model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), YOUR_NEW_LAYER , tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) Lets then compile, fit, and evaluate our model. What happens to the error? How does this compare to the original model and the model with double the number of neurons? # compile fit and evaluate the model again model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 ) model . evaluate ( test_images , test_labels ) Before you trained, you normalized the data. What would be the impact of removing that? To see it for yourself fill in the following lines of code to get a non-normalized set of data and then re-fit and evaluate the model using this data. # get new non-normalized mnist data training_images_non = #YOUR_CODE_HERE# test_images_non = #YOUR_CODE_HERE# # re-compile, re-fit and re-evaluate model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), YOUR_NEW_LAYER , tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images_non , training_labels , epochs = 5 ) model . evaluate ( test_images_non , test_labels ) classifications = model . predict ( test_images_non ) Sometimes if you set the training for too many epochs you may find that training stops improving and you wish you could quit early. Good news, you can! TensorFlow has a function called Callbacks which can check the results from each epoch. Modify this callback function to make sure it exits training early but not before reaching at least the second epoch! A hint: logs.get(METRIC_NAME) will return the value of METRIC_NAME at the current step # define and instantiate your custom Callback class myCallback ( tf . keras . callbacks . Callback ): def on_epoch_end ( self , epoch , logs = {}): if ( #YOUR_CODE_HERE# ): self . model . stop_training = True callbacks = myCallback () # re-compile, re-fit and re-evaluate model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), YOUR_NEW_LAYER , tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) model . compile ( optimizer = tf . keras . optimizers . Adam (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 , callbacks = [ callbacks ])","title":"Exploring DNN learning with TensorFlow"},{"location":"2_4_3_HorsesOrHumans/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Download the neccessary data into the Colab Instance try : # %tensorflow_version only exists in Colab. % tensorflow_version 2. x except Exception : pass ! wget -- no - check - certificate \\ https : // storage . googleapis . com / laurencemoroney - blog . appspot . com / horse - or - human . zip \\ - O / tmp / horse - or - human . zip ! wget -- no - check - certificate \\ https : // storage . googleapis . com / laurencemoroney - blog . appspot . com / validation - horse - or - human . zip \\ - O / tmp / validation - horse - or - human . zip import os import zipfile local_zip = '/tmp/horse-or-human.zip' zip_ref = zipfile . ZipFile ( local_zip , 'r' ) zip_ref . extractall ( '/tmp/horse-or-human' ) local_zip = '/tmp/validation-horse-or-human.zip' zip_ref = zipfile . ZipFile ( local_zip , 'r' ) zip_ref . extractall ( '/tmp/validation-horse-or-human' ) zip_ref . close () # Directory with our training horse pictures train_horse_dir = os . path . join ( '/tmp/horse-or-human/horses' ) # Directory with our training human pictures train_human_dir = os . path . join ( '/tmp/horse-or-human/humans' ) # Directory with our training horse pictures validation_horse_dir = os . path . join ( '/tmp/validation-horse-or-human/horses' ) # Directory with our training human pictures validation_human_dir = os . path . join ( '/tmp/validation-horse-or-human/humans' ) train_horse_names = os . listdir ( '/tmp/horse-or-human/horses' ) print ( train_horse_names [: 10 ]) train_human_names = os . listdir ( '/tmp/horse-or-human/humans' ) print ( train_human_names [: 10 ]) validation_horse_hames = os . listdir ( '/tmp/validation-horse-or-human/horses' ) print ( validation_horse_hames [: 10 ]) validation_human_names = os . listdir ( '/tmp/validation-horse-or-human/humans' ) print ( validation_human_names [: 10 ]) import tensorflow as tf Define your model and optimizer model = tf . keras . models . Sequential ([ # Note the input shape is the desired size of the image with 3 bytes color # This is the first convolution tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 100 , 100 , 3 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The second convolution tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The third convolution tf . keras . layers . Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The fourth convolution tf . keras . layers . Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # Flatten the results to feed into a DNN tf . keras . layers . Flatten (), # 512 neuron hidden layer tf . keras . layers . Dense ( 512 , activation = 'relu' ), tf . keras . layers . Dense ( 256 , activation = 'relu' ), # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans') tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) ]) print ( model . summary ()) from tensorflow.keras.optimizers import RMSprop optimizer = RMSprop ( lr = 0.0001 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'acc' ]) Organize your data into Generators from tensorflow.keras.preprocessing.image import ImageDataGenerator # All images will be augmented according to whichever lines are uncommented below. # we can first try without any of the augmentation beyond the rescaling train_datagen = ImageDataGenerator ( rescale = 1. / 255 , #rotation_range=40, #width_shift_range=0.2, #height_shift_range=0.2, #shear_range=0.2, #zoom_range=0.2, #horizontal_flip=True, #fill_mode='nearest' ) # Flow training images in batches of 128 using train_datagen generator train_generator = train_datagen . flow_from_directory ( '/tmp/horse-or-human/' , # This is the source directory for training images target_size = ( 100 , 100 ), # All images will be resized to 100x100 batch_size = 128 , # Since we use binary_crossentropy loss, we need binary labels class_mode = 'binary' ) validation_datagen = ImageDataGenerator ( rescale = 1. / 255 ) validation_generator = validation_datagen . flow_from_directory ( '/tmp/validation-horse-or-human' , target_size = ( 100 , 100 ), class_mode = 'binary' ) Train your model This may take a little while. Remember we are now building and training relatively complex computer vision models! history = model . fit ( train_generator , steps_per_epoch = 8 , epochs = 100 , verbose = 1 , validation_data = validation_generator ) Run your Model Let's now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model, giving an indication of whether the object is a horse or a human. Was the model correct? Try a couple more images and see if you can confuse it! import numpy as np from google.colab import files from keras.preprocessing import image uploaded = files . upload () for fn in uploaded . keys (): # predicting images path = '/content/' + fn img = image . load_img ( path , target_size = ( 100 , 100 )) x = image . img_to_array ( img ) x = x / 255.0 x = np . expand_dims ( x , axis = 0 ) image_tensor = np . vstack ([ x ]) classes = model . predict ( image_tensor ) print ( classes ) print ( classes [ 0 ]) if classes [ 0 ] > 0.5 : print ( fn + \" is a human\" ) else : print ( fn + \" is a horse\" ) Finally lets visualize all of the model layers! import matplotlib.pyplot as plt import numpy as np import random from tensorflow.keras.preprocessing.image import img_to_array , load_img % matplotlib inline import matplotlib.pyplot as plt import matplotlib.image as mpimg # Let's define a new Model that will take an image as input, and will output # intermediate representations for all layers in the previous model after the first. successive_outputs = [ layer . output for layer in model . layers [ 1 :]] visualization_model = tf . keras . models . Model ( inputs = model . input , outputs = successive_outputs ) # Let's prepare a random input image from the training set. horse_img_files = [ os . path . join ( train_horse_dir , f ) for f in train_horse_names ] human_img_files = [ os . path . join ( train_human_dir , f ) for f in train_human_names ] img_path = random . choice ( horse_img_files + human_img_files ) # uncomment the following line if you want to pick the Xth human file manually img_path = human_img_files [ 0 ] img = load_img ( img_path , target_size = ( 100 , 100 )) # this is a PIL image x = img_to_array ( img ) # Numpy array with shape (100, 100, 3) x = x . reshape (( 1 ,) + x . shape ) # Numpy array with shape (1, 100, 100, 3) # Rescale by 1/255 x /= 255.0 # Let's run our image through our network, thus obtaining all # intermediate representations for this image. successive_feature_maps = visualization_model . predict ( x ) # These are the names of the layers, so can have them as part of our plot layer_names = [ layer . name for layer in model . layers ] # Now let's display our representations for layer_name , feature_map in zip ( layer_names , successive_feature_maps ): if len ( feature_map . shape ) == 4 : # Just do this for the conv / maxpool layers, not the fully-connected layers n_features = feature_map . shape [ - 1 ] # number of features in feature map n_features = min ( n_features , 5 ) # limit to 5 features for easier viewing # The feature map has shape (1, size, size, n_features) size = feature_map . shape [ 1 ] # We will tile our images in this matrix display_grid = np . zeros (( size , size * n_features )) for i in range ( n_features ): # Postprocess the feature to make it visually palatable x = feature_map [ 0 , :, :, i ] x -= x . mean () x /= x . std () x *= 64 x += 128 x = np . clip ( x , 0 , 255 ) . astype ( 'uint8' ) # We'll tile each filter into this big horizontal grid display_grid [:, i * size : ( i + 1 ) * size ] = x # Display the grid scale = 20. / n_features plt . figure ( figsize = ( scale * n_features , scale )) plt . title ( layer_name ) plt . grid ( False ) plt . imshow ( display_grid , aspect = 'auto' , cmap = 'viridis' ) Clean Up Before running the next exercise, run the following cell to terminate the kernel and free memory resources: import os , signal os . kill ( os . getpid (), signal . SIGKILL )","title":"2 4 3 HorsesOrHumans"},{"location":"2_4_3_HorsesOrHumans/#download-the-neccessary-data-into-the-colab-instance","text":"try : # %tensorflow_version only exists in Colab. % tensorflow_version 2. x except Exception : pass ! wget -- no - check - certificate \\ https : // storage . googleapis . com / laurencemoroney - blog . appspot . com / horse - or - human . zip \\ - O / tmp / horse - or - human . zip ! wget -- no - check - certificate \\ https : // storage . googleapis . com / laurencemoroney - blog . appspot . com / validation - horse - or - human . zip \\ - O / tmp / validation - horse - or - human . zip import os import zipfile local_zip = '/tmp/horse-or-human.zip' zip_ref = zipfile . ZipFile ( local_zip , 'r' ) zip_ref . extractall ( '/tmp/horse-or-human' ) local_zip = '/tmp/validation-horse-or-human.zip' zip_ref = zipfile . ZipFile ( local_zip , 'r' ) zip_ref . extractall ( '/tmp/validation-horse-or-human' ) zip_ref . close () # Directory with our training horse pictures train_horse_dir = os . path . join ( '/tmp/horse-or-human/horses' ) # Directory with our training human pictures train_human_dir = os . path . join ( '/tmp/horse-or-human/humans' ) # Directory with our training horse pictures validation_horse_dir = os . path . join ( '/tmp/validation-horse-or-human/horses' ) # Directory with our training human pictures validation_human_dir = os . path . join ( '/tmp/validation-horse-or-human/humans' ) train_horse_names = os . listdir ( '/tmp/horse-or-human/horses' ) print ( train_horse_names [: 10 ]) train_human_names = os . listdir ( '/tmp/horse-or-human/humans' ) print ( train_human_names [: 10 ]) validation_horse_hames = os . listdir ( '/tmp/validation-horse-or-human/horses' ) print ( validation_horse_hames [: 10 ]) validation_human_names = os . listdir ( '/tmp/validation-horse-or-human/humans' ) print ( validation_human_names [: 10 ]) import tensorflow as tf","title":"Download the neccessary data into the Colab Instance"},{"location":"2_4_3_HorsesOrHumans/#define-your-model-and-optimizer","text":"model = tf . keras . models . Sequential ([ # Note the input shape is the desired size of the image with 3 bytes color # This is the first convolution tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 100 , 100 , 3 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The second convolution tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The third convolution tf . keras . layers . Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The fourth convolution tf . keras . layers . Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # Flatten the results to feed into a DNN tf . keras . layers . Flatten (), # 512 neuron hidden layer tf . keras . layers . Dense ( 512 , activation = 'relu' ), tf . keras . layers . Dense ( 256 , activation = 'relu' ), # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans') tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) ]) print ( model . summary ()) from tensorflow.keras.optimizers import RMSprop optimizer = RMSprop ( lr = 0.0001 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'acc' ])","title":"Define your model and optimizer"},{"location":"2_4_3_HorsesOrHumans/#organize-your-data-into-generators","text":"from tensorflow.keras.preprocessing.image import ImageDataGenerator # All images will be augmented according to whichever lines are uncommented below. # we can first try without any of the augmentation beyond the rescaling train_datagen = ImageDataGenerator ( rescale = 1. / 255 , #rotation_range=40, #width_shift_range=0.2, #height_shift_range=0.2, #shear_range=0.2, #zoom_range=0.2, #horizontal_flip=True, #fill_mode='nearest' ) # Flow training images in batches of 128 using train_datagen generator train_generator = train_datagen . flow_from_directory ( '/tmp/horse-or-human/' , # This is the source directory for training images target_size = ( 100 , 100 ), # All images will be resized to 100x100 batch_size = 128 , # Since we use binary_crossentropy loss, we need binary labels class_mode = 'binary' ) validation_datagen = ImageDataGenerator ( rescale = 1. / 255 ) validation_generator = validation_datagen . flow_from_directory ( '/tmp/validation-horse-or-human' , target_size = ( 100 , 100 ), class_mode = 'binary' )","title":"Organize your data into Generators"},{"location":"2_4_3_HorsesOrHumans/#train-your-model","text":"This may take a little while. Remember we are now building and training relatively complex computer vision models! history = model . fit ( train_generator , steps_per_epoch = 8 , epochs = 100 , verbose = 1 , validation_data = validation_generator )","title":"Train your model"},{"location":"2_4_3_HorsesOrHumans/#run-your-model","text":"Let's now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model, giving an indication of whether the object is a horse or a human. Was the model correct? Try a couple more images and see if you can confuse it! import numpy as np from google.colab import files from keras.preprocessing import image uploaded = files . upload () for fn in uploaded . keys (): # predicting images path = '/content/' + fn img = image . load_img ( path , target_size = ( 100 , 100 )) x = image . img_to_array ( img ) x = x / 255.0 x = np . expand_dims ( x , axis = 0 ) image_tensor = np . vstack ([ x ]) classes = model . predict ( image_tensor ) print ( classes ) print ( classes [ 0 ]) if classes [ 0 ] > 0.5 : print ( fn + \" is a human\" ) else : print ( fn + \" is a horse\" )","title":"Run your Model"},{"location":"2_4_3_HorsesOrHumans/#finally-lets-visualize-all-of-the-model-layers","text":"import matplotlib.pyplot as plt import numpy as np import random from tensorflow.keras.preprocessing.image import img_to_array , load_img % matplotlib inline import matplotlib.pyplot as plt import matplotlib.image as mpimg # Let's define a new Model that will take an image as input, and will output # intermediate representations for all layers in the previous model after the first. successive_outputs = [ layer . output for layer in model . layers [ 1 :]] visualization_model = tf . keras . models . Model ( inputs = model . input , outputs = successive_outputs ) # Let's prepare a random input image from the training set. horse_img_files = [ os . path . join ( train_horse_dir , f ) for f in train_horse_names ] human_img_files = [ os . path . join ( train_human_dir , f ) for f in train_human_names ] img_path = random . choice ( horse_img_files + human_img_files ) # uncomment the following line if you want to pick the Xth human file manually img_path = human_img_files [ 0 ] img = load_img ( img_path , target_size = ( 100 , 100 )) # this is a PIL image x = img_to_array ( img ) # Numpy array with shape (100, 100, 3) x = x . reshape (( 1 ,) + x . shape ) # Numpy array with shape (1, 100, 100, 3) # Rescale by 1/255 x /= 255.0 # Let's run our image through our network, thus obtaining all # intermediate representations for this image. successive_feature_maps = visualization_model . predict ( x ) # These are the names of the layers, so can have them as part of our plot layer_names = [ layer . name for layer in model . layers ] # Now let's display our representations for layer_name , feature_map in zip ( layer_names , successive_feature_maps ): if len ( feature_map . shape ) == 4 : # Just do this for the conv / maxpool layers, not the fully-connected layers n_features = feature_map . shape [ - 1 ] # number of features in feature map n_features = min ( n_features , 5 ) # limit to 5 features for easier viewing # The feature map has shape (1, size, size, n_features) size = feature_map . shape [ 1 ] # We will tile our images in this matrix display_grid = np . zeros (( size , size * n_features )) for i in range ( n_features ): # Postprocess the feature to make it visually palatable x = feature_map [ 0 , :, :, i ] x -= x . mean () x /= x . std () x *= 64 x += 128 x = np . clip ( x , 0 , 255 ) . astype ( 'uint8' ) # We'll tile each filter into this big horizontal grid display_grid [:, i * size : ( i + 1 ) * size ] = x # Display the grid scale = 20. / n_features plt . figure ( figsize = ( scale * n_features , scale )) plt . title ( layer_name ) plt . grid ( False ) plt . imshow ( display_grid , aspect = 'auto' , cmap = 'viridis' )","title":"Finally lets visualize all of the model layers!"},{"location":"2_4_3_HorsesOrHumans/#clean-up","text":"Before running the next exercise, run the following cell to terminate the kernel and free memory resources: import os , signal os . kill ( os . getpid (), signal . SIGKILL )","title":"Clean Up"},{"location":"2_4_6_HorseOrHumanWithAugmentation/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); #@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Again start by downloading the neccessary data into the Colab Instance try : # %tensorflow_version only exists in Colab. % tensorflow_version 2. x except Exception : pass ! wget -- no - check - certificate \\ https : // storage . googleapis . com / laurencemoroney - blog . appspot . com / horse - or - human . zip \\ - O / tmp / horse - or - human . zip ! wget -- no - check - certificate \\ https : // storage . googleapis . com / laurencemoroney - blog . appspot . com / validation - horse - or - human . zip \\ - O / tmp / validation - horse - or - human . zip import os import zipfile local_zip = '/tmp/horse-or-human.zip' zip_ref = zipfile . ZipFile ( local_zip , 'r' ) zip_ref . extractall ( '/tmp/horse-or-human' ) local_zip = '/tmp/validation-horse-or-human.zip' zip_ref = zipfile . ZipFile ( local_zip , 'r' ) zip_ref . extractall ( '/tmp/validation-horse-or-human' ) zip_ref . close () # Directory with our training horse pictures train_horse_dir = os . path . join ( '/tmp/horse-or-human/horses' ) # Directory with our training human pictures train_human_dir = os . path . join ( '/tmp/horse-or-human/humans' ) # Directory with our training horse pictures validation_horse_dir = os . path . join ( '/tmp/validation-horse-or-human/horses' ) # Directory with our training human pictures validation_human_dir = os . path . join ( '/tmp/validation-horse-or-human/humans' ) train_horse_names = os . listdir ( '/tmp/horse-or-human/horses' ) print ( train_horse_names [: 10 ]) train_human_names = os . listdir ( '/tmp/horse-or-human/humans' ) print ( train_human_names [: 10 ]) validation_horse_hames = os . listdir ( '/tmp/validation-horse-or-human/horses' ) print ( validation_horse_hames [: 10 ]) validation_human_names = os . listdir ( '/tmp/validation-horse-or-human/humans' ) print ( validation_human_names [: 10 ]) import tensorflow as tf Then again define your model and optimizer model = tf . keras . models . Sequential ([ # Note the input shape is the desired size of the image 100x100 with 3 bytes color # This is the first convolution tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 100 , 100 , 3 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The second convolution tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The third convolution tf . keras . layers . Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The fourth convolution tf . keras . layers . Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # Flatten the results to feed into a DNN tf . keras . layers . Flatten (), # 512 neuron hidden layer tf . keras . layers . Dense ( 512 , activation = 'relu' ), tf . keras . layers . Dense ( 256 , activation = 'relu' ), # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans') tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) ]) print ( model . summary ()) from tensorflow.keras.optimizers import RMSprop optimizer = tf . keras . optimizers . Adam ( lr = 0.0001 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'acc' ]) Now when we organize the data into Generators note how we use many more kinds of Data Augmentation! from tensorflow.keras.preprocessing.image import ImageDataGenerator # All images will be augmented wiht the full list of augmentation techniques below train_datagen = ImageDataGenerator ( rescale = 1. / 255 , rotation_range = 20 , width_shift_range = 0.2 , height_shift_range = 0.2 , shear_range = 0.2 , zoom_range = 0.2 , horizontal_flip = True , fill_mode = 'nearest' ) # Flow training images in batches of 128 using train_datagen generator train_generator = train_datagen . flow_from_directory ( '/tmp/horse-or-human/' , # This is the source directory for training images target_size = ( 100 , 100 ), # All images will be resized to 300x300 batch_size = 128 , # Since we use binary_crossentropy loss, we need binary labels class_mode = 'binary' ) validation_datagen = ImageDataGenerator ( rescale = 1. / 255 ) validation_generator = validation_datagen . flow_from_directory ( '/tmp/validation-horse-or-human' , target_size = ( 100 , 100 ), class_mode = 'binary' ) Train your model with the new augmented data Since we now have more data due to the data augmentation this training process will take a bit longer than the last time. However, you'll find that the results are much better! history = model . fit ( train_generator , steps_per_epoch = 8 , epochs = 100 , verbose = 1 , validation_data = validation_generator ) Try Running the Model Again Can you confuse it this time? Or did the extra data augmentation help the model generalize? What do you think it was about your confusing examples that are no longer confusing (or what is still confusing)? import numpy as np from google.colab import files from keras.preprocessing import image uploaded = files . upload () for fn in uploaded . keys (): # predicting images path = '/content/' + fn img = image . load_img ( path , target_size = ( 100 , 100 )) x = image . img_to_array ( img ) x = x / 255.0 x = np . expand_dims ( x , axis = 0 ) image_tensor = np . vstack ([ x ]) classes = model . predict ( image_tensor ) print ( classes ) print ( classes [ 0 ]) if classes [ 0 ] > 0.5 : print ( fn + \" is a human\" ) else : print ( fn + \" is a horse\" ) Finally again lets visualize some of the layers for intuition import numpy as np import random from tensorflow.keras.preprocessing.image import img_to_array , load_img % matplotlib inline import matplotlib.pyplot as plt import matplotlib.image as mpimg # Let's define a new Model that will take an image as input, and will output # intermediate representations for all layers in the previous model after the first. successive_outputs = [ layer . output for layer in model . layers [ 1 :]] visualization_model = tf . keras . models . Model ( inputs = model . input , outputs = successive_outputs ) # Let's prepare a random input image from the training set. horse_img_files = [ os . path . join ( train_horse_dir , f ) for f in train_horse_names ] human_img_files = [ os . path . join ( train_human_dir , f ) for f in train_human_names ] img_path = random . choice ( horse_img_files + human_img_files ) # uncomment the following line if you want to pick the Xth human file manually # img_path = human_img_files[0] img = load_img ( img_path , target_size = ( 100 , 100 )) # this is a PIL image x = img_to_array ( img ) # Numpy array with shape (100, 100, 3) x = x . reshape (( 1 ,) + x . shape ) # Numpy array with shape (1, 100, 100, 3) # Rescale by 1/255 x /= 255.0 # Let's run our image through our network, thus obtaining all # intermediate representations for this image. successive_feature_maps = visualization_model . predict ( x ) # These are the names of the layers, so can have them as part of our plot layer_names = [ layer . name for layer in model . layers [ 1 :]] # Now let's display our representations for layer_name , feature_map in zip ( layer_names , successive_feature_maps ): if len ( feature_map . shape ) == 4 : # Just do this for the conv / maxpool layers, not the fully-connected layers n_features = feature_map . shape [ - 1 ] # number of features in feature map n_features = min ( n_features , 5 ) # limit to 5 features for easier viewing # The feature map has shape (1, size, size, n_features) size = feature_map . shape [ 1 ] # We will tile our images in this matrix display_grid = np . zeros (( size , size * n_features )) for i in range ( n_features ): # Postprocess the feature to make it visually palatable x = feature_map [ 0 , :, :, i ] x -= x . mean () x /= x . std () x *= 64 x += 128 x = np . clip ( x , 0 , 255 ) . astype ( 'uint8' ) # We'll tile each filter into this big horizontal grid display_grid [:, i * size : ( i + 1 ) * size ] = x # Display the grid scale = 20. / n_features plt . figure ( figsize = ( scale * n_features , scale )) #plt.title(layer_name) plt . grid ( False ) plt . imshow ( display_grid , aspect = 'auto' , cmap = 'viridis' ) Clean Up Before running the next exercise, run the following cell to terminate the kernel and free memory resources: import os , signal os . kill ( os . getpid (), signal . SIGKILL )","title":"2 4 6 HorseOrHumanWithAugmentation"},{"location":"2_4_6_HorseOrHumanWithAugmentation/#again-start-by-downloading-the-neccessary-data-into-the-colab-instance","text":"try : # %tensorflow_version only exists in Colab. % tensorflow_version 2. x except Exception : pass ! wget -- no - check - certificate \\ https : // storage . googleapis . com / laurencemoroney - blog . appspot . com / horse - or - human . zip \\ - O / tmp / horse - or - human . zip ! wget -- no - check - certificate \\ https : // storage . googleapis . com / laurencemoroney - blog . appspot . com / validation - horse - or - human . zip \\ - O / tmp / validation - horse - or - human . zip import os import zipfile local_zip = '/tmp/horse-or-human.zip' zip_ref = zipfile . ZipFile ( local_zip , 'r' ) zip_ref . extractall ( '/tmp/horse-or-human' ) local_zip = '/tmp/validation-horse-or-human.zip' zip_ref = zipfile . ZipFile ( local_zip , 'r' ) zip_ref . extractall ( '/tmp/validation-horse-or-human' ) zip_ref . close () # Directory with our training horse pictures train_horse_dir = os . path . join ( '/tmp/horse-or-human/horses' ) # Directory with our training human pictures train_human_dir = os . path . join ( '/tmp/horse-or-human/humans' ) # Directory with our training horse pictures validation_horse_dir = os . path . join ( '/tmp/validation-horse-or-human/horses' ) # Directory with our training human pictures validation_human_dir = os . path . join ( '/tmp/validation-horse-or-human/humans' ) train_horse_names = os . listdir ( '/tmp/horse-or-human/horses' ) print ( train_horse_names [: 10 ]) train_human_names = os . listdir ( '/tmp/horse-or-human/humans' ) print ( train_human_names [: 10 ]) validation_horse_hames = os . listdir ( '/tmp/validation-horse-or-human/horses' ) print ( validation_horse_hames [: 10 ]) validation_human_names = os . listdir ( '/tmp/validation-horse-or-human/humans' ) print ( validation_human_names [: 10 ]) import tensorflow as tf","title":"Again start by downloading the neccessary data into the Colab Instance"},{"location":"2_4_6_HorseOrHumanWithAugmentation/#then-again-define-your-model-and-optimizer","text":"model = tf . keras . models . Sequential ([ # Note the input shape is the desired size of the image 100x100 with 3 bytes color # This is the first convolution tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 100 , 100 , 3 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The second convolution tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The third convolution tf . keras . layers . Conv2D ( 128 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # The fourth convolution tf . keras . layers . Conv2D ( 256 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), # Flatten the results to feed into a DNN tf . keras . layers . Flatten (), # 512 neuron hidden layer tf . keras . layers . Dense ( 512 , activation = 'relu' ), tf . keras . layers . Dense ( 256 , activation = 'relu' ), # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans') tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) ]) print ( model . summary ()) from tensorflow.keras.optimizers import RMSprop optimizer = tf . keras . optimizers . Adam ( lr = 0.0001 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'acc' ])","title":"Then again define your model and optimizer"},{"location":"2_4_6_HorseOrHumanWithAugmentation/#now-when-we-organize-the-data-into-generators-note-how-we-use-many-more-kinds-of-data-augmentation","text":"from tensorflow.keras.preprocessing.image import ImageDataGenerator # All images will be augmented wiht the full list of augmentation techniques below train_datagen = ImageDataGenerator ( rescale = 1. / 255 , rotation_range = 20 , width_shift_range = 0.2 , height_shift_range = 0.2 , shear_range = 0.2 , zoom_range = 0.2 , horizontal_flip = True , fill_mode = 'nearest' ) # Flow training images in batches of 128 using train_datagen generator train_generator = train_datagen . flow_from_directory ( '/tmp/horse-or-human/' , # This is the source directory for training images target_size = ( 100 , 100 ), # All images will be resized to 300x300 batch_size = 128 , # Since we use binary_crossentropy loss, we need binary labels class_mode = 'binary' ) validation_datagen = ImageDataGenerator ( rescale = 1. / 255 ) validation_generator = validation_datagen . flow_from_directory ( '/tmp/validation-horse-or-human' , target_size = ( 100 , 100 ), class_mode = 'binary' )","title":"Now when we organize the data into Generators note how we use many more kinds of Data Augmentation!"},{"location":"2_4_6_HorseOrHumanWithAugmentation/#train-your-model-with-the-new-augmented-data","text":"Since we now have more data due to the data augmentation this training process will take a bit longer than the last time. However, you'll find that the results are much better! history = model . fit ( train_generator , steps_per_epoch = 8 , epochs = 100 , verbose = 1 , validation_data = validation_generator )","title":"Train your model with the new augmented data"},{"location":"2_4_6_HorseOrHumanWithAugmentation/#try-running-the-model-again","text":"Can you confuse it this time? Or did the extra data augmentation help the model generalize? What do you think it was about your confusing examples that are no longer confusing (or what is still confusing)? import numpy as np from google.colab import files from keras.preprocessing import image uploaded = files . upload () for fn in uploaded . keys (): # predicting images path = '/content/' + fn img = image . load_img ( path , target_size = ( 100 , 100 )) x = image . img_to_array ( img ) x = x / 255.0 x = np . expand_dims ( x , axis = 0 ) image_tensor = np . vstack ([ x ]) classes = model . predict ( image_tensor ) print ( classes ) print ( classes [ 0 ]) if classes [ 0 ] > 0.5 : print ( fn + \" is a human\" ) else : print ( fn + \" is a horse\" )","title":"Try Running the Model Again"},{"location":"2_4_6_HorseOrHumanWithAugmentation/#finally-again-lets-visualize-some-of-the-layers-for-intuition","text":"import numpy as np import random from tensorflow.keras.preprocessing.image import img_to_array , load_img % matplotlib inline import matplotlib.pyplot as plt import matplotlib.image as mpimg # Let's define a new Model that will take an image as input, and will output # intermediate representations for all layers in the previous model after the first. successive_outputs = [ layer . output for layer in model . layers [ 1 :]] visualization_model = tf . keras . models . Model ( inputs = model . input , outputs = successive_outputs ) # Let's prepare a random input image from the training set. horse_img_files = [ os . path . join ( train_horse_dir , f ) for f in train_horse_names ] human_img_files = [ os . path . join ( train_human_dir , f ) for f in train_human_names ] img_path = random . choice ( horse_img_files + human_img_files ) # uncomment the following line if you want to pick the Xth human file manually # img_path = human_img_files[0] img = load_img ( img_path , target_size = ( 100 , 100 )) # this is a PIL image x = img_to_array ( img ) # Numpy array with shape (100, 100, 3) x = x . reshape (( 1 ,) + x . shape ) # Numpy array with shape (1, 100, 100, 3) # Rescale by 1/255 x /= 255.0 # Let's run our image through our network, thus obtaining all # intermediate representations for this image. successive_feature_maps = visualization_model . predict ( x ) # These are the names of the layers, so can have them as part of our plot layer_names = [ layer . name for layer in model . layers [ 1 :]] # Now let's display our representations for layer_name , feature_map in zip ( layer_names , successive_feature_maps ): if len ( feature_map . shape ) == 4 : # Just do this for the conv / maxpool layers, not the fully-connected layers n_features = feature_map . shape [ - 1 ] # number of features in feature map n_features = min ( n_features , 5 ) # limit to 5 features for easier viewing # The feature map has shape (1, size, size, n_features) size = feature_map . shape [ 1 ] # We will tile our images in this matrix display_grid = np . zeros (( size , size * n_features )) for i in range ( n_features ): # Postprocess the feature to make it visually palatable x = feature_map [ 0 , :, :, i ] x -= x . mean () x /= x . std () x *= 64 x += 128 x = np . clip ( x , 0 , 255 ) . astype ( 'uint8' ) # We'll tile each filter into this big horizontal grid display_grid [:, i * size : ( i + 1 ) * size ] = x # Display the grid scale = 20. / n_features plt . figure ( figsize = ( scale * n_features , scale )) #plt.title(layer_name) plt . grid ( False ) plt . imshow ( display_grid , aspect = 'auto' , cmap = 'viridis' )","title":"Finally again lets visualize some of the layers for intuition"},{"location":"2_4_6_HorseOrHumanWithAugmentation/#clean-up","text":"Before running the next exercise, run the following cell to terminate the kernel and free memory resources: import os , signal os . kill ( os . getpid (), signal . SIGKILL )","title":"Clean Up"},{"location":"Assignment_2_1_13_question/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Your first assignment In this assignment you will design your own linear regression model using TensorFlow. If you are taking the course for a certificate during the end of section test you will be asked questions about this assignment. We start by setting up the problem for you. # DO NOT CHANGE THIS CELL # We first import TensorFlow and other libraries import tensorflow as tf import numpy as np from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense import matplotlib.pyplot as plt # We then set up some functions and local variables predictions = [] class myCallback ( tf . keras . callbacks . Callback ): def on_epoch_end ( self , epoch , logs = {}): predictions . append ( model . predict ( xs )) callbacks = myCallback () # We then define the xs (inputs) and ys (outputs) xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) ys = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ], dtype = float ) Please fill out the cell below to define your model and compile it. A hint: your model may want to learn the average error over the dataset. # PLEASE COMPELTE THIS CELL SHAPE = #YOUR CODE HERE# LOSS = #YOUR CODE HERE# # Define your model type model = Sequential ([ Dense ( units = 1 , input_shape = SHAPE )]) # Compile your model with choice of optimizer and loss function model . compile ( optimizer = 'sgd' , loss = LOSS ) We then include the call to fit your model to the data. # DO NOT CHANGE THIS CELL # We then fit the model model . fit ( xs , ys , epochs = 300 , callbacks = [ callbacks ], verbose = 2 ) We then plot the resulting prediction at EPOCH_NUMBERS = 1,25,50,150,300. If you'd like to see other Epochs simply update the EPOCH_NUMBERS variable and re-run the cell! EPOCH_NUMBERS = [ 1 , 25 , 50 , 150 , 300 ] # Update me to see other Epochs plt . plot ( xs , ys , label = \"Ys\" ) for EPOCH in EPOCH_NUMBERS : plt . plot ( xs , predictions [ EPOCH - 1 ], label = \"Epoch = \" + str ( EPOCH )) plt . legend () plt . show ()","title":"Assignment 2 1 13 question"},{"location":"Assignment_2_1_13_question/#your-first-assignment","text":"In this assignment you will design your own linear regression model using TensorFlow. If you are taking the course for a certificate during the end of section test you will be asked questions about this assignment. We start by setting up the problem for you. # DO NOT CHANGE THIS CELL # We first import TensorFlow and other libraries import tensorflow as tf import numpy as np from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense import matplotlib.pyplot as plt # We then set up some functions and local variables predictions = [] class myCallback ( tf . keras . callbacks . Callback ): def on_epoch_end ( self , epoch , logs = {}): predictions . append ( model . predict ( xs )) callbacks = myCallback () # We then define the xs (inputs) and ys (outputs) xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) ys = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ], dtype = float ) Please fill out the cell below to define your model and compile it. A hint: your model may want to learn the average error over the dataset. # PLEASE COMPELTE THIS CELL SHAPE = #YOUR CODE HERE# LOSS = #YOUR CODE HERE# # Define your model type model = Sequential ([ Dense ( units = 1 , input_shape = SHAPE )]) # Compile your model with choice of optimizer and loss function model . compile ( optimizer = 'sgd' , loss = LOSS ) We then include the call to fit your model to the data. # DO NOT CHANGE THIS CELL # We then fit the model model . fit ( xs , ys , epochs = 300 , callbacks = [ callbacks ], verbose = 2 ) We then plot the resulting prediction at EPOCH_NUMBERS = 1,25,50,150,300. If you'd like to see other Epochs simply update the EPOCH_NUMBERS variable and re-run the cell! EPOCH_NUMBERS = [ 1 , 25 , 50 , 150 , 300 ] # Update me to see other Epochs plt . plot ( xs , ys , label = \"Ys\" ) for EPOCH in EPOCH_NUMBERS : plt . plot ( xs , predictions [ EPOCH - 1 ], label = \"Epoch = \" + str ( EPOCH )) plt . legend () plt . show ()","title":"Your first assignment"},{"location":"ExploringLoss/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import math # Edit these parameters to try different loss # measurements. Rerun this cell when done # Your Y will be calculated as Y=wX+b, so # if w=3, and b=-1, then Y=3x-1 w = 3 b = - 1 x = [ - 1 , 0 , 1 , 2 , 3 , 4 ] y = [ - 3 , - 1 , 1 , 3 , 5 , 7 ] myY = [] for thisX in x : thisY = ( w * thisX ) + b myY . append ( thisY ) print ( \"Real Y is \" + str ( y )) print ( \"My Y is \" + str ( myY )) # let's calculate the loss total_square_error = 0 for i in range ( 0 , len ( y )): square_error = ( y [ i ] - myY [ i ]) ** 2 total_square_error += square_error print ( \"My loss is: \" + str ( math . sqrt ( total_square_error ))) Real Y is [-3, -1, 1, 3, 5, 7] My Y is [-4, -1, 2, 5, 8, 11] My loss is: 5.5677643628300215","title":"ExploringLoss"},{"location":"Exploring_Categorical/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Start with a simple neural network for MNIST Note that there are 2 layers, one with 20 neurons, and one with 10. The 10-neuron layer is our final layer because we have 10 classes we want to classify. Train this, and you should see it get about 98% accuracy import tensorflow as tf data = tf . keras . datasets . mnist ( training_images , training_labels ), ( val_images , val_labels ) = data . load_data () training_images = training_images / 255.0 val_images = val_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 20 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 20 , validation_data = ( val_images , val_labels )) Epoch 1/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4104 - accuracy: 0.8838 - val_loss: 0.2347 - val_accuracy: 0.9304 Epoch 2/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.2216 - accuracy: 0.9364 - val_loss: 0.1985 - val_accuracy: 0.9410 Epoch 3/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1879 - accuracy: 0.9463 - val_loss: 0.1734 - val_accuracy: 0.9484 Epoch 4/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1675 - accuracy: 0.9519 - val_loss: 0.1609 - val_accuracy: 0.9519 Epoch 5/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1514 - accuracy: 0.9565 - val_loss: 0.1519 - val_accuracy: 0.9561 Epoch 6/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1408 - accuracy: 0.9589 - val_loss: 0.1457 - val_accuracy: 0.9575 Epoch 7/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1308 - accuracy: 0.9621 - val_loss: 0.1422 - val_accuracy: 0.9600 Epoch 8/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1234 - accuracy: 0.9639 - val_loss: 0.1345 - val_accuracy: 0.9602 Epoch 9/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1171 - accuracy: 0.9657 - val_loss: 0.1385 - val_accuracy: 0.9593 Epoch 10/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1122 - accuracy: 0.9671 - val_loss: 0.1368 - val_accuracy: 0.9606 Epoch 11/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1071 - accuracy: 0.9688 - val_loss: 0.1324 - val_accuracy: 0.9616 Epoch 12/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1029 - accuracy: 0.9697 - val_loss: 0.1346 - val_accuracy: 0.9615 Epoch 13/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0995 - accuracy: 0.9701 - val_loss: 0.1385 - val_accuracy: 0.9596 Epoch 14/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0960 - accuracy: 0.9713 - val_loss: 0.1410 - val_accuracy: 0.9588 Epoch 15/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0933 - accuracy: 0.9722 - val_loss: 0.1353 - val_accuracy: 0.9626 Epoch 16/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0910 - accuracy: 0.9729 - val_loss: 0.1356 - val_accuracy: 0.9622 Epoch 17/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0880 - accuracy: 0.9734 - val_loss: 0.1339 - val_accuracy: 0.9625 Epoch 18/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0855 - accuracy: 0.9740 - val_loss: 0.1349 - val_accuracy: 0.9619 Epoch 19/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0832 - accuracy: 0.9754 - val_loss: 0.1341 - val_accuracy: 0.9624 Epoch 20/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0808 - accuracy: 0.9759 - val_loss: 0.1340 - val_accuracy: 0.9626 <tensorflow.python.keras.callbacks.History at 0x7feecc40e748> Examine the test data Using model.evaluate, you can get metrics for a test set. In this case we only have a training set and a validation set, so we can try it out with the validation set. The accuracy will be slightly lower, at maybe 96%. This is because the model hasn't previously seen this data and may not be fully generalized for all data. Still it's a pretty good score. You can also predict images, and compare against their actual label. The [0] image in the set is a number 7, and here you can see that neuron 7 has a 9.9e-1 (99%+) probability, so it got it right! model . evaluate ( val_images , val_labels ) classifications = model . predict ( val_images ) print ( classifications [ 0 ]) print ( val_labels [ 0 ]) 313/313 [==============================] - 1s 2ms/step - loss: 0.1495 - accuracy: 0.9569 [2.4921512e-09 1.3765138e-10 8.8281205e-08 1.0477231e-03 2.8455029e-12 4.0820678e-06 2.0070659e-16 9.9894780e-01 1.0296049e-07 2.9972372e-07] 7 Modify to inspect learned values This code is identical, except that the layers are named prior to adding to the sequential. This allows us to inspect their learned parameters later. import tensorflow as tf data = tf . keras . datasets . mnist ( training_images , training_labels ), ( val_images , val_labels ) = data . load_data () training_images = training_images / 255.0 val_images = val_images / 255.0 layer_1 = tf . keras . layers . Dense ( 20 , activation = tf . nn . relu ) layer_2 = tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), layer_1 , layer_2 ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 20 ) model . evaluate ( val_images , val_labels ) classifications = model . predict ( val_images ) print ( classifications [ 0 ]) print ( val_labels [ 0 ]) Epoch 1/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3985 - accuracy: 0.8888 Epoch 2/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2472 - accuracy: 0.9293 Epoch 3/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2085 - accuracy: 0.9404 Epoch 4/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1803 - accuracy: 0.9483 Epoch 5/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1606 - accuracy: 0.9534 Epoch 6/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1458 - accuracy: 0.9571 Epoch 7/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1359 - accuracy: 0.9600 Epoch 8/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1288 - accuracy: 0.9618 Epoch 9/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1211 - accuracy: 0.9639 Epoch 10/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1151 - accuracy: 0.9658 Epoch 11/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1108 - accuracy: 0.9664 Epoch 12/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1059 - accuracy: 0.9685 Epoch 13/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1017 - accuracy: 0.9702 Epoch 14/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0981 - accuracy: 0.9704 Epoch 15/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0951 - accuracy: 0.9707 Epoch 16/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0927 - accuracy: 0.9720 Epoch 17/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0896 - accuracy: 0.9733 Epoch 18/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0871 - accuracy: 0.9739 Epoch 19/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0845 - accuracy: 0.9741 Epoch 20/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0829 - accuracy: 0.9748 313/313 [==============================] - 1s 2ms/step - loss: 0.1495 - accuracy: 0.9569 [2.4921512e-09 1.3765138e-10 8.8281205e-08 1.0477231e-03 2.8455029e-12 4.0820678e-06 2.0070659e-16 9.9894780e-01 1.0296049e-07 2.9972372e-07] 7 Inspect weights If you print layer_1.get_weights(), you'll see a lot of data. Let's unpack it. First, there are 2 arrays in the result, so let's look at the first one. In particular let's look at its size. print ( layer_1 . get_weights ()[ 0 ] . size ) 15680 The above code should print 15680. Why? Recall that there are 20 neurons in the first layer. Recall also that the images are 28x28, which is 784. If you multiply 784 x 20 you get 15680. So...this layer has 20 neurons, and each neuron learns a W parameter for each pixel. So instead of y=Mx+c, we have y=M1X1+M2X2+M3X3+....+M784X784+C in every neuron! Every pixel has a weight in every neuron. Those weights are multiplied by the pixel value, summed up, and given a bias. print ( layer_1 . get_weights ()[ 1 ] . size ) 20 The above code will give you 20 -- the get_weights()[1] contains the biases for each of the 20 neurons in this layer. Inspecting layer 2 Now let's look at layer 2. Printing the get_weights will give us 2 lists, the first a list of weights for the 10 neurons, and the second a list of biases for the 10 neurons Let's look first at the weights: print ( layer_2 . get_weights ()[ 0 ] . size ) 200 This should return 200. Again, consider why? There are 10 neurons in this layer, but there are 20 neurons in the previous layer. So, each neuron in this layer will learn a weight for the incoming value from the previous layer. So, for example, the if the first neuron in this layer is N21, and the neurons output from the previous layers are N11-N120, then this neuron will have 20 weights (W1-W20) and it will calculate its output to be: W1N11+W2N12+W3N13+...+W20N120+Bias So each of these weights will be learned as will the bias, for every neuron. Note that N11 refers to Layer 1 Neuron 1. print ( layer_2 . get_weights ()[ 1 ] . size ) 10 ...and as expected there are 10 elements in this array, representing the 10 biases for the 10 neurons. Hopefully this helps you see how the element of a simple neuron containing y=mx+c can be expanded greatly into a deep neural network, and that DNN can learn the parameters that match the 784 pixels of an image to their output!","title":"Exploring Categorical"},{"location":"Exploring_Categorical/#start-with-a-simple-neural-network-for-mnist","text":"Note that there are 2 layers, one with 20 neurons, and one with 10. The 10-neuron layer is our final layer because we have 10 classes we want to classify. Train this, and you should see it get about 98% accuracy import tensorflow as tf data = tf . keras . datasets . mnist ( training_images , training_labels ), ( val_images , val_labels ) = data . load_data () training_images = training_images / 255.0 val_images = val_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 20 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 20 , validation_data = ( val_images , val_labels )) Epoch 1/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4104 - accuracy: 0.8838 - val_loss: 0.2347 - val_accuracy: 0.9304 Epoch 2/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.2216 - accuracy: 0.9364 - val_loss: 0.1985 - val_accuracy: 0.9410 Epoch 3/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1879 - accuracy: 0.9463 - val_loss: 0.1734 - val_accuracy: 0.9484 Epoch 4/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1675 - accuracy: 0.9519 - val_loss: 0.1609 - val_accuracy: 0.9519 Epoch 5/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1514 - accuracy: 0.9565 - val_loss: 0.1519 - val_accuracy: 0.9561 Epoch 6/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1408 - accuracy: 0.9589 - val_loss: 0.1457 - val_accuracy: 0.9575 Epoch 7/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1308 - accuracy: 0.9621 - val_loss: 0.1422 - val_accuracy: 0.9600 Epoch 8/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1234 - accuracy: 0.9639 - val_loss: 0.1345 - val_accuracy: 0.9602 Epoch 9/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1171 - accuracy: 0.9657 - val_loss: 0.1385 - val_accuracy: 0.9593 Epoch 10/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1122 - accuracy: 0.9671 - val_loss: 0.1368 - val_accuracy: 0.9606 Epoch 11/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1071 - accuracy: 0.9688 - val_loss: 0.1324 - val_accuracy: 0.9616 Epoch 12/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1029 - accuracy: 0.9697 - val_loss: 0.1346 - val_accuracy: 0.9615 Epoch 13/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0995 - accuracy: 0.9701 - val_loss: 0.1385 - val_accuracy: 0.9596 Epoch 14/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0960 - accuracy: 0.9713 - val_loss: 0.1410 - val_accuracy: 0.9588 Epoch 15/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0933 - accuracy: 0.9722 - val_loss: 0.1353 - val_accuracy: 0.9626 Epoch 16/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0910 - accuracy: 0.9729 - val_loss: 0.1356 - val_accuracy: 0.9622 Epoch 17/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0880 - accuracy: 0.9734 - val_loss: 0.1339 - val_accuracy: 0.9625 Epoch 18/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0855 - accuracy: 0.9740 - val_loss: 0.1349 - val_accuracy: 0.9619 Epoch 19/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0832 - accuracy: 0.9754 - val_loss: 0.1341 - val_accuracy: 0.9624 Epoch 20/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0808 - accuracy: 0.9759 - val_loss: 0.1340 - val_accuracy: 0.9626 <tensorflow.python.keras.callbacks.History at 0x7feecc40e748>","title":"Start with a simple neural network for MNIST"},{"location":"Exploring_Categorical/#examine-the-test-data","text":"Using model.evaluate, you can get metrics for a test set. In this case we only have a training set and a validation set, so we can try it out with the validation set. The accuracy will be slightly lower, at maybe 96%. This is because the model hasn't previously seen this data and may not be fully generalized for all data. Still it's a pretty good score. You can also predict images, and compare against their actual label. The [0] image in the set is a number 7, and here you can see that neuron 7 has a 9.9e-1 (99%+) probability, so it got it right! model . evaluate ( val_images , val_labels ) classifications = model . predict ( val_images ) print ( classifications [ 0 ]) print ( val_labels [ 0 ]) 313/313 [==============================] - 1s 2ms/step - loss: 0.1495 - accuracy: 0.9569 [2.4921512e-09 1.3765138e-10 8.8281205e-08 1.0477231e-03 2.8455029e-12 4.0820678e-06 2.0070659e-16 9.9894780e-01 1.0296049e-07 2.9972372e-07] 7","title":"Examine the test data"},{"location":"Exploring_Categorical/#modify-to-inspect-learned-values","text":"This code is identical, except that the layers are named prior to adding to the sequential. This allows us to inspect their learned parameters later. import tensorflow as tf data = tf . keras . datasets . mnist ( training_images , training_labels ), ( val_images , val_labels ) = data . load_data () training_images = training_images / 255.0 val_images = val_images / 255.0 layer_1 = tf . keras . layers . Dense ( 20 , activation = tf . nn . relu ) layer_2 = tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), layer_1 , layer_2 ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 20 ) model . evaluate ( val_images , val_labels ) classifications = model . predict ( val_images ) print ( classifications [ 0 ]) print ( val_labels [ 0 ]) Epoch 1/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3985 - accuracy: 0.8888 Epoch 2/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2472 - accuracy: 0.9293 Epoch 3/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2085 - accuracy: 0.9404 Epoch 4/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1803 - accuracy: 0.9483 Epoch 5/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1606 - accuracy: 0.9534 Epoch 6/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1458 - accuracy: 0.9571 Epoch 7/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1359 - accuracy: 0.9600 Epoch 8/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1288 - accuracy: 0.9618 Epoch 9/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1211 - accuracy: 0.9639 Epoch 10/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1151 - accuracy: 0.9658 Epoch 11/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1108 - accuracy: 0.9664 Epoch 12/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1059 - accuracy: 0.9685 Epoch 13/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1017 - accuracy: 0.9702 Epoch 14/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0981 - accuracy: 0.9704 Epoch 15/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0951 - accuracy: 0.9707 Epoch 16/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0927 - accuracy: 0.9720 Epoch 17/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0896 - accuracy: 0.9733 Epoch 18/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0871 - accuracy: 0.9739 Epoch 19/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0845 - accuracy: 0.9741 Epoch 20/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0829 - accuracy: 0.9748 313/313 [==============================] - 1s 2ms/step - loss: 0.1495 - accuracy: 0.9569 [2.4921512e-09 1.3765138e-10 8.8281205e-08 1.0477231e-03 2.8455029e-12 4.0820678e-06 2.0070659e-16 9.9894780e-01 1.0296049e-07 2.9972372e-07] 7","title":"Modify to inspect learned values"},{"location":"Exploring_Categorical/#inspect-weights","text":"If you print layer_1.get_weights(), you'll see a lot of data. Let's unpack it. First, there are 2 arrays in the result, so let's look at the first one. In particular let's look at its size. print ( layer_1 . get_weights ()[ 0 ] . size ) 15680 The above code should print 15680. Why? Recall that there are 20 neurons in the first layer. Recall also that the images are 28x28, which is 784. If you multiply 784 x 20 you get 15680. So...this layer has 20 neurons, and each neuron learns a W parameter for each pixel. So instead of y=Mx+c, we have y=M1X1+M2X2+M3X3+....+M784X784+C in every neuron! Every pixel has a weight in every neuron. Those weights are multiplied by the pixel value, summed up, and given a bias. print ( layer_1 . get_weights ()[ 1 ] . size ) 20 The above code will give you 20 -- the get_weights()[1] contains the biases for each of the 20 neurons in this layer.","title":"Inspect weights"},{"location":"Exploring_Categorical/#inspecting-layer-2","text":"Now let's look at layer 2. Printing the get_weights will give us 2 lists, the first a list of weights for the 10 neurons, and the second a list of biases for the 10 neurons Let's look first at the weights: print ( layer_2 . get_weights ()[ 0 ] . size ) 200 This should return 200. Again, consider why? There are 10 neurons in this layer, but there are 20 neurons in the previous layer. So, each neuron in this layer will learn a weight for the incoming value from the previous layer. So, for example, the if the first neuron in this layer is N21, and the neurons output from the previous layers are N11-N120, then this neuron will have 20 weights (W1-W20) and it will calculate its output to be: W1N11+W2N12+W3N13+...+W20N120+Bias So each of these weights will be learned as will the bias, for every neuron. Note that N11 refers to Layer 1 Neuron 1. print ( layer_2 . get_weights ()[ 1 ] . size ) 10 ...and as expected there are 10 elements in this array, representing the 10 biases for the 10 neurons. Hopefully this helps you see how the element of a simple neuron containing y=mx+c can be expanded greatly into a deep neural network, and that DNN can learn the parameters that match the 784 pixels of an image to their output!","title":"Inspecting layer 2"},{"location":"Exploring_Convolutions/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); What are Convolutions? What are convolutions? In this lab you'll explore what they are and how they work. In later lessons, you'll see how to use them in your neural network. Together with convolutions, you'll use something called 'Pooling', which compresses your image, further emphasising the features. You'll also see how pooling works in this lab. Limitations of the previous DNN In an earlier exercise you saw how to train an image classifier for fashion items using the Fashion MNIST dataset. This gave you a pretty accuract classifier, but there was an obvious constraint: the images were 28x28, grey scale and the item was centered in the image. For example here are a couple of the images in Fashion MNIST The DNN that you created simply learned from the raw pixels what made up a sweater, and what made up a boot in this context. But consider how it might classify this image? (Image is Public domain CC0 from Pixabay: https://pixabay.com/photos/boots-travel-railroad-tracks-181744/) While it's clear that there are boots in this image, the classifier would fail for a number of reasons. First, of course, it's not 28x28 greyscale, but more importantly, the classifier was trained on the raw pixels of a left-facing boot, and not the features that make up what a boot is. That's where Convolutions are very powerful. A convolution is a filter that passes over an image, processing it, and extracting features that show a commonolatity in the image. In this lab you'll see how they work, but processing an image to see if you can extract features from it! Generating convolutions is very simple -- you simply scan every pixel in the image and then look at it's neighboring pixels. You multiply out the values of these pixels by the equivalent weights in a filter. So, for example, consider this: In this case a 3x3 Convolution is specified. The current pixel value is 192, but you can calculate the new one by looking at the neighbor values, and multiplying them out by the values specified in the filter, and making the new pixel value the final amount. Let's explore how convolutions work by creating a basic convolution on a 2D Grey Scale image. First we can load the image by taking the 'ascent' image from scipy. It's a nice, built-in picture with lots of angles and lines. Let's start by importing some python libraries. import cv2 import numpy as np from scipy import misc i = misc . ascent () Next, we can use the pyplot library to draw the image so we know what it looks like. import matplotlib.pyplot as plt plt . grid ( False ) plt . gray () plt . axis ( 'off' ) plt . imshow ( i ) plt . show () We can see that this is an image of a stairwell. There are lots of features in here that we can play with seeing if we can isolate them -- for example there are strong vertical lines. The image is stored as a numpy array, so we can create the transformed image by just copying that array. Let's also get the dimensions of the image so we can loop over it later. i_transformed = np . copy ( i ) size_x = i_transformed . shape [ 0 ] size_y = i_transformed . shape [ 1 ] Now we can create a filter as a 3x3 array. # This filter detects edges nicely # It creates a convolution that only passes through sharp edges and straight # lines. #Experiment with different values for fun effects. #filter = [ [0, 1, 0], [1, -4, 1], [0, 1, 0]] # A couple more filters to try for fun! filter = [ [ - 1 , - 2 , - 1 ], [ 0 , 0 , 0 ], [ 1 , 2 , 1 ]] #filter = [ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]] # If all the digits in the filter don't add up to 0 or 1, you # should probably do a weight to get it to do so # so, for example, if your weights are 1,1,1 1,2,1 1,1,1 # They add up to 10, so you would set a weight of .1 if you want to normalize them weight = 1 Now let's create a convolution. We will iterate over the image, leaving a 1 pixel margin, and multiply out each of the neighbors of the current pixel by the value defined in the filter. i.e. the current pixel's neighbor above it and to the left will be multiplied by the top left item in the filter etc. etc. We'll then multiply the result by the weight, and then ensure the result is in the range 0-255 Finally we'll load the new value into the transformed image. for x in range ( 1 , size_x - 1 ): for y in range ( 1 , size_y - 1 ): convolution = 0.0 convolution = convolution + ( i [ x - 1 , y - 1 ] * filter [ 0 ][ 0 ]) convolution = convolution + ( i [ x , y - 1 ] * filter [ 1 ][ 0 ]) convolution = convolution + ( i [ x + 1 , y - 1 ] * filter [ 2 ][ 0 ]) convolution = convolution + ( i [ x - 1 , y ] * filter [ 0 ][ 1 ]) convolution = convolution + ( i [ x , y ] * filter [ 1 ][ 1 ]) convolution = convolution + ( i [ x + 1 , y ] * filter [ 2 ][ 1 ]) convolution = convolution + ( i [ x - 1 , y + 1 ] * filter [ 0 ][ 2 ]) convolution = convolution + ( i [ x , y + 1 ] * filter [ 1 ][ 2 ]) convolution = convolution + ( i [ x + 1 , y + 1 ] * filter [ 2 ][ 2 ]) convolution = convolution * weight if ( convolution < 0 ): convolution = 0 if ( convolution > 255 ): convolution = 255 i_transformed [ x , y ] = convolution Now we can plot the image to see the effect of the convolution! # Plot the image. Note the size of the axes -- they are 512 by 512 plt . gray () plt . grid ( False ) plt . imshow ( i_transformed ) #plt.axis('off') plt . show () So, consider the following filter values, and their impact on the image. Using -1,0,1,-2,0,2,-1,0,1 gives us a very strong set of vertical lines: Using -1, -2, -1, 0, 0, 0, 1, 2, 1 gives us horizontal lines: Explore different values for yourself! Pooling As well as using convolutions, pooling helps us greatly in detecting features. The goal is to reduce the overall amount of information in an image, while maintaining the features that are detected as present. There are a number of different types of pooling, but for this lab we'll use one called MAX pooling. The idea here is to iterate over the image, and look at the pixel and it's immediate neighbors to the right, beneath, and right-beneath. Take the largest (hence the name MAX pooling) of them and load it into the new image. Thus the new image will be 1/4 the size of the old -- with the dimensions on X and Y being halved by this process. You'll see that the features get maintained despite this compression! This code will show (4, 4) pooling. Run it to see the output, and you'll see that while the image is 1/4 the size of the original in both length and width, the extracted features are maintained! new_x = int ( size_x / 4 ) new_y = int ( size_y / 4 ) newImage = np . zeros (( new_x , new_y )) for x in range ( 0 , size_x , 4 ): for y in range ( 0 , size_y , 4 ): pixels = [] pixels . append ( i_transformed [ x , y ]) pixels . append ( i_transformed [ x + 1 , y ]) pixels . append ( i_transformed [ x + 2 , y ]) pixels . append ( i_transformed [ x + 3 , y ]) pixels . append ( i_transformed [ x , y + 1 ]) pixels . append ( i_transformed [ x + 1 , y + 1 ]) pixels . append ( i_transformed [ x + 2 , y + 1 ]) pixels . append ( i_transformed [ x + 3 , y + 1 ]) pixels . append ( i_transformed [ x , y + 2 ]) pixels . append ( i_transformed [ x + 1 , y + 2 ]) pixels . append ( i_transformed [ x + 2 , y + 2 ]) pixels . append ( i_transformed [ x + 3 , y + 2 ]) pixels . append ( i_transformed [ x , y + 3 ]) pixels . append ( i_transformed [ x + 1 , y + 3 ]) pixels . append ( i_transformed [ x + 2 , y + 3 ]) pixels . append ( i_transformed [ x + 3 , y + 3 ]) pixels . sort ( reverse = True ) newImage [ int ( x / 4 ), int ( y / 4 )] = pixels [ 0 ] # Plot the image. Note the size of the axes -- now 128 pixels instead of 512 plt . gray () plt . grid ( False ) plt . imshow ( newImage ) #plt.axis('off') plt . show () In the next lab you'll see how to add convolutions to your Fashion MNIST neural network to make it more efficient -- because it will classify based on features, and not on raw pixels.","title":"Exploring Convolutions"},{"location":"Exploring_Convolutions/#what-are-convolutions","text":"What are convolutions? In this lab you'll explore what they are and how they work. In later lessons, you'll see how to use them in your neural network. Together with convolutions, you'll use something called 'Pooling', which compresses your image, further emphasising the features. You'll also see how pooling works in this lab.","title":"What are Convolutions?"},{"location":"Exploring_Convolutions/#limitations-of-the-previous-dnn","text":"In an earlier exercise you saw how to train an image classifier for fashion items using the Fashion MNIST dataset. This gave you a pretty accuract classifier, but there was an obvious constraint: the images were 28x28, grey scale and the item was centered in the image. For example here are a couple of the images in Fashion MNIST The DNN that you created simply learned from the raw pixels what made up a sweater, and what made up a boot in this context. But consider how it might classify this image? (Image is Public domain CC0 from Pixabay: https://pixabay.com/photos/boots-travel-railroad-tracks-181744/) While it's clear that there are boots in this image, the classifier would fail for a number of reasons. First, of course, it's not 28x28 greyscale, but more importantly, the classifier was trained on the raw pixels of a left-facing boot, and not the features that make up what a boot is. That's where Convolutions are very powerful. A convolution is a filter that passes over an image, processing it, and extracting features that show a commonolatity in the image. In this lab you'll see how they work, but processing an image to see if you can extract features from it! Generating convolutions is very simple -- you simply scan every pixel in the image and then look at it's neighboring pixels. You multiply out the values of these pixels by the equivalent weights in a filter. So, for example, consider this: In this case a 3x3 Convolution is specified. The current pixel value is 192, but you can calculate the new one by looking at the neighbor values, and multiplying them out by the values specified in the filter, and making the new pixel value the final amount. Let's explore how convolutions work by creating a basic convolution on a 2D Grey Scale image. First we can load the image by taking the 'ascent' image from scipy. It's a nice, built-in picture with lots of angles and lines. Let's start by importing some python libraries. import cv2 import numpy as np from scipy import misc i = misc . ascent () Next, we can use the pyplot library to draw the image so we know what it looks like. import matplotlib.pyplot as plt plt . grid ( False ) plt . gray () plt . axis ( 'off' ) plt . imshow ( i ) plt . show () We can see that this is an image of a stairwell. There are lots of features in here that we can play with seeing if we can isolate them -- for example there are strong vertical lines. The image is stored as a numpy array, so we can create the transformed image by just copying that array. Let's also get the dimensions of the image so we can loop over it later. i_transformed = np . copy ( i ) size_x = i_transformed . shape [ 0 ] size_y = i_transformed . shape [ 1 ] Now we can create a filter as a 3x3 array. # This filter detects edges nicely # It creates a convolution that only passes through sharp edges and straight # lines. #Experiment with different values for fun effects. #filter = [ [0, 1, 0], [1, -4, 1], [0, 1, 0]] # A couple more filters to try for fun! filter = [ [ - 1 , - 2 , - 1 ], [ 0 , 0 , 0 ], [ 1 , 2 , 1 ]] #filter = [ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]] # If all the digits in the filter don't add up to 0 or 1, you # should probably do a weight to get it to do so # so, for example, if your weights are 1,1,1 1,2,1 1,1,1 # They add up to 10, so you would set a weight of .1 if you want to normalize them weight = 1 Now let's create a convolution. We will iterate over the image, leaving a 1 pixel margin, and multiply out each of the neighbors of the current pixel by the value defined in the filter. i.e. the current pixel's neighbor above it and to the left will be multiplied by the top left item in the filter etc. etc. We'll then multiply the result by the weight, and then ensure the result is in the range 0-255 Finally we'll load the new value into the transformed image. for x in range ( 1 , size_x - 1 ): for y in range ( 1 , size_y - 1 ): convolution = 0.0 convolution = convolution + ( i [ x - 1 , y - 1 ] * filter [ 0 ][ 0 ]) convolution = convolution + ( i [ x , y - 1 ] * filter [ 1 ][ 0 ]) convolution = convolution + ( i [ x + 1 , y - 1 ] * filter [ 2 ][ 0 ]) convolution = convolution + ( i [ x - 1 , y ] * filter [ 0 ][ 1 ]) convolution = convolution + ( i [ x , y ] * filter [ 1 ][ 1 ]) convolution = convolution + ( i [ x + 1 , y ] * filter [ 2 ][ 1 ]) convolution = convolution + ( i [ x - 1 , y + 1 ] * filter [ 0 ][ 2 ]) convolution = convolution + ( i [ x , y + 1 ] * filter [ 1 ][ 2 ]) convolution = convolution + ( i [ x + 1 , y + 1 ] * filter [ 2 ][ 2 ]) convolution = convolution * weight if ( convolution < 0 ): convolution = 0 if ( convolution > 255 ): convolution = 255 i_transformed [ x , y ] = convolution Now we can plot the image to see the effect of the convolution! # Plot the image. Note the size of the axes -- they are 512 by 512 plt . gray () plt . grid ( False ) plt . imshow ( i_transformed ) #plt.axis('off') plt . show () So, consider the following filter values, and their impact on the image. Using -1,0,1,-2,0,2,-1,0,1 gives us a very strong set of vertical lines: Using -1, -2, -1, 0, 0, 0, 1, 2, 1 gives us horizontal lines: Explore different values for yourself!","title":"Limitations of the previous DNN"},{"location":"Exploring_Convolutions/#pooling","text":"As well as using convolutions, pooling helps us greatly in detecting features. The goal is to reduce the overall amount of information in an image, while maintaining the features that are detected as present. There are a number of different types of pooling, but for this lab we'll use one called MAX pooling. The idea here is to iterate over the image, and look at the pixel and it's immediate neighbors to the right, beneath, and right-beneath. Take the largest (hence the name MAX pooling) of them and load it into the new image. Thus the new image will be 1/4 the size of the old -- with the dimensions on X and Y being halved by this process. You'll see that the features get maintained despite this compression! This code will show (4, 4) pooling. Run it to see the output, and you'll see that while the image is 1/4 the size of the original in both length and width, the extracted features are maintained! new_x = int ( size_x / 4 ) new_y = int ( size_y / 4 ) newImage = np . zeros (( new_x , new_y )) for x in range ( 0 , size_x , 4 ): for y in range ( 0 , size_y , 4 ): pixels = [] pixels . append ( i_transformed [ x , y ]) pixels . append ( i_transformed [ x + 1 , y ]) pixels . append ( i_transformed [ x + 2 , y ]) pixels . append ( i_transformed [ x + 3 , y ]) pixels . append ( i_transformed [ x , y + 1 ]) pixels . append ( i_transformed [ x + 1 , y + 1 ]) pixels . append ( i_transformed [ x + 2 , y + 1 ]) pixels . append ( i_transformed [ x + 3 , y + 1 ]) pixels . append ( i_transformed [ x , y + 2 ]) pixels . append ( i_transformed [ x + 1 , y + 2 ]) pixels . append ( i_transformed [ x + 2 , y + 2 ]) pixels . append ( i_transformed [ x + 3 , y + 2 ]) pixels . append ( i_transformed [ x , y + 3 ]) pixels . append ( i_transformed [ x + 1 , y + 3 ]) pixels . append ( i_transformed [ x + 2 , y + 3 ]) pixels . append ( i_transformed [ x + 3 , y + 3 ]) pixels . sort ( reverse = True ) newImage [ int ( x / 4 ), int ( y / 4 )] = pixels [ 0 ] # Plot the image. Note the size of the axes -- now 128 pixels instead of 512 plt . gray () plt . grid ( False ) plt . imshow ( newImage ) #plt.axis('off') plt . show () In the next lab you'll see how to add convolutions to your Fashion MNIST neural network to make it more efficient -- because it will classify based on features, and not on raw pixels.","title":"Pooling"},{"location":"Fashion_MNIST_Convolutions%281%29/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); How does the model see? Lets retrain our convolutional model for the Fashion-MNIST dataset and then visualize the filters and pooling. import tensorflow as tf mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( val_images , val_labels ) = mnist . load_data () training_images = training_images . reshape ( 60000 , 28 , 28 , 1 ) training_images = training_images / 255.0 val_images = val_images . reshape ( 10000 , 28 , 28 , 1 ) val_images = val_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 20 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . summary () model . fit ( training_images , training_labels , validation_data = ( val_images , val_labels ), epochs = 20 ) Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_2 (Conv2D) (None, 26, 26, 64) 640 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 11, 11, 64) 36928 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 1600) 0 _________________________________________________________________ dense_2 (Dense) (None, 20) 32020 _________________________________________________________________ dense_3 (Dense) (None, 10) 210 ================================================================= Total params: 69,798 Trainable params: 69,798 Non-trainable params: 0 _________________________________________________________________ Epoch 1/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.5161 - accuracy: 0.8153 - val_loss: 0.3862 - val_accuracy: 0.8601 Epoch 2/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.3406 - accuracy: 0.8774 - val_loss: 0.3269 - val_accuracy: 0.8822 Epoch 3/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2945 - accuracy: 0.8950 - val_loss: 0.3115 - val_accuracy: 0.8885 Epoch 4/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2653 - accuracy: 0.9033 - val_loss: 0.3027 - val_accuracy: 0.8916 Epoch 5/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2414 - accuracy: 0.9111 - val_loss: 0.2707 - val_accuracy: 0.9010 Epoch 6/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2232 - accuracy: 0.9178 - val_loss: 0.2778 - val_accuracy: 0.8994 Epoch 7/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2068 - accuracy: 0.9240 - val_loss: 0.2646 - val_accuracy: 0.9027 Epoch 8/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1931 - accuracy: 0.9282 - val_loss: 0.2711 - val_accuracy: 0.9016 Epoch 9/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1837 - accuracy: 0.9315 - val_loss: 0.2655 - val_accuracy: 0.9066 Epoch 10/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1673 - accuracy: 0.9381 - val_loss: 0.2618 - val_accuracy: 0.9089 Epoch 11/20 1875/1875 [==============================] - 7s 3ms/step - loss: 0.1575 - accuracy: 0.9424 - val_loss: 0.2808 - val_accuracy: 0.9017 Epoch 12/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1491 - accuracy: 0.9439 - val_loss: 0.2711 - val_accuracy: 0.9112 Epoch 13/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1385 - accuracy: 0.9484 - val_loss: 0.2902 - val_accuracy: 0.9060 Epoch 14/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1320 - accuracy: 0.9517 - val_loss: 0.2748 - val_accuracy: 0.9069 Epoch 15/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1245 - accuracy: 0.9531 - val_loss: 0.2988 - val_accuracy: 0.9075 Epoch 16/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1172 - accuracy: 0.9565 - val_loss: 0.3046 - val_accuracy: 0.9069 Epoch 17/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1108 - accuracy: 0.9583 - val_loss: 0.3028 - val_accuracy: 0.9113 Epoch 18/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1043 - accuracy: 0.9612 - val_loss: 0.3380 - val_accuracy: 0.9104 Epoch 19/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1026 - accuracy: 0.9617 - val_loss: 0.3316 - val_accuracy: 0.9072 Epoch 20/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.0933 - accuracy: 0.9657 - val_loss: 0.3219 - val_accuracy: 0.9109 <tensorflow.python.keras.callbacks.History at 0x7f7a10fc8748> Visualizing the Convolutions and Pooling This code will show us the convolutions graphically. The print (test_labels[:100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the final dense layers are trained on this resulting data, it's working with a lot less, more targeted, data -- the features generated by this convolution/pooling combination. print ( val_labels [: 100 ]) [9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2] import matplotlib.pyplot as plt def show_image ( img ): plt . figure () plt . imshow ( val_images [ img ] . reshape ( 28 , 28 )) plt . grid ( False ) plt . show () f , axarr = plt . subplots ( 3 , 2 ) # By scanning the list above I saw that the 0, 23 and 28 entries are all label 9 FIRST_IMAGE = 0 SECOND_IMAGE = 23 THIRD_IMAGE = 28 # For shoes (0, 23, 28), Convolution_Number=1 (i.e. the second filter) shows # the sole being filtered out very clearly CONVOLUTION_NUMBER = 1 from tensorflow.keras import models layer_outputs = [ layer . output for layer in model . layers ] activation_model = tf . keras . models . Model ( inputs = model . input , outputs = layer_outputs ) for x in range ( 0 , 2 ): f1 = activation_model . predict ( val_images [ FIRST_IMAGE ] . reshape ( 1 , 28 , 28 , 1 ))[ x ] axarr [ 0 , x ] . imshow ( f1 [ 0 , : , :, CONVOLUTION_NUMBER ], cmap = 'inferno' ) axarr [ 0 , x ] . grid ( False ) f2 = activation_model . predict ( val_images [ SECOND_IMAGE ] . reshape ( 1 , 28 , 28 , 1 ))[ x ] axarr [ 1 , x ] . imshow ( f2 [ 0 , : , :, CONVOLUTION_NUMBER ], cmap = 'inferno' ) axarr [ 1 , x ] . grid ( False ) f3 = activation_model . predict ( val_images [ THIRD_IMAGE ] . reshape ( 1 , 28 , 28 , 1 ))[ x ] axarr [ 2 , x ] . imshow ( f3 [ 0 , : , :, CONVOLUTION_NUMBER ], cmap = 'inferno' ) axarr [ 2 , x ] . grid ( False ) show_image ( FIRST_IMAGE ) show_image ( SECOND_IMAGE ) show_image ( THIRD_IMAGE )","title":"Fashion MNIST Convolutions(1)"},{"location":"Fashion_MNIST_Convolutions%281%29/#how-does-the-model-see","text":"Lets retrain our convolutional model for the Fashion-MNIST dataset and then visualize the filters and pooling. import tensorflow as tf mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( val_images , val_labels ) = mnist . load_data () training_images = training_images . reshape ( 60000 , 28 , 28 , 1 ) training_images = training_images / 255.0 val_images = val_images . reshape ( 10000 , 28 , 28 , 1 ) val_images = val_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 20 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . summary () model . fit ( training_images , training_labels , validation_data = ( val_images , val_labels ), epochs = 20 ) Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_2 (Conv2D) (None, 26, 26, 64) 640 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 11, 11, 64) 36928 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 1600) 0 _________________________________________________________________ dense_2 (Dense) (None, 20) 32020 _________________________________________________________________ dense_3 (Dense) (None, 10) 210 ================================================================= Total params: 69,798 Trainable params: 69,798 Non-trainable params: 0 _________________________________________________________________ Epoch 1/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.5161 - accuracy: 0.8153 - val_loss: 0.3862 - val_accuracy: 0.8601 Epoch 2/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.3406 - accuracy: 0.8774 - val_loss: 0.3269 - val_accuracy: 0.8822 Epoch 3/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2945 - accuracy: 0.8950 - val_loss: 0.3115 - val_accuracy: 0.8885 Epoch 4/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2653 - accuracy: 0.9033 - val_loss: 0.3027 - val_accuracy: 0.8916 Epoch 5/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2414 - accuracy: 0.9111 - val_loss: 0.2707 - val_accuracy: 0.9010 Epoch 6/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2232 - accuracy: 0.9178 - val_loss: 0.2778 - val_accuracy: 0.8994 Epoch 7/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2068 - accuracy: 0.9240 - val_loss: 0.2646 - val_accuracy: 0.9027 Epoch 8/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1931 - accuracy: 0.9282 - val_loss: 0.2711 - val_accuracy: 0.9016 Epoch 9/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1837 - accuracy: 0.9315 - val_loss: 0.2655 - val_accuracy: 0.9066 Epoch 10/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1673 - accuracy: 0.9381 - val_loss: 0.2618 - val_accuracy: 0.9089 Epoch 11/20 1875/1875 [==============================] - 7s 3ms/step - loss: 0.1575 - accuracy: 0.9424 - val_loss: 0.2808 - val_accuracy: 0.9017 Epoch 12/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1491 - accuracy: 0.9439 - val_loss: 0.2711 - val_accuracy: 0.9112 Epoch 13/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1385 - accuracy: 0.9484 - val_loss: 0.2902 - val_accuracy: 0.9060 Epoch 14/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1320 - accuracy: 0.9517 - val_loss: 0.2748 - val_accuracy: 0.9069 Epoch 15/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1245 - accuracy: 0.9531 - val_loss: 0.2988 - val_accuracy: 0.9075 Epoch 16/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1172 - accuracy: 0.9565 - val_loss: 0.3046 - val_accuracy: 0.9069 Epoch 17/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1108 - accuracy: 0.9583 - val_loss: 0.3028 - val_accuracy: 0.9113 Epoch 18/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1043 - accuracy: 0.9612 - val_loss: 0.3380 - val_accuracy: 0.9104 Epoch 19/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1026 - accuracy: 0.9617 - val_loss: 0.3316 - val_accuracy: 0.9072 Epoch 20/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.0933 - accuracy: 0.9657 - val_loss: 0.3219 - val_accuracy: 0.9109 <tensorflow.python.keras.callbacks.History at 0x7f7a10fc8748>","title":"How does the model see?"},{"location":"Fashion_MNIST_Convolutions%281%29/#visualizing-the-convolutions-and-pooling","text":"This code will show us the convolutions graphically. The print (test_labels[:100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the final dense layers are trained on this resulting data, it's working with a lot less, more targeted, data -- the features generated by this convolution/pooling combination. print ( val_labels [: 100 ]) [9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2] import matplotlib.pyplot as plt def show_image ( img ): plt . figure () plt . imshow ( val_images [ img ] . reshape ( 28 , 28 )) plt . grid ( False ) plt . show () f , axarr = plt . subplots ( 3 , 2 ) # By scanning the list above I saw that the 0, 23 and 28 entries are all label 9 FIRST_IMAGE = 0 SECOND_IMAGE = 23 THIRD_IMAGE = 28 # For shoes (0, 23, 28), Convolution_Number=1 (i.e. the second filter) shows # the sole being filtered out very clearly CONVOLUTION_NUMBER = 1 from tensorflow.keras import models layer_outputs = [ layer . output for layer in model . layers ] activation_model = tf . keras . models . Model ( inputs = model . input , outputs = layer_outputs ) for x in range ( 0 , 2 ): f1 = activation_model . predict ( val_images [ FIRST_IMAGE ] . reshape ( 1 , 28 , 28 , 1 ))[ x ] axarr [ 0 , x ] . imshow ( f1 [ 0 , : , :, CONVOLUTION_NUMBER ], cmap = 'inferno' ) axarr [ 0 , x ] . grid ( False ) f2 = activation_model . predict ( val_images [ SECOND_IMAGE ] . reshape ( 1 , 28 , 28 , 1 ))[ x ] axarr [ 1 , x ] . imshow ( f2 [ 0 , : , :, CONVOLUTION_NUMBER ], cmap = 'inferno' ) axarr [ 1 , x ] . grid ( False ) f3 = activation_model . predict ( val_images [ THIRD_IMAGE ] . reshape ( 1 , 28 , 28 , 1 ))[ x ] axarr [ 2 , x ] . imshow ( f3 [ 0 , : , :, CONVOLUTION_NUMBER ], cmap = 'inferno' ) axarr [ 2 , x ] . grid ( False ) show_image ( FIRST_IMAGE ) show_image ( SECOND_IMAGE ) show_image ( THIRD_IMAGE )","title":"Visualizing the Convolutions and Pooling"},{"location":"Fashion_MNIST_Convolutions/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Improving Computer Vision Accuracy using Convolutions In previous lessons you saw how to do image recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. For convenience, here's the entire code again. Run it and take a note of the accuracy that is printed out at the end. import tensorflow as tf mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( val_images , val_labels ) = mnist . load_data () training_images = training_images / 255.0 val_images = val_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 20 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , validation_data = ( val_images , val_labels ), epochs = 20 ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step Epoch 1/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5839 - accuracy: 0.7997 - val_loss: 0.4742 - val_accuracy: 0.8348 Epoch 2/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4258 - accuracy: 0.8510 - val_loss: 0.4388 - val_accuracy: 0.8475 Epoch 3/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3965 - accuracy: 0.8596 - val_loss: 0.4402 - val_accuracy: 0.8460 Epoch 4/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3787 - accuracy: 0.8664 - val_loss: 0.4270 - val_accuracy: 0.8511 Epoch 5/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3643 - accuracy: 0.8706 - val_loss: 0.4093 - val_accuracy: 0.8571 Epoch 6/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3565 - accuracy: 0.8731 - val_loss: 0.3998 - val_accuracy: 0.8585 Epoch 7/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3473 - accuracy: 0.8757 - val_loss: 0.3955 - val_accuracy: 0.8599 Epoch 8/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3430 - accuracy: 0.8773 - val_loss: 0.4003 - val_accuracy: 0.8561 Epoch 9/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3360 - accuracy: 0.8801 - val_loss: 0.3938 - val_accuracy: 0.8637 Epoch 10/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3313 - accuracy: 0.8810 - val_loss: 0.4017 - val_accuracy: 0.8553 Epoch 11/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3267 - accuracy: 0.8820 - val_loss: 0.3882 - val_accuracy: 0.8644 Epoch 12/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3230 - accuracy: 0.8844 - val_loss: 0.3831 - val_accuracy: 0.8668 Epoch 13/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3186 - accuracy: 0.8856 - val_loss: 0.4011 - val_accuracy: 0.8590 Epoch 14/20 1862/1875 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.8861 Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to get into the details of Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, features. If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar. In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced. This is perfect for computer vision, because often it's features like edges that distinguish one item for another. And once we move from raw image data to feature data, the amount of information needed is then much less...because you'll just train on the highlighted features. That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate. Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy: import tensorflow as tf mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( val_images , val_labels ) = mnist . load_data () training_images = training_images . reshape ( 60000 , 28 , 28 , 1 ) training_images = training_images / 255.0 val_images = val_images . reshape ( 10000 , 28 , 28 , 1 ) val_images = val_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 20 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . summary () model . fit ( training_images , training_labels , validation_data = ( val_images , val_labels ), epochs = 20 ) Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_2 (Conv2D) (None, 26, 26, 64) 640 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 11, 11, 64) 36928 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 1600) 0 _________________________________________________________________ dense_2 (Dense) (None, 20) 32020 _________________________________________________________________ dense_3 (Dense) (None, 10) 210 ================================================================= Total params: 69,798 Trainable params: 69,798 Non-trainable params: 0 _________________________________________________________________ Epoch 1/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.5161 - accuracy: 0.8153 - val_loss: 0.3862 - val_accuracy: 0.8601 Epoch 2/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.3406 - accuracy: 0.8774 - val_loss: 0.3269 - val_accuracy: 0.8822 Epoch 3/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2945 - accuracy: 0.8950 - val_loss: 0.3115 - val_accuracy: 0.8885 Epoch 4/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2653 - accuracy: 0.9033 - val_loss: 0.3027 - val_accuracy: 0.8916 Epoch 5/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2414 - accuracy: 0.9111 - val_loss: 0.2707 - val_accuracy: 0.9010 Epoch 6/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2232 - accuracy: 0.9178 - val_loss: 0.2778 - val_accuracy: 0.8994 Epoch 7/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2068 - accuracy: 0.9240 - val_loss: 0.2646 - val_accuracy: 0.9027 Epoch 8/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1931 - accuracy: 0.9282 - val_loss: 0.2711 - val_accuracy: 0.9016 Epoch 9/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1837 - accuracy: 0.9315 - val_loss: 0.2655 - val_accuracy: 0.9066 Epoch 10/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1673 - accuracy: 0.9381 - val_loss: 0.2618 - val_accuracy: 0.9089 Epoch 11/20 1875/1875 [==============================] - 7s 3ms/step - loss: 0.1575 - accuracy: 0.9424 - val_loss: 0.2808 - val_accuracy: 0.9017 Epoch 12/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1491 - accuracy: 0.9439 - val_loss: 0.2711 - val_accuracy: 0.9112 Epoch 13/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1385 - accuracy: 0.9484 - val_loss: 0.2902 - val_accuracy: 0.9060 Epoch 14/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1320 - accuracy: 0.9517 - val_loss: 0.2748 - val_accuracy: 0.9069 Epoch 15/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1245 - accuracy: 0.9531 - val_loss: 0.2988 - val_accuracy: 0.9075 Epoch 16/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1172 - accuracy: 0.9565 - val_loss: 0.3046 - val_accuracy: 0.9069 Epoch 17/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1108 - accuracy: 0.9583 - val_loss: 0.3028 - val_accuracy: 0.9113 Epoch 18/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1043 - accuracy: 0.9612 - val_loss: 0.3380 - val_accuracy: 0.9104 Epoch 19/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1026 - accuracy: 0.9617 - val_loss: 0.3316 - val_accuracy: 0.9072 Epoch 20/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.0933 - accuracy: 0.9657 - val_loss: 0.3219 - val_accuracy: 0.9109 <tensorflow.python.keras.callbacks.History at 0x7f7a10fc8748> It's likely gone up to about 97% on the training data and 91% on the validation data. That's significant, and a step in the right direction! Try running it for more epochs -- say about 100, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. (In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing other data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.) Then, look at the code again, and see, step by step how the Convolutions were built: Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the validation images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. import tensorflow as tf mnist = tf.keras.datasets.fashion_mnist (training_images, training_labels), (val_images, val_labels) = mnist.load_data() training_images=training_images.reshape(60000, 28, 28, 1) training_images=training_images / 255.0 val_images = val_images.reshape(10000, 28, 28, 1) val_images=val_images/255.0 Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are: The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 64 The size of the Convolution, in this case a 3x3 grid The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0 In the first layer, the shape of the input data. You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convolution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image to 25% of its original size. You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(2, 2), Add another convolution tf.keras.layers.Conv2D(64, (3,3), activation='relu'), tf.keras.layers.MaxPooling2D(2,2) Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version tf.keras.layers.Flatten(), The same 20 dense layers, and 10 output layers as in the pre-convolution example: tf.keras.layers.Dense(20, activation='relu'), tf.keras.layers.Dense(10, activation='softmax') ]) Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the validation set. model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(training_images, training_labels, validation_data=(val_images, val_labels), epochs=20) Optional Additonal Exercises Try editing the convolutions. Change the 32s to either 16 or 64. Explore what impact this will have on accuracy and/or training time. Remove the final Convolution. What impact will this have on accuracy or training time? How about adding more Convolutions? What impact do you think this will have? Experiment with it. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it.","title":"Fashion MNIST Convolutions"},{"location":"Fashion_MNIST_Convolutions/#improving-computer-vision-accuracy-using-convolutions","text":"In previous lessons you saw how to do image recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. For convenience, here's the entire code again. Run it and take a note of the accuracy that is printed out at the end. import tensorflow as tf mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( val_images , val_labels ) = mnist . load_data () training_images = training_images / 255.0 val_images = val_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 20 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , validation_data = ( val_images , val_labels ), epochs = 20 ) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step Epoch 1/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5839 - accuracy: 0.7997 - val_loss: 0.4742 - val_accuracy: 0.8348 Epoch 2/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4258 - accuracy: 0.8510 - val_loss: 0.4388 - val_accuracy: 0.8475 Epoch 3/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3965 - accuracy: 0.8596 - val_loss: 0.4402 - val_accuracy: 0.8460 Epoch 4/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3787 - accuracy: 0.8664 - val_loss: 0.4270 - val_accuracy: 0.8511 Epoch 5/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3643 - accuracy: 0.8706 - val_loss: 0.4093 - val_accuracy: 0.8571 Epoch 6/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3565 - accuracy: 0.8731 - val_loss: 0.3998 - val_accuracy: 0.8585 Epoch 7/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3473 - accuracy: 0.8757 - val_loss: 0.3955 - val_accuracy: 0.8599 Epoch 8/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3430 - accuracy: 0.8773 - val_loss: 0.4003 - val_accuracy: 0.8561 Epoch 9/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3360 - accuracy: 0.8801 - val_loss: 0.3938 - val_accuracy: 0.8637 Epoch 10/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3313 - accuracy: 0.8810 - val_loss: 0.4017 - val_accuracy: 0.8553 Epoch 11/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3267 - accuracy: 0.8820 - val_loss: 0.3882 - val_accuracy: 0.8644 Epoch 12/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3230 - accuracy: 0.8844 - val_loss: 0.3831 - val_accuracy: 0.8668 Epoch 13/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.3186 - accuracy: 0.8856 - val_loss: 0.4011 - val_accuracy: 0.8590 Epoch 14/20 1862/1875 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.8861 Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to get into the details of Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, features. If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar. In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced. This is perfect for computer vision, because often it's features like edges that distinguish one item for another. And once we move from raw image data to feature data, the amount of information needed is then much less...because you'll just train on the highlighted features. That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate. Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy: import tensorflow as tf mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( val_images , val_labels ) = mnist . load_data () training_images = training_images . reshape ( 60000 , 28 , 28 , 1 ) training_images = training_images / 255.0 val_images = val_images . reshape ( 10000 , 28 , 28 , 1 ) val_images = val_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 20 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . summary () model . fit ( training_images , training_labels , validation_data = ( val_images , val_labels ), epochs = 20 ) Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_2 (Conv2D) (None, 26, 26, 64) 640 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 11, 11, 64) 36928 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 1600) 0 _________________________________________________________________ dense_2 (Dense) (None, 20) 32020 _________________________________________________________________ dense_3 (Dense) (None, 10) 210 ================================================================= Total params: 69,798 Trainable params: 69,798 Non-trainable params: 0 _________________________________________________________________ Epoch 1/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.5161 - accuracy: 0.8153 - val_loss: 0.3862 - val_accuracy: 0.8601 Epoch 2/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.3406 - accuracy: 0.8774 - val_loss: 0.3269 - val_accuracy: 0.8822 Epoch 3/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2945 - accuracy: 0.8950 - val_loss: 0.3115 - val_accuracy: 0.8885 Epoch 4/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2653 - accuracy: 0.9033 - val_loss: 0.3027 - val_accuracy: 0.8916 Epoch 5/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2414 - accuracy: 0.9111 - val_loss: 0.2707 - val_accuracy: 0.9010 Epoch 6/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2232 - accuracy: 0.9178 - val_loss: 0.2778 - val_accuracy: 0.8994 Epoch 7/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.2068 - accuracy: 0.9240 - val_loss: 0.2646 - val_accuracy: 0.9027 Epoch 8/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1931 - accuracy: 0.9282 - val_loss: 0.2711 - val_accuracy: 0.9016 Epoch 9/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1837 - accuracy: 0.9315 - val_loss: 0.2655 - val_accuracy: 0.9066 Epoch 10/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1673 - accuracy: 0.9381 - val_loss: 0.2618 - val_accuracy: 0.9089 Epoch 11/20 1875/1875 [==============================] - 7s 3ms/step - loss: 0.1575 - accuracy: 0.9424 - val_loss: 0.2808 - val_accuracy: 0.9017 Epoch 12/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1491 - accuracy: 0.9439 - val_loss: 0.2711 - val_accuracy: 0.9112 Epoch 13/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1385 - accuracy: 0.9484 - val_loss: 0.2902 - val_accuracy: 0.9060 Epoch 14/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1320 - accuracy: 0.9517 - val_loss: 0.2748 - val_accuracy: 0.9069 Epoch 15/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1245 - accuracy: 0.9531 - val_loss: 0.2988 - val_accuracy: 0.9075 Epoch 16/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1172 - accuracy: 0.9565 - val_loss: 0.3046 - val_accuracy: 0.9069 Epoch 17/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1108 - accuracy: 0.9583 - val_loss: 0.3028 - val_accuracy: 0.9113 Epoch 18/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1043 - accuracy: 0.9612 - val_loss: 0.3380 - val_accuracy: 0.9104 Epoch 19/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.1026 - accuracy: 0.9617 - val_loss: 0.3316 - val_accuracy: 0.9072 Epoch 20/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.0933 - accuracy: 0.9657 - val_loss: 0.3219 - val_accuracy: 0.9109 <tensorflow.python.keras.callbacks.History at 0x7f7a10fc8748> It's likely gone up to about 97% on the training data and 91% on the validation data. That's significant, and a step in the right direction! Try running it for more epochs -- say about 100, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. (In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing other data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.) Then, look at the code again, and see, step by step how the Convolutions were built: Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the validation images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. import tensorflow as tf mnist = tf.keras.datasets.fashion_mnist (training_images, training_labels), (val_images, val_labels) = mnist.load_data() training_images=training_images.reshape(60000, 28, 28, 1) training_images=training_images / 255.0 val_images = val_images.reshape(10000, 28, 28, 1) val_images=val_images/255.0 Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are: The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 64 The size of the Convolution, in this case a 3x3 grid The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0 In the first layer, the shape of the input data. You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convolution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image to 25% of its original size. You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(2, 2), Add another convolution tf.keras.layers.Conv2D(64, (3,3), activation='relu'), tf.keras.layers.MaxPooling2D(2,2) Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version tf.keras.layers.Flatten(), The same 20 dense layers, and 10 output layers as in the pre-convolution example: tf.keras.layers.Dense(20, activation='relu'), tf.keras.layers.Dense(10, activation='softmax') ]) Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the validation set. model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(training_images, training_labels, validation_data=(val_images, val_labels), epochs=20)","title":"Improving Computer Vision Accuracy using Convolutions"},{"location":"Fashion_MNIST_Convolutions/#optional-additonal-exercises","text":"Try editing the convolutions. Change the 32s to either 16 or 64. Explore what impact this will have on accuracy and/or training time. Remove the final Convolution. What impact will this have on accuracy or training time? How about adding more Convolutions? What impact do you think this will have? Experiment with it. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it.","title":"Optional Additonal Exercises"},{"location":"FirstNeuralNetwork/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import tensorflow as tf import numpy as np from tensorflow import keras # define a neural network with one neuron # for more information on TF functions see: https://www.tensorflow.org/api_docs model = tf . keras . Sequential ([ keras . layers . Dense ( units = 1 , input_shape = [ 1 ])]) # use stochastic gradient descent for optimization and # the mean squared error loss function model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) # define some training data (xs as inputs and ys as outputs) xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) ys = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ], dtype = float ) # fit the model to the data (aka train the model) model . fit ( xs , ys , epochs = 500 ) print ( model . predict ([ 10.0 ]))","title":"FirstNeuralNetwork"},{"location":"FirstNeuralNetwork_revisited/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import tensorflow as tf import numpy as np from tensorflow import keras First lets re-train our original single layer network and see what the prediction is for X = 10.0 and what the learned weights are. my_layer = keras . layers . Dense ( units = 1 , input_shape = [ 1 ]) model = tf . keras . Sequential ([ my_layer ]) model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) ys = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ], dtype = float ) model . fit ( xs , ys , epochs = 500 ) print ( model . predict ([ 10.0 ])) print ( my_layer . get_weights ()) Next lets train a 2-layer network and see what its prediction and weights are. my_layer_1 = keras . layers . Dense ( units = 2 , input_shape = [ 1 ]) my_layer_2 = keras . layers . Dense ( units = 1 ) model = tf . keras . Sequential ([ my_layer_1 , my_layer_2 ]) model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) ys = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ], dtype = float ) model . fit ( xs , ys , epochs = 500 ) print ( model . predict ([ 10.0 ])) print ( my_layer_1 . get_weights ()) print ( my_layer_2 . get_weights ()) Finally we can manually compute the output for our 2-layer network to better understand how it works. value_to_predict = 10.0 layer1_w1 = ( my_layer_1 . get_weights ()[ 0 ][ 0 ][ 0 ]) layer1_w2 = ( my_layer_1 . get_weights ()[ 0 ][ 0 ][ 1 ]) layer1_b1 = ( my_layer_1 . get_weights ()[ 1 ][ 0 ]) layer1_b2 = ( my_layer_1 . get_weights ()[ 1 ][ 1 ]) layer2_w1 = ( my_layer_2 . get_weights ()[ 0 ][ 0 ]) layer2_w2 = ( my_layer_2 . get_weights ()[ 0 ][ 1 ]) layer2_b = ( my_layer_2 . get_weights ()[ 1 ][ 0 ]) neuron1_output = ( layer1_w1 * value_to_predict ) + layer1_b1 neuron2_output = ( layer1_w2 * value_to_predict ) + layer1_b2 neuron3_output = ( layer2_w1 * neuron1_output ) + ( layer2_w2 * neuron2_output ) + layer2_b print ( neuron3_output )","title":"FirstNeuralNetwork revisited"},{"location":"FirstNeuralNetwork_revisited/#first-lets-re-train-our-original-single-layer-network-and-see-what-the-prediction-is-for-x-100-and-what-the-learned-weights-are","text":"my_layer = keras . layers . Dense ( units = 1 , input_shape = [ 1 ]) model = tf . keras . Sequential ([ my_layer ]) model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) ys = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ], dtype = float ) model . fit ( xs , ys , epochs = 500 ) print ( model . predict ([ 10.0 ])) print ( my_layer . get_weights ())","title":"First lets re-train our original single layer network and see what the prediction is for X = 10.0 and what the learned weights are."},{"location":"FirstNeuralNetwork_revisited/#next-lets-train-a-2-layer-network-and-see-what-its-prediction-and-weights-are","text":"my_layer_1 = keras . layers . Dense ( units = 2 , input_shape = [ 1 ]) my_layer_2 = keras . layers . Dense ( units = 1 ) model = tf . keras . Sequential ([ my_layer_1 , my_layer_2 ]) model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) ys = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ], dtype = float ) model . fit ( xs , ys , epochs = 500 ) print ( model . predict ([ 10.0 ])) print ( my_layer_1 . get_weights ()) print ( my_layer_2 . get_weights ())","title":"Next lets train a 2-layer network and see what its prediction and weights are."},{"location":"FirstNeuralNetwork_revisited/#finally-we-can-manually-compute-the-output-for-our-2-layer-network-to-better-understand-how-it-works","text":"value_to_predict = 10.0 layer1_w1 = ( my_layer_1 . get_weights ()[ 0 ][ 0 ][ 0 ]) layer1_w2 = ( my_layer_1 . get_weights ()[ 0 ][ 0 ][ 1 ]) layer1_b1 = ( my_layer_1 . get_weights ()[ 1 ][ 0 ]) layer1_b2 = ( my_layer_1 . get_weights ()[ 1 ][ 1 ]) layer2_w1 = ( my_layer_2 . get_weights ()[ 0 ][ 0 ]) layer2_w2 = ( my_layer_2 . get_weights ()[ 0 ][ 1 ]) layer2_b = ( my_layer_2 . get_weights ()[ 1 ][ 0 ]) neuron1_output = ( layer1_w1 * value_to_predict ) + layer1_b1 neuron2_output = ( layer1_w2 * value_to_predict ) + layer1_b2 neuron3_output = ( layer2_w1 * neuron1_output ) + ( layer2_w2 * neuron2_output ) + layer2_b print ( neuron3_output )","title":"Finally we can manually compute the output for our 2-layer network to better understand how it works."},{"location":"Linear_Regression_with_Synthetic_Data/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); #@title Copyright 2020 Google LLC. Double-click here for license information. # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Simple Linear Regression with Synthetic Data In this first Colab, you'll explore linear regression with a simple database. Learning objectives: After doing this exercise, you'll know how to do the following: Run Colabs. Tune the following hyperparameters : learning rate number of epochs batch size Interpret different kinds of loss curves . About Colabs Machine Learning Crash Course uses Colaboratories ( Colabs ) for all programming exercises. Colab is Google's implementation of Jupyter Notebook . Like all Jupyter Notebooks, a Colab consists of two kinds of components: Text cells , which contain explanations. You are currently reading a text cell. Code cells , which contain Python code for you to run. Code cells have a light gray background. You read the text cells and run the code cells. Running code cells You must run code cells in order. In other words, you may only run a code cell once all the code cells preceding it have already been run. To run a code cell: Place the cursor anywhere inside the [ ] area at the top left of a code cell. The area inside the [ ] will display an arrow. Click the arrow. Alternatively, you may invoke Runtime->Run all . Note, though, that some of the code cells will fail because not all the coding is complete. (You'll complete the coding as part of the exercise.) Understanding hidden code cells We've hidden the code in code cells that don't advance the learning objectives. For example, we've hidden the code that plots graphs. However, you must still run code cells containing hidden code . You'll know that the code is hidden because you'll see a title (for example, \"Load the functions that build and train a model\") without seeing the code. To view the hidden code, just double click the header. Why did you see an error? If a code cell returns an error when you run it, consider two common problems: You didn't run all of the code cells preceding the current code cell. If the code cell is labeled as a Task , then you haven't written the necessary code. Use the right version of TensorFlow The following hidden code cell ensures that the Colab will run on TensorFlow 2.X, which is the most recent version of TensorFlow: #@title Run this Colab on TensorFlow 2.x %tensorflow_version 2.x Import relevant modules The following cell imports the packages that the program requires: import pandas as pd import tensorflow as tf from matplotlib import pyplot as plt Define functions that build and train a model The following code defines two functions: build_model(my_learning_rate) , which builds an empty model. train_model(model, feature, label, epochs) , which trains the model from the examples (feature and label) you pass. Since you don't need to understand model building code right now, we've hidden this code cell. You may optionally double-click the headline to explore this code. #@title Define the functions that build and train a model def build_model(my_learning_rate): \"\"\"Create and compile a simple linear regression model.\"\"\" # Most simple tf.keras models are sequential. # A sequential model contains one or more layers. model = tf.keras.models.Sequential() # Describe the topography of the model. # The topography of a simple linear regression model # is a single node in a single layer. model.add(tf.keras.layers.Dense(units=1, input_shape=(1,))) # Compile the model topography into code that # TensorFlow can efficiently execute. Configure # training to minimize the model's mean squared error. model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()]) return model def train_model(model, feature, label, epochs, batch_size): \"\"\"Train the model by feeding it data.\"\"\" # Feed the feature values and the label values to the # model. The model will train for the specified number # of epochs, gradually learning how the feature values # relate to the label values. history = model.fit(x=feature, y=label, batch_size=batch_size, epochs=epochs) # Gather the trained model's weight and bias. trained_weight = model.get_weights()[0] trained_bias = model.get_weights()[1] # The list of epochs is stored separately from the # rest of history. epochs = history.epoch # Gather the history (a snapshot) of each epoch. hist = pd.DataFrame(history.history) # Specifically gather the model's root mean #squared error at each epoch. rmse = hist[\"root_mean_squared_error\"] return trained_weight, trained_bias, epochs, rmse print(\"Defined create_model and train_model\") Define plotting functions We're using a popular Python library called Matplotlib to create the following two plots: a plot of the feature values vs. the label values, and a line showing the output of the trained model. a loss curve . We hid the following code cell because learning Matplotlib is not relevant to the learning objectives. Regardless, you must still run all hidden code cells. #@title Define the plotting functions def plot_the_model(trained_weight, trained_bias, feature, label): \"\"\"Plot the trained model against the training feature and label.\"\"\" # Label the axes. plt.xlabel(\"feature\") plt.ylabel(\"label\") # Plot the feature values vs. label values. plt.scatter(feature, label) # Create a red line representing the model. The red line starts # at coordinates (x0, y0) and ends at coordinates (x1, y1). x0 = 0 y0 = trained_bias x1 = feature[-1] y1 = trained_bias + (trained_weight * x1) plt.plot([x0, x1], [y0, y1], c='r') # Render the scatter plot and the red line. plt.show() def plot_the_loss_curve(epochs, rmse): \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\" plt.figure() plt.xlabel(\"Epoch\") plt.ylabel(\"Root Mean Squared Error\") plt.plot(epochs, rmse, label=\"Loss\") plt.legend() plt.ylim([rmse.min()*0.97, rmse.max()]) plt.show() print(\"Defined the plot_the_model and plot_the_loss_curve functions.\") Define the dataset The dataset consists of 12 examples . Each example consists of one feature and one label . my_feature = ([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]) my_label = ([5.0, 8.8, 9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2]) Specify the hyperparameters The hyperparameters in this Colab are as follows: learning rate epochs batch_size The following code cell initializes these hyperparameters and then invokes the functions that build and train the model. learning_rate=0.01 epochs=10 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Task 1: Examine the graphs Examine the top graph. The blue dots identify the actual data; the red line identifies the output of the trained model. Ideally, the red line should align nicely with the blue dots. Does it? Probably not. A certain amount of randomness plays into training a model, so you'll get somewhat different results every time you train. That said, unless you are an extremely lucky person, the red line probably doesn't align nicely with the blue dots. Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently. Task 2: Increase the number of epochs Training loss should steadily decrease, steeply at first, and then more slowly. Eventually, training loss should eventually stay steady (zero slope or nearly zero slope), which indicates that training has converged . In Task 1, the training loss did not converge. One possible solution is to train for more epochs. Your task is to increase the number of epochs sufficiently to get the model to converge. However, it is inefficient to train past convergence, so don't just set the number of epochs to an arbitrarily high value. Examine the loss curve. Does the model converge? learning_rate=0.01 epochs= ? # Replace ? with an integer. my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.01 epochs=450 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) # The loss curve suggests that the model does converge. Task 3: Increase the learning rate In Task 2, you increased the number of epochs to get the model to converge. Sometimes, you can get the model to converge more quickly by increasing the learning rate. However, setting the learning rate too high often makes it impossible for a model to converge. In Task 3, we've intentionally set the learning rate too high. Run the following code cell and see what happens. # Increase the learning rate and decrease the number of epochs. learning_rate=100 epochs=500 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) The resulting model is terrible; the red line doesn't align with the blue dots. Furthermore, the loss curve oscillates like a roller coaster . An oscillating loss curve strongly suggests that the learning rate is too high. Task 4: Find the ideal combination of epochs and learning rate Assign values to the following two hyperparameters to make training converge as efficiently as possible: learning_rate epochs # Set the learning rate and number of epochs learning_rate= ? # Replace ? with a floating-point number epochs= ? # Replace ? with an integer my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.14 epochs=70 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Task 5: Adjust the batch size The system recalculates the model's loss value and adjusts the model's weights and bias after each iteration . Each iteration is the span in which the system processes one batch. For example, if the batch size is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples. One epoch spans sufficient iterations to process every example in the dataset. For example, if the batch size is 12, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations. It is tempting to simply set the batch size to the number of examples in the dataset (12, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. Experiment with batch_size in the following code cell. What's the smallest integer you can set for batch_size and still have the model converge in a hundred epochs? learning_rate=0.05 epochs=100 my_batch_size= ? # Replace ? with an integer. my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.05 epochs=125 my_batch_size=1 # Wow, a batch size of 1 works! my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) Summary of hyperparameter tuning Most machine learning problems require a lot of hyperparameter tuning. Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb: Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. If the training loss does not converge, train for more epochs. If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging. If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate. Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination. Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation. For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.","title":"Linear Regression with Synthetic Data"},{"location":"Linear_Regression_with_Synthetic_Data/#simple-linear-regression-with-synthetic-data","text":"In this first Colab, you'll explore linear regression with a simple database.","title":"Simple Linear Regression with Synthetic Data"},{"location":"Linear_Regression_with_Synthetic_Data/#learning-objectives","text":"After doing this exercise, you'll know how to do the following: Run Colabs. Tune the following hyperparameters : learning rate number of epochs batch size Interpret different kinds of loss curves .","title":"Learning objectives:"},{"location":"Linear_Regression_with_Synthetic_Data/#about-colabs","text":"Machine Learning Crash Course uses Colaboratories ( Colabs ) for all programming exercises. Colab is Google's implementation of Jupyter Notebook . Like all Jupyter Notebooks, a Colab consists of two kinds of components: Text cells , which contain explanations. You are currently reading a text cell. Code cells , which contain Python code for you to run. Code cells have a light gray background. You read the text cells and run the code cells.","title":"About Colabs"},{"location":"Linear_Regression_with_Synthetic_Data/#running-code-cells","text":"You must run code cells in order. In other words, you may only run a code cell once all the code cells preceding it have already been run. To run a code cell: Place the cursor anywhere inside the [ ] area at the top left of a code cell. The area inside the [ ] will display an arrow. Click the arrow. Alternatively, you may invoke Runtime->Run all . Note, though, that some of the code cells will fail because not all the coding is complete. (You'll complete the coding as part of the exercise.)","title":"Running code cells"},{"location":"Linear_Regression_with_Synthetic_Data/#understanding-hidden-code-cells","text":"We've hidden the code in code cells that don't advance the learning objectives. For example, we've hidden the code that plots graphs. However, you must still run code cells containing hidden code . You'll know that the code is hidden because you'll see a title (for example, \"Load the functions that build and train a model\") without seeing the code. To view the hidden code, just double click the header.","title":"Understanding hidden code cells"},{"location":"Linear_Regression_with_Synthetic_Data/#why-did-you-see-an-error","text":"If a code cell returns an error when you run it, consider two common problems: You didn't run all of the code cells preceding the current code cell. If the code cell is labeled as a Task , then you haven't written the necessary code.","title":"Why did you see an error?"},{"location":"Linear_Regression_with_Synthetic_Data/#use-the-right-version-of-tensorflow","text":"The following hidden code cell ensures that the Colab will run on TensorFlow 2.X, which is the most recent version of TensorFlow: #@title Run this Colab on TensorFlow 2.x %tensorflow_version 2.x","title":"Use the right version of TensorFlow"},{"location":"Linear_Regression_with_Synthetic_Data/#import-relevant-modules","text":"The following cell imports the packages that the program requires: import pandas as pd import tensorflow as tf from matplotlib import pyplot as plt","title":"Import relevant modules"},{"location":"Linear_Regression_with_Synthetic_Data/#define-functions-that-build-and-train-a-model","text":"The following code defines two functions: build_model(my_learning_rate) , which builds an empty model. train_model(model, feature, label, epochs) , which trains the model from the examples (feature and label) you pass. Since you don't need to understand model building code right now, we've hidden this code cell. You may optionally double-click the headline to explore this code. #@title Define the functions that build and train a model def build_model(my_learning_rate): \"\"\"Create and compile a simple linear regression model.\"\"\" # Most simple tf.keras models are sequential. # A sequential model contains one or more layers. model = tf.keras.models.Sequential() # Describe the topography of the model. # The topography of a simple linear regression model # is a single node in a single layer. model.add(tf.keras.layers.Dense(units=1, input_shape=(1,))) # Compile the model topography into code that # TensorFlow can efficiently execute. Configure # training to minimize the model's mean squared error. model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()]) return model def train_model(model, feature, label, epochs, batch_size): \"\"\"Train the model by feeding it data.\"\"\" # Feed the feature values and the label values to the # model. The model will train for the specified number # of epochs, gradually learning how the feature values # relate to the label values. history = model.fit(x=feature, y=label, batch_size=batch_size, epochs=epochs) # Gather the trained model's weight and bias. trained_weight = model.get_weights()[0] trained_bias = model.get_weights()[1] # The list of epochs is stored separately from the # rest of history. epochs = history.epoch # Gather the history (a snapshot) of each epoch. hist = pd.DataFrame(history.history) # Specifically gather the model's root mean #squared error at each epoch. rmse = hist[\"root_mean_squared_error\"] return trained_weight, trained_bias, epochs, rmse print(\"Defined create_model and train_model\")","title":"Define functions that build and train a model"},{"location":"Linear_Regression_with_Synthetic_Data/#define-plotting-functions","text":"We're using a popular Python library called Matplotlib to create the following two plots: a plot of the feature values vs. the label values, and a line showing the output of the trained model. a loss curve . We hid the following code cell because learning Matplotlib is not relevant to the learning objectives. Regardless, you must still run all hidden code cells. #@title Define the plotting functions def plot_the_model(trained_weight, trained_bias, feature, label): \"\"\"Plot the trained model against the training feature and label.\"\"\" # Label the axes. plt.xlabel(\"feature\") plt.ylabel(\"label\") # Plot the feature values vs. label values. plt.scatter(feature, label) # Create a red line representing the model. The red line starts # at coordinates (x0, y0) and ends at coordinates (x1, y1). x0 = 0 y0 = trained_bias x1 = feature[-1] y1 = trained_bias + (trained_weight * x1) plt.plot([x0, x1], [y0, y1], c='r') # Render the scatter plot and the red line. plt.show() def plot_the_loss_curve(epochs, rmse): \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\" plt.figure() plt.xlabel(\"Epoch\") plt.ylabel(\"Root Mean Squared Error\") plt.plot(epochs, rmse, label=\"Loss\") plt.legend() plt.ylim([rmse.min()*0.97, rmse.max()]) plt.show() print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")","title":"Define plotting functions"},{"location":"Linear_Regression_with_Synthetic_Data/#define-the-dataset","text":"The dataset consists of 12 examples . Each example consists of one feature and one label . my_feature = ([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]) my_label = ([5.0, 8.8, 9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])","title":"Define the dataset"},{"location":"Linear_Regression_with_Synthetic_Data/#specify-the-hyperparameters","text":"The hyperparameters in this Colab are as follows: learning rate epochs batch_size The following code cell initializes these hyperparameters and then invokes the functions that build and train the model. learning_rate=0.01 epochs=10 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Specify the hyperparameters"},{"location":"Linear_Regression_with_Synthetic_Data/#task-1-examine-the-graphs","text":"Examine the top graph. The blue dots identify the actual data; the red line identifies the output of the trained model. Ideally, the red line should align nicely with the blue dots. Does it? Probably not. A certain amount of randomness plays into training a model, so you'll get somewhat different results every time you train. That said, unless you are an extremely lucky person, the red line probably doesn't align nicely with the blue dots. Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently.","title":"Task 1: Examine the graphs"},{"location":"Linear_Regression_with_Synthetic_Data/#task-2-increase-the-number-of-epochs","text":"Training loss should steadily decrease, steeply at first, and then more slowly. Eventually, training loss should eventually stay steady (zero slope or nearly zero slope), which indicates that training has converged . In Task 1, the training loss did not converge. One possible solution is to train for more epochs. Your task is to increase the number of epochs sufficiently to get the model to converge. However, it is inefficient to train past convergence, so don't just set the number of epochs to an arbitrarily high value. Examine the loss curve. Does the model converge? learning_rate=0.01 epochs= ? # Replace ? with an integer. my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.01 epochs=450 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) # The loss curve suggests that the model does converge.","title":"Task 2: Increase the number of epochs"},{"location":"Linear_Regression_with_Synthetic_Data/#task-3-increase-the-learning-rate","text":"In Task 2, you increased the number of epochs to get the model to converge. Sometimes, you can get the model to converge more quickly by increasing the learning rate. However, setting the learning rate too high often makes it impossible for a model to converge. In Task 3, we've intentionally set the learning rate too high. Run the following code cell and see what happens. # Increase the learning rate and decrease the number of epochs. learning_rate=100 epochs=500 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) The resulting model is terrible; the red line doesn't align with the blue dots. Furthermore, the loss curve oscillates like a roller coaster . An oscillating loss curve strongly suggests that the learning rate is too high.","title":"Task 3: Increase the learning rate"},{"location":"Linear_Regression_with_Synthetic_Data/#task-4-find-the-ideal-combination-of-epochs-and-learning-rate","text":"Assign values to the following two hyperparameters to make training converge as efficiently as possible: learning_rate epochs # Set the learning rate and number of epochs learning_rate= ? # Replace ? with a floating-point number epochs= ? # Replace ? with an integer my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.14 epochs=70 my_batch_size=12 my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Task 4: Find the ideal combination of epochs and learning rate"},{"location":"Linear_Regression_with_Synthetic_Data/#task-5-adjust-the-batch-size","text":"The system recalculates the model's loss value and adjusts the model's weights and bias after each iteration . Each iteration is the span in which the system processes one batch. For example, if the batch size is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples. One epoch spans sufficient iterations to process every example in the dataset. For example, if the batch size is 12, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations. It is tempting to simply set the batch size to the number of examples in the dataset (12, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. Experiment with batch_size in the following code cell. What's the smallest integer you can set for batch_size and still have the model converge in a hundred epochs? learning_rate=0.05 epochs=100 my_batch_size= ? # Replace ? with an integer. my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse) #@title Double-click to view a possible solution learning_rate=0.05 epochs=125 my_batch_size=1 # Wow, a batch size of 1 works! my_model = build_model(learning_rate) trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size) plot_the_model(trained_weight, trained_bias, my_feature, my_label) plot_the_loss_curve(epochs, rmse)","title":"Task 5: Adjust the batch size"},{"location":"Linear_Regression_with_Synthetic_Data/#summary-of-hyperparameter-tuning","text":"Most machine learning problems require a lot of hyperparameter tuning. Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb: Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. If the training loss does not converge, train for more epochs. If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging. If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate. Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination. Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation. For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.","title":"Summary of hyperparameter tuning"},{"location":"Mimimizing_Loss/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # First import the functions we will need from __future__ import absolute_import , division , print_function , unicode_literals try : # %tensorflow_version only exists in Colab. % tensorflow_version 2. x except Exception : pass import tensorflow as tf import numpy as np import matplotlib.pyplot as plt GradientTape The Calculus is managed by a TensorFlow Gradient Tape. You can learn more about the gradient tape at https://www.tensorflow.org/api_docs/python/tf/GradientTape, and we will discuss it later in the course. # Define our initial guess INITIAL_W = 10.0 INITIAL_B = 10.0 # Define our loss function def loss ( predicted_y , target_y ): return tf . reduce_mean ( tf . square ( predicted_y - target_y )) # Define our training procedure def train ( model , inputs , outputs , learning_rate ): with tf . GradientTape () as t : current_loss = loss ( model ( inputs ), outputs ) # Here is where you differentiate the model values with respect to the loss function dw , db = t . gradient ( current_loss , [ model . w , model . b ]) # And here is where you update the model values based on the learning rate chosen model . w . assign_sub ( learning_rate * dw ) model . b . assign_sub ( learning_rate * db ) return current_loss # Define our simple linear regression model class Model ( object ): def __init__ ( self ): # Initialize the weights self . w = tf . Variable ( INITIAL_W ) self . b = tf . Variable ( INITIAL_B ) def __call__ ( self , x ): return self . w * x + self . b Train our model # Define our input data and learning rate xs = [ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ] ys = [ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ] LEARNING_RATE = 0.09 # Instantiate our model model = Model () # Collect the history of w-values and b-values to plot later list_w , list_b = [], [] epochs = range ( 50 ) losses = [] for epoch in epochs : list_w . append ( model . w . numpy ()) list_b . append ( model . b . numpy ()) current_loss = train ( model , xs , ys , learning_rate = LEARNING_RATE ) losses . append ( current_loss ) print ( 'Epoch %2d : w= %1.2f b= %1.2f , loss= %2.5f ' % ( epoch , list_w [ - 1 ], list_b [ - 1 ], current_loss )) Plot our trained values over time # Plot the w-values and b-values for each training Epoch against the true values TRUE_w = 2.0 TRUE_b = - 1.0 plt . plot ( epochs , list_w , 'r' , epochs , list_b , 'b' ) plt . plot ([ TRUE_w ] * len ( epochs ), 'r--' , [ TRUE_b ] * len ( epochs ), 'b--' ) plt . legend ([ 'w' , 'b' , 'True w' , 'True b' ]) plt . show ()","title":"Mimimizing Loss"},{"location":"Mimimizing_Loss/#gradienttape","text":"The Calculus is managed by a TensorFlow Gradient Tape. You can learn more about the gradient tape at https://www.tensorflow.org/api_docs/python/tf/GradientTape, and we will discuss it later in the course. # Define our initial guess INITIAL_W = 10.0 INITIAL_B = 10.0 # Define our loss function def loss ( predicted_y , target_y ): return tf . reduce_mean ( tf . square ( predicted_y - target_y )) # Define our training procedure def train ( model , inputs , outputs , learning_rate ): with tf . GradientTape () as t : current_loss = loss ( model ( inputs ), outputs ) # Here is where you differentiate the model values with respect to the loss function dw , db = t . gradient ( current_loss , [ model . w , model . b ]) # And here is where you update the model values based on the learning rate chosen model . w . assign_sub ( learning_rate * dw ) model . b . assign_sub ( learning_rate * db ) return current_loss # Define our simple linear regression model class Model ( object ): def __init__ ( self ): # Initialize the weights self . w = tf . Variable ( INITIAL_W ) self . b = tf . Variable ( INITIAL_B ) def __call__ ( self , x ): return self . w * x + self . b","title":"GradientTape"},{"location":"Mimimizing_Loss/#train-our-model","text":"# Define our input data and learning rate xs = [ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ] ys = [ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 7.0 ] LEARNING_RATE = 0.09 # Instantiate our model model = Model () # Collect the history of w-values and b-values to plot later list_w , list_b = [], [] epochs = range ( 50 ) losses = [] for epoch in epochs : list_w . append ( model . w . numpy ()) list_b . append ( model . b . numpy ()) current_loss = train ( model , xs , ys , learning_rate = LEARNING_RATE ) losses . append ( current_loss ) print ( 'Epoch %2d : w= %1.2f b= %1.2f , loss= %2.5f ' % ( epoch , list_w [ - 1 ], list_b [ - 1 ], current_loss ))","title":"Train our model"},{"location":"Mimimizing_Loss/#plot-our-trained-values-over-time","text":"# Plot the w-values and b-values for each training Epoch against the true values TRUE_w = 2.0 TRUE_b = - 1.0 plt . plot ( epochs , list_w , 'r' , epochs , list_b , 'b' ) plt . plot ([ TRUE_w ] * len ( epochs ), 'r--' , [ TRUE_b ] * len ( epochs ), 'b--' ) plt . legend ([ 'w' , 'b' , 'True w' , 'True b' ]) plt . show ()","title":"Plot our trained values over time"}]}